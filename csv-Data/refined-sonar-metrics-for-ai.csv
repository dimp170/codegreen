file_name,full_code,complexity,bugs,code_smells,duplicated_lines_density,cognitive_complexity,security_rating,ncloc,vulnerabilities
12306.py,"# -*- coding: utf-8 -*-
""""""
@author: liuyw
""""""
from splinter.browser import Browser
from time import sleep
import traceback
import time, sys

class huoche(object):
	driver_name = ''
	executable_path = ''
	#用户名，密码
	username = u""xxx""
	passwd = u""xxx""
	# cookies值得自己去找, 下面两个分别是沈阳, 哈尔滨
	starts = u""%u6C88%u9633%2CSYT""
	ends = u""%u54C8%u5C14%u6EE8%2CHBB""
	
	# 时间格式2018-01-19
	dtime = u""2018-01-19""
	# 车次，选择第几趟，0则从上之下依次点击
	order = 0
	###乘客名
	users = [u""xxx"",u""xxx""]
	##席位
	xb = u""二等座""
	pz = u""成人票""

	""""""网址""""""
	ticket_url = ""https://kyfw.12306.cn/otn/leftTicket/init""
	login_url = ""https://kyfw.12306.cn/otn/login/init""
	initmy_url = ""https://kyfw.12306.cn/otn/index/initMy12306""
	buy = ""https://kyfw.12306.cn/otn/confirmPassenger/initDc""
	
	def __init__(self):
		self.driver_name = 'chrome'
		self.executable_path = 'D:/chromedriver'

	def login(self):
		self.driver.visit(self.login_url)
		self.driver.fill(""loginUserDTO.user_name"", self.username)
		# sleep(1)
		self.driver.fill(""userDTO.password"", self.passwd)
		print(u""等待验证码，自行输入..."")
		while True:
			if self.driver.url != self.initmy_url:
				sleep(1)
			else:
				break

	def start(self):
		self.driver = Browser(driver_name=self.driver_name,executable_path=self.executable_path)
		self.driver.driver.set_window_size(1400, 1000)
		self.login()
		# sleep(1)
		self.driver.visit(self.ticket_url)
		try:
			print(u""购票页面开始..."")
			# sleep(1)
			# 加载查询信息
			self.driver.cookies.add({""_jc_save_fromStation"": self.starts})
			self.driver.cookies.add({""_jc_save_toStation"": self.ends})
			self.driver.cookies.add({""_jc_save_fromDate"": self.dtime})

			self.driver.reload()

			count = 0
			if self.order != 0:
				while self.driver.url == self.ticket_url:
					self.driver.find_by_text(u""查询"").click()
					count += 1
					print(u""循环点击查询... 第 %s 次"" % count)
					# sleep(1)
					try:
						self.driver.find_by_text(u""预订"")[self.order - 1].click()
					except Exception as e:
						print(e)
						print(u""还没开始预订"")
						continue
			else:
				while self.driver.url == self.ticket_url:
					self.driver.find_by_text(u""查询"").click()
					count += 1
					print(u""循环点击查询... 第 %s 次"" % count)
					# sleep(0.8)
					try:
						for i in self.driver.find_by_text(u""预订""):
							i.click()
							sleep(1)
					except Exception as e:
						print(e)
						print(u""还没开始预订 %s"" % count)
						continue
			print(u""开始预订..."")
			# sleep(3)
			# self.driver.reload()
			sleep(1)
			print(u'开始选择用户...')
			for user in self.users:
				self.driver.find_by_text(user).last.click()

			print(u""提交订单..."")
			sleep(1)
			self.driver.find_by_text(self.pz).click()
			self.driver.find_by_id('').select(self.pz)
			# sleep(1)
			self.driver.find_by_text(self.xb).click()
			sleep(1)
			self.driver.find_by_id('submitOrder_id').click()
			print(u""开始选座..."")
			self.driver.find_by_id('1D').last.click()
			self.driver.find_by_id('1F').last.click()

			sleep(1.5)
			print(u""确认选座..."")
			self.driver.find_by_id('qr_submit_id').click()

		except Exception as e:
			print(e)

if __name__ == '__main__':
	huoche = huoche()
	huoche.start()",11,0,3,0.0,22,1.0,93,0
8_puzzle.py,"from queue import PriorityQueue

class PuzzleState:
    def __init__(self, board, goal, moves=0, previous=None):
        self.board = board
        self.goal = goal
        self.moves = moves
        self.previous = previous

    def __lt__(self, other):
        return self.priority() < other.priority()

    def priority(self):
        return self.moves + self.manhattan()

    def manhattan(self):
        distance = 0
        for i in range(3):
            for j in range(3):
                if self.board[i][j] != 0:
                    x, y = divmod(self.board[i][j] - 1, 3)
                    distance += abs(x - i) + abs(y - j)
        return distance

    def is_goal(self):
        return self.board == self.goal

    def neighbors(self):
        neighbors = []
        x, y = next((i, j) for i in range(3) for j in range(3) if self.board[i][j] == 0)
        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 <= nx < 3 and 0 <= ny < 3:
                new_board = [row[:] for row in self.board]
                new_board[x][y], new_board[nx][ny] = new_board[nx][ny], new_board[x][y]
                neighbors.append(PuzzleState(new_board, self.goal, self.moves + 1, self))

        return neighbors

def solve_puzzle(initial_board, goal_board):
    initial_state = PuzzleState(initial_board, goal_board)
    frontier = PriorityQueue()
    frontier.put(initial_state)
    explored = set()

    while not frontier.empty():
        current_state = frontier.get()

        if current_state.is_goal():
            return current_state

        explored.add(tuple(map(tuple, current_state.board)))

        for neighbor in current_state.neighbors():
            if tuple(map(tuple, neighbor.board)) not in explored:
                frontier.put(neighbor)

    return None

def print_solution(solution):
    steps = []
    while solution:
        steps.append(solution.board)
        solution = solution.previous
    steps.reverse()

    for step in steps:
        for row in step:
            print(' '.join(map(str, row)))
        print()

# Example usage
initial_board = [
    [1, 2, 3],
    [4, 0, 5],
    [7, 8, 6]
]

goal_board = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 0]
]

solution = solve_puzzle(initial_board, goal_board)
if solution:
    print(""Solution found:"")
    print_solution(solution)
else:
    print(""No solution found."")
",23,0,0,0.0,24,1.0,72,0
__init__.py,,0,0,0,0.0,0,1.0,0,0
__main__.py,"""""""Allow cookiecutter to be executable from a checkout or zip file.""""""

import runpy

if __name__ == ""__main__"":
    runpy.run_module(""cookiecutter"", run_name=""__main__"")
",1,0,0,0.0,1,1.0,3,0
_metadata.py,"# Copyright 2021 The gRPC authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# AUTO-GENERATED FROM `$REPO_ROOT/templates/_metadata.py.template`!!!

__version__ = """"""1.71.0.dev0""""""
",0,0,0,0.0,0,1.0,1,0
A solution to project euler problem 3.py,"""""""
Problem:
The prime factors of 13195 are 5,7,13 and 29. What is the largest prime factor
of a given number N?

e.g. for 10, largest prime factor = 5. For 17, largest prime factor = 17.
""""""


# def solution(n: int) -> int:
def solution(n: int = 600851475143) -> int:
    """"""Returns the largest prime factor of a given number n.
    >>> solution(13195)
    29
    >>> solution(10)
    5
    >>> solution(17)
    17
    >>> solution(3.4)
    3
    >>> solution(0)
    Traceback (most recent call last):
        ...
    ValueError: Parameter n must be greater or equal to one.
    >>> solution(-17)
    Traceback (most recent call last):
        ...
    ValueError: Parameter n must be greater or equal to one.
    >>> solution([])
    Traceback (most recent call last):
        ...
    TypeError: Parameter n must be int or passive of cast to int.
    >>> solution(""asd"")
    Traceback (most recent call last):
        ...
    TypeError: Parameter n must be int or passive of cast to int.
    """"""
    try:
        n = int(n)
    except (TypeError, ValueError):
        raise TypeError(""Parameter n must be int or passive of cast to int."")
    if n <= 0:
        raise ValueError(""Parameter n must be greater or equal to one."")

    i = 2
    ans = 0

    if n == 2:
        return 2

    while n > 2:
        while n % i != 0:
            i += 1

        ans = i

        while n % i == 0:
            n = n / i

        i += 1

    return int(ans)


if __name__ == ""__main__"":
    # print(solution(int(input().strip())))
    import doctest

    doctest.testmod()
    print(solution(int(input().strip())))
",7,0,0,0.0,9,1.0,23,0
Add_two_Linked_List.py,"class Node:

    def __init__(self, data):

        self.data = data

        self.next = None





class LinkedList:

    def __init__(self):

        self.head = None



    def insert_at_beginning(self, new_data):

        new_node = Node(new_data)

        if self.head is None:

            self.head = new_node

            return

        new_node.next = self.head

        self.head = new_node



    def add_two_no(self, first, second):

        prev = None

        temp = None

        carry = 0

        while first is not None or second is not None:

            first_data = 0 if first is None else first.data

            second_data = 0 if second is None else second.data

            Sum = carry + first_data + second_data

            carry = 1 if Sum >= 10 else 0

            Sum = Sum if Sum < 10 else Sum % 10

            temp = Node(Sum)

            if self.head is None:

                self.head = temp

            else:

                prev.next = temp

            prev = temp

            if first is not None:

                first = first.next

            if second is not None:

                second = second.next

        if carry > 0:

            temp.next = Node(carry)



    def __str__(self):

        temp = self.head

        while temp:

            print(temp.data, ""->"", end="" "")

            temp = temp.next

        return ""None""





if __name__ == ""__main__"":

    first = LinkedList()

    second = LinkedList()

    first.insert_at_beginning(6)

    first.insert_at_beginning(4)

    first.insert_at_beginning(9)



    second.insert_at_beginning(2)

    second.insert_at_beginning(2)



    print(""First Linked List: "")

    print(first)

    print(""Second Linked List: "")

    print(second)



    result = LinkedList()

    result.add_two_no(first.head, second.head)

    print(""Final Result: "")

    print(result)

",18,1,2,0.0,21,1.0,58,0
Anonymous_TextApp.py,"import tkinter as tk
from PIL import Image, ImageTk
from twilio.rest import Client

window = tk.Tk()
window.title(""Anonymous_Text_App"")
window.geometry(""800x750"")

# Define global variables
body = """"
to = """"

def message():
    global body, to
    account_sid = 'Your_account_sid' # Your account sid
    auth_token = 'Your_auth_token' # Your auth token
    client = Client(account_sid, auth_token)
    msg = client.messages.create(
        from_='Twilio_number',    # Twilio number
        body=body,
        to=to
    )
    print(msg.sid)
    confirmation_label.config(text=""Message Sent!"")  



try:
    # Load the background image
    bg_img = Image.open(r""D:\Downloads\img2.png"")
    
    #Canvas widget
    canvas = tk.Canvas(window, width=800, height=750)
    canvas.pack(fill=""both"", expand=True)
    
    #  background image to the Canvas
    bg_photo = ImageTk.PhotoImage(bg_img)
    bg_image_id = canvas.create_image(0, 0, image=bg_photo, anchor=""nw"")
    bg_image_id = canvas.create_image(550, 250, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(1100, 250, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(1250, 250, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(250, 750, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(850, 750, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(1300, 750, image=bg_photo, anchor=""center"")
    
  
    
    # Foreground Image
    img = Image.open(r""D:\Downloads\output-onlinepngtools.png"")
    photo = ImageTk.PhotoImage(img)
    img_label = tk.Label(window, image=photo, anchor=""w"")
    img_label.image = photo  
    img_label.place(x=10, y=20)  
    
    # Text for number input
    canvas.create_text(1050, 300, text=""Enter the number starting with +[country code]"", font=(""Poppins"", 18, ""bold""), fill=""black"", anchor=""n"")
    text_field_number = tk.Entry(canvas, width=17, font=(""Poppins"", 25, ""bold""), bg=""#404040"", fg=""white"", show=""*"")
    canvas.create_window(1100, 350, window=text_field_number, anchor=""n"")
    
    # Text for message input
    canvas.create_text(1050, 450, text=""Enter the Message"", font=(""Poppins"", 18, ""bold""), fill=""black"", anchor=""n"")
    text_field_text = tk.Entry(canvas, width=17, font=(""Poppins"", 25, ""bold""), bg=""#404040"", fg=""white"")
    canvas.create_window(1100, 500, window=text_field_text, anchor=""n"")
    
    #  label for confirmation message
    confirmation_label = tk.Label(window, text="""", font=(""Poppins"", 16), fg=""green"")
    canvas.create_window(1100, 600, window=confirmation_label, anchor=""n"")
    
except Exception as e:
    print(f""Error loading image: {e}"")

# Function to save input and send message
def save_and_send():
    global body, to
    to = str(text_field_number.get())
    body = str(text_field_text.get())
    message()

# Button to save input and send message
save_button = tk.Button(window, text=""Save and Send"", command=save_and_send)
canvas.create_window(1200, 550, window=save_button, anchor='n')

window.mainloop()",2,0,0,0.0,1,5.0,55,1
api_gen.py,"""""""Script to generate keras public API in `keras/api` directory.

Usage:

Run via `./shell/api_gen.sh`.
It generates API and formats user and generated APIs.
""""""

import importlib
import os
import re
import shutil

import namex

PACKAGE = ""keras""
BUILD_DIR_NAME = ""tmp_build_dir""


def ignore_files(_, filenames):
    return [f for f in filenames if f.endswith(""_test.py"")]


def copy_source_to_build_directory(root_path):
    # Copy sources (`keras/` directory and setup files) to build dir
    build_dir = os.path.join(root_path, BUILD_DIR_NAME)
    if os.path.exists(build_dir):
        shutil.rmtree(build_dir)
    os.mkdir(build_dir)
    shutil.copytree(
        PACKAGE, os.path.join(build_dir, PACKAGE), ignore=ignore_files
    )
    return build_dir


def create_legacy_directory(package_dir):
    src_dir = os.path.join(package_dir, ""src"")
    api_dir = os.path.join(package_dir, ""api"")
    # Make keras/_tf_keras/ by copying keras/
    tf_keras_dirpath_parent = os.path.join(api_dir, ""_tf_keras"")
    tf_keras_dirpath = os.path.join(tf_keras_dirpath_parent, ""keras"")
    os.makedirs(tf_keras_dirpath, exist_ok=True)
    with open(os.path.join(tf_keras_dirpath_parent, ""__init__.py""), ""w"") as f:
        f.write(""from keras.api._tf_keras import keras\n"")
    with open(os.path.join(api_dir, ""__init__.py"")) as f:
        init_file = f.read()
        init_file = init_file.replace(
            ""from keras.api import _legacy"",
            ""from keras.api import _tf_keras"",
        )
    with open(os.path.join(api_dir, ""__init__.py""), ""w"") as f:
        f.write(init_file)
    # Remove the import of `_tf_keras` in `keras/_tf_keras/keras/__init__.py`
    init_file = init_file.replace(""from keras.api import _tf_keras\n"", ""\n"")
    with open(os.path.join(tf_keras_dirpath, ""__init__.py""), ""w"") as f:
        f.write(init_file)
    for dirname in os.listdir(api_dir):
        dirpath = os.path.join(api_dir, dirname)
        if os.path.isdir(dirpath) and dirname not in (
            ""_legacy"",
            ""_tf_keras"",
            ""src"",
        ):
            destpath = os.path.join(tf_keras_dirpath, dirname)
            if os.path.exists(destpath):
                shutil.rmtree(destpath)
            shutil.copytree(
                dirpath,
                destpath,
                ignore=ignore_files,
            )

    # Copy keras/_legacy/ file contents to keras/_tf_keras/keras
    legacy_submodules = [
        path[:-3]
        for path in os.listdir(os.path.join(src_dir, ""legacy""))
        if path.endswith("".py"")
    ]
    legacy_submodules += [
        path
        for path in os.listdir(os.path.join(src_dir, ""legacy""))
        if os.path.isdir(os.path.join(src_dir, ""legacy"", path))
    ]
    for root, _, fnames in os.walk(os.path.join(api_dir, ""_legacy"")):
        for fname in fnames:
            if fname.endswith("".py""):
                legacy_fpath = os.path.join(root, fname)
                tf_keras_root = root.replace(""/_legacy"", ""/_tf_keras/keras"")
                core_api_fpath = os.path.join(
                    root.replace(""/_legacy"", """"), fname
                )
                if not os.path.exists(tf_keras_root):
                    os.makedirs(tf_keras_root)
                tf_keras_fpath = os.path.join(tf_keras_root, fname)
                with open(legacy_fpath) as f:
                    legacy_contents = f.read()
                    legacy_contents = legacy_contents.replace(
                        ""keras.api._legacy"", ""keras.api._tf_keras.keras""
                    )
                if os.path.exists(core_api_fpath):
                    with open(core_api_fpath) as f:
                        core_api_contents = f.read()
                    core_api_contents = core_api_contents.replace(
                        ""from keras.api import _tf_keras\n"", """"
                    )
                    for legacy_submodule in legacy_submodules:
                        core_api_contents = core_api_contents.replace(
                            f""from keras.api import {legacy_submodule}\n"",
                            """",
                        )
                        core_api_contents = core_api_contents.replace(
                            f""keras.api.{legacy_submodule}"",
                            f""keras.api._tf_keras.keras.{legacy_submodule}"",
                        )
                    # Remove duplicate generated comments string.
                    legacy_contents = re.sub(r""\n"", r""\\n"", legacy_contents)
                    legacy_contents = re.sub('"""""".*""""""', """", legacy_contents)
                    legacy_contents = re.sub(r""\\n"", r""\n"", legacy_contents)
                    # If the same module is in legacy and core_api, use legacy
                    legacy_imports = re.findall(
                        r""import (\w+)"", legacy_contents
                    )
                    for import_name in legacy_imports:
                        core_api_contents = re.sub(
                            f""\n.* import {import_name}\n"",
                            r""\n"",
                            core_api_contents,
                        )
                    legacy_contents = core_api_contents + ""\n"" + legacy_contents
                with open(tf_keras_fpath, ""w"") as f:
                    f.write(legacy_contents)

    # Delete keras/api/_legacy/
    shutil.rmtree(os.path.join(api_dir, ""_legacy""))


def export_version_string(api_init_fname):
    with open(api_init_fname) as f:
        contents = f.read()
    with open(api_init_fname, ""w"") as f:
        contents += ""from keras.src.version import __version__\n""
        f.write(contents)


def update_package_init(template_fname, dest_fname, api_module):
    with open(template_fname) as template_file:
        with open(dest_fname, ""w"") as dest_file:
            for line in template_file:
                if ""# DO NOT EDIT."" in line:
                    dest_file.write(line)
                    # Import all public symbols from `api/` and `__version__`.
                    for symbol in api_module.__dict__.keys():
                        if symbol.startswith(""_"") and symbol != ""__version__"":
                            continue
                        dest_file.write(f""from keras.api import {symbol}\n"")
                    # Skip the previous autogenerated block.
                    for line in template_file:
                        if ""# END DO NOT EDIT."" in line:
                            break
                dest_file.write(line)


def build():
    # Backup the `keras/__init__.py` and restore it on error in api gen.
    root_path = os.path.dirname(os.path.abspath(__file__))
    code_api_dir = os.path.join(root_path, PACKAGE, ""api"")
    code_init_fname = os.path.join(root_path, PACKAGE, ""__init__.py"")
    # Create temp build dir
    build_dir = copy_source_to_build_directory(root_path)
    build_api_dir = os.path.join(build_dir, PACKAGE, ""api"")
    build_init_fname = os.path.join(build_dir, PACKAGE, ""__init__.py"")
    build_api_init_fname = os.path.join(build_api_dir, ""__init__.py"")
    try:
        os.chdir(build_dir)
        # Generates `keras/api` directory.
        if os.path.exists(build_api_dir):
            shutil.rmtree(build_api_dir)
        if os.path.exists(build_init_fname):
            os.remove(build_init_fname)
        os.makedirs(build_api_dir)
        namex.generate_api_files(
            ""keras"", code_directory=""src"", target_directory=""api""
        )
        # Add __version__ to `api/`.
        export_version_string(build_api_init_fname)
        # Creates `_tf_keras` with full keras API
        create_legacy_directory(package_dir=os.path.join(build_dir, PACKAGE))
        # Update toplevel init with all `api/` imports.
        api_module = importlib.import_module(f""{BUILD_DIR_NAME}.keras.api"")
        update_package_init(code_init_fname, build_init_fname, api_module)
        # Copy back the keras/api and keras/__init__.py from build directory
        if os.path.exists(code_api_dir):
            shutil.rmtree(code_api_dir)
        shutil.copytree(build_api_dir, code_api_dir)
        shutil.copy(build_init_fname, code_init_fname)
    finally:
        # Clean up: remove the build directory (no longer needed)
        shutil.rmtree(build_dir)


if __name__ == ""__main__"":
    build()
",32,0,5,0.0,54,1.0,159,0
AREA OF TRIANGLE.py,"# Python Program to find the area of triangle
# calculates area of traingle in efficient way!!
a = 5
b = 6
c = 7

# Uncomment below to take inputs from the user
# a = float(input('Enter first side: '))
# b = float(input('Enter second side: '))
# c = float(input('Enter third side: '))

# calculate the semi-perimeter
s = (a + b + c) / 2

# calculate the area
area = (s * (s - a) * (s - b) * (s - c)) ** 0.5
print(""The area of the triangle is %0.2f"" % area)
",0,0,0,0.0,0,1.0,6,0
AreaOfTriangle.py,"# Python Program to find the area of triangle when all three side-lengths are known!

a = 5
b = 6
c = 7

# Uncomment below to take inputs from the user
# a = float(input('Enter first side: '))
# b = float(input('Enter second side: '))
# c = float(input('Enter third side: '))

# calculate the semi-perimeter
s = (a + b + c) / 2

# calculate the area
area = (s * (s - a) * (s - b) * (s - c)) ** 0.5
print(""The area of the triangle is: "" + area)
",0,0,0,0.0,0,1.0,6,0
ARKA.py,"def sumOfSeries(n):
    x = n * (n + 1) / 2
    return (int)(x * x)


# Driver Function
n = 5
print(sumOfSeries(n))
",1,0,1,0.0,0,1.0,5,0
Armstrong_number.py,"""""""

In number theory, a narcissistic number (also known as a pluperfect digital invariant (PPDI), an Armstrong number (after Michael F. Armstrong) or a plus perfect number), 

in a given number base b, is a number that is the total of its own digits each raised to the power of the number of digits.

Source: https://en.wikipedia.org/wiki/Narcissistic_number

NOTE:

this scripts only works for number in base 10

""""""



def is_armstrong_number(number:str):

    total:int = 0

    exp:int = len(number) #get the number of digits, this will determinate the exponent



    digits:list[int] = []

    for digit in number: digits.append(int(digit)) #get the single digits

    for x in digits: total += x ** exp #get the power of each digit and sum it to the total

    

    # display the result

    if int(number) == total:

       print(number,""is an Armstrong number"")

    else:

       print(number,""is not an Armstrong number"")



number = input(""Enter the number : "")

is_armstrong_number(number)

",4,0,0,0.0,4,1.0,12,0
ASCIIvaluecharacter.py,"# Program to find the ASCII value of the given character

c = ""p""
print(""The ASCII value of '"" + c + ""' is"", ord(c))
",0,0,0,0.0,0,1.0,2,0
baiduwenku.py,"# -*- coding:UTF-8 -*-
from selenium import webdriver
from bs4 import BeautifulSoup
import re
import time

if __name__ == '__main__':

	options = webdriver.ChromeOptions()
	options.add_argument('user-agent=""Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19""')
	driver = webdriver.Chrome('J:\迅雷下载\chromedriver.exe', chrome_options=options)
	driver.get('https://wenku.baidu.com/view/aa31a84bcf84b9d528ea7a2c.html')

	html = driver.page_source
	bf1 = BeautifulSoup(html, 'lxml')
	result = bf1.find_all(class_='rtcspage')
	bf2 = BeautifulSoup(str(result[0]), 'lxml')
	title = bf2.div.div.h1.string
	pagenum = bf2.find_all(class_='size')
	pagenum = BeautifulSoup(str(pagenum), 'lxml').span.string
	pagepattern = re.compile('页数：(\d+)页')
	num = int(pagepattern.findall(pagenum)[0])
	print('文章标题：%s' % title)
	print('文章页数：%d' % num)


	while True:
		num = num / 5.0
		html = driver.page_source
		bf1 = BeautifulSoup(html, 'lxml')
		result = bf1.find_all(class_='rtcspage')
		for each_result in result:
			bf2 = BeautifulSoup(str(each_result), 'lxml')
			texts = bf2.find_all('p')
			for each_text in texts:
				main_body = BeautifulSoup(str(each_text), 'lxml')
				for each in main_body.find_all(True):
					if each.name == 'span':
						print(each.string.replace('\xa0',''),end='')
					elif each.name == 'br':
						print('')
			print('\n')
		if num > 1:
			page = driver.find_elements_by_xpath(""//div[@class='page']"")
			driver.execute_script('arguments[0].scrollIntoView();', page[-1]) #拖动到可见的元素去
			nextpage = driver.find_element_by_xpath(""//a[@data-fun='next']"")
			nextpage.click()
			time.sleep(3)
		else:
			break",7,0,0,0.0,26,1.0,44,0
baiduwenku_pro_1.py,"import requests
import re
import json
import os

session = requests.session()


def fetch_url(url):
    return session.get(url).content.decode('gbk')


def get_doc_id(url):
    return re.findall('view/(.*).html', url)[0]


def parse_type(content):
    return re.findall(r""docType.*?\:.*?\'(.*?)\'\,"", content)[0]


def parse_title(content):
    return re.findall(r""title.*?\:.*?\'(.*?)\'\,"", content)[0]


def parse_doc(content):
    result = ''
    url_list = re.findall('(https.*?0.json.*?)\\\\x22}', content)
    url_list = [addr.replace(""\\\\\\/"", ""/"") for addr in url_list]
    for url in url_list[:-5]:
        content = fetch_url(url)
        y = 0
        txtlists = re.findall('""c"":""(.*?)"".*?""y"":(.*?),', content)
        for item in txtlists:
            if not y == item[1]:
                y = item[1]
                n = '\n'
            else:
                n = ''
            result += n
            result += item[0].encode('utf-8').decode('unicode_escape', 'ignore')
    return result


def parse_txt(doc_id):
    content_url = 'https://wenku.baidu.com/api/doc/getdocinfo?callback=cb&doc_id=' + doc_id
    content = fetch_url(content_url)
    md5 = re.findall('""md5sum"":""(.*?)""', content)[0]
    pn = re.findall('""totalPageNum"":""(.*?)""', content)[0]
    rsign = re.findall('""rsign"":""(.*?)""', content)[0]
    content_url = 'https://wkretype.bdimg.com/retype/text/' + doc_id + '?rn=' + pn + '&type=txt' + md5 + '&rsign=' + rsign
    content = json.loads(fetch_url(content_url))
    result = ''
    for item in content:
        for i in item['parags']:
            result += i['c'].replace('\\r', '\r').replace('\\n', '\n')
    return result


def parse_other(doc_id):
    content_url = ""https://wenku.baidu.com/browse/getbcsurl?doc_id="" + doc_id + ""&pn=1&rn=99999&type=ppt""
    content = fetch_url(content_url)
    url_list = re.findall('{""zoom"":""(.*?)"",""page""', content)
    url_list = [item.replace(""\\"", '') for item in url_list]
    if not os.path.exists(doc_id):
        os.mkdir(doc_id)
    for index, url in enumerate(url_list):
        content = session.get(url).content
        path = os.path.join(doc_id, str(index) + '.jpg')
        with open(path, 'wb') as f:
            f.write(content)
    print(""图片保存在"" + doc_id + ""文件夹"")


def save_file(filename, content):
    with open(filename, 'w', encoding='utf8') as f:
        f.write(content)
        print('已保存为:' + filename)


# test_txt_url = 'https://wenku.baidu.com/view/cbb4af8b783e0912a3162a89.html?from=search'
# test_ppt_url = 'https://wenku.baidu.com/view/2b7046e3f78a6529657d5376.html?from=search'
# test_pdf_url = 'https://wenku.baidu.com/view/dd6e15c1227916888586d795.html?from=search'
# test_xls_url = 'https://wenku.baidu.com/view/eb4a5bb7312b3169a551a481.html?from=search'
def main():
    url = input('请输入要下载的文库URL地址')
    content = fetch_url(url)
    doc_id = get_doc_id(url)
    type = parse_type(content)
    title = parse_title(content)
    if type == 'doc':
        result = parse_doc(content)
        save_file(title + '.txt', result)
    elif type == 'txt':
        result = parse_txt(doc_id)
        save_file(title + '.txt', result)
    else:
        parse_other(doc_id)


if __name__ == ""__main__"":
    main()
",18,0,3,0.0,16,1.0,76,0
Base Converter Number system.py,"def base_check(xnumber, xbase):
    for char in xnumber[len(xnumber) - 1]:
        if int(char) >= int(xbase):
            return False
    return True


def convert_from_10(xnumber, xbase, arr, ybase):
    if int(xbase) == 2 or int(xbase) == 4 or int(xbase) == 6 or int(xbase) == 8:

        if xnumber == 0:
            return arr
        else:
            quotient = int(xnumber) // int(xbase)
            remainder = int(xnumber) % int(xbase)
            arr.append(remainder)
            dividend = quotient
            convert_from_10(dividend, xbase, arr, base)
    elif int(xbase) == 16:
        if int(xnumber) == 0:
            return arr
        else:
            quotient = int(xnumber) // int(xbase)
            remainder = int(xnumber) % int(xbase)
            if remainder > 9:
                if remainder == 10:
                    remainder = ""A""
                if remainder == 11:
                    remainder = ""B""
                if remainder == 12:
                    remainder = ""C""
                if remainder == 13:
                    remainder = ""D""
                if remainder == 14:
                    remainder = ""E""
                if remainder == 15:
                    remainder = ""F""
            arr.append(remainder)
            dividend = quotient
            convert_from_10(dividend, xbase, arr, ybase)


def convert_to_10(xnumber, xbase, arr, ybase):
    if int(xbase) == 10:
        for char in xnumber:
            arr.append(char)
        flipped = arr[::-1]
        ans = 0
        j = 0

        for i in flipped:
            ans = ans + (int(i) * (int(ybase) ** j))
            j = j + 1
        return ans


arrayfrom = []
arrayto = []
is_base_possible = False
number = input(""Enter the number you would like to convert: "")

while not is_base_possible:
    base = input(""What is the base of this number? "")
    is_base_possible = base_check(number, base)
    if not is_base_possible:
        print(f""The number {number} is not a base {base} number"")
        base = input
    else:
        break
dBase = input(""What is the base you would like to convert to? "")
if int(base) == 10:
    convert_from_10(number, dBase, arrayfrom, base)
    answer = arrayfrom[::-1]  # reverses the array
    print(f""In base {dBase} this number is: "")
    print(*answer, sep="""")
elif int(dBase) == 10:
    answer = convert_to_10(number, dBase, arrayto, base)
    print(f""In base {dBase} this number is: {answer} "")
else:
    number = convert_to_10(number, 10, arrayto, base)
    convert_from_10(number, dBase, arrayfrom, base)
    answer = arrayfrom[::-1]
    print(f""In base {dBase} this number is: "")
    print(*answer, sep="""")
",24,1,1,0.0,51,1.0,75,0
Battery_notifier.py,"from plyer import notification  # pip install plyer
import psutil  # pip install psutil

# psutil.sensors_battery() will return the information related to battery
battery = psutil.sensors_battery()

# battery percent will return the current battery prcentage
percent = battery.percent
charging = (
    battery.power_plugged
)

# Notification(title, description, duration)--to send
# notification to desktop
# help(Notification)
if charging:
    if percent == 100:
        charging_message = ""Unplug your Charger""
    else:
        charging_message = ""Charging""
else:
    charging_message = ""Not Charging""
message = str(percent) + ""% Charged\n"" + charging_message

notification.notify(""Battery Information"", message, timeout=10)
",2,0,0,0.0,5,1.0,16,0
benchmark.py,"#!./kitty/launcher/kitty +launch
# License: GPL v3 Copyright: 2016, Kovid Goyal <kovid at kovidgoyal.net>

import fcntl
import io
import os
import select
import signal
import struct
import sys
import termios
import time
from pty import CHILD, fork

from kitty.constants import kitten_exe
from kitty.fast_data_types import Screen, safe_pipe
from kitty.utils import read_screen_size


def run_parsing_benchmark(cell_width: int = 10, cell_height: int = 20, scrollback: int = 20000) -> None:
    isatty = sys.stdout.isatty()
    if isatty:
        sz = read_screen_size()
        columns, rows = sz.cols, sz.rows
    else:
        columns, rows = 80, 25
    child_pid, master_fd = fork()
    is_child = child_pid == CHILD
    argv = [kitten_exe(), '__benchmark__', '--with-scrollback']
    if is_child:
        while read_screen_size().width != columns * cell_width:
            time.sleep(0.01)
        signal.pthread_sigmask(signal.SIG_SETMASK, ())
        os.execvp(argv[0], argv)
    # os.set_blocking(master_fd, False)
    x_pixels = columns * cell_width
    y_pixels = rows * cell_height
    s = struct.pack('HHHH', rows, columns, x_pixels, y_pixels)
    fcntl.ioctl(master_fd, termios.TIOCSWINSZ, s)

    write_buf = b''
    r_pipe, w_pipe = safe_pipe(True)
    class ToChild:
        def write(self, x: bytes | str) -> None:
            nonlocal write_buf
            if isinstance(x, str):
                x = x.encode()
            write_buf += x
            os.write(w_pipe, b'1')

    screen = Screen(None, rows, columns, scrollback, cell_width, cell_height, 0, ToChild())

    def parse_bytes(data: bytes) -> None:
        data = memoryview(data)
        while data:
            dest = screen.test_create_write_buffer()
            s = screen.test_commit_write_buffer(data, dest)
            data = data[s:]
            screen.test_parse_written_data()


    while True:
        rd, wd, _ = select.select([master_fd, r_pipe], [master_fd] if write_buf else [], [])
        if r_pipe in rd:
            os.read(r_pipe, 256)
        if master_fd in rd:
            try:
                data = os.read(master_fd, io.DEFAULT_BUFFER_SIZE)
            except OSError:
                data = b''
            if not data:
                break
            parse_bytes(data)
        if master_fd in wd:
            n = os.write(master_fd, write_buf)
            write_buf = write_buf[n:]
    if isatty:
        lines: list[str] = []
        screen.linebuf.as_ansi(lines.append)
        sys.stdout.write(''.join(lines))
    else:
        sys.stdout.write(str(screen.linebuf))


def main() -> None:
    run_parsing_benchmark()


if __name__ == '__main__':
    main()
",17,0,1,0.0,26,1.0,74,0
Binary Coefficients.py,"def pascal_triangle(lineNumber):
    list1 = list()
    list1.append([1])
    i = 1
    while i <= lineNumber:
        j = 1
        l = []
        l.append(1)
        while j < i:
            l.append(list1[i - 1][j] + list1[i - 1][j - 1])
            j = j + 1
        l.append(1)
        list1.append(l)
        i = i + 1
    return list1


def binomial_coef(n, k):
    pascalTriangle = pascal_triangle(n)
    return pascalTriangle[n][k - 1]
",4,0,2,0.0,3,1.0,18,0
Binary_search.py,"# It returns location of x in given array arr
# if present, else returns -1
def binary_search(arr, l, r, x):
    # Base case: if left index is greater than right index, element is not present
    if l > r:
        return -1

    # Calculate the mid index
    mid = (l + r) // 2

    # If element is present at the middle itself
    if arr[mid] == x:
        return mid

    # If element is smaller than mid, then it can only be present in left subarray
    elif arr[mid] > x:
        return binary_search(arr, l, mid - 1, x)

    # Else the element can only be present in right subarray
    else:
        return binary_search(arr, mid + 1, r, x)


# Main Function
if __name__ == ""__main__"":
    # User input array
    arr = [int(x) for x in input(""Enter the array with elements separated by commas: "").split("","")]

    # User input element to search for
    x = int(input(""Enter the element you want to search for: ""))

    # Function call
    result = binary_search(arr, 0, len(arr) - 1, x)

    # printing the output
    if result != -1:
        print(""Element is present at index {}"".format(result))
    else:
        print(""Element is not present in array"")
",5,0,0,0.0,8,1.0,18,0
Binary_to_Decimal.py,"# Program to convert binary to decimal


def binaryToDecimal(binary):
    """"""
    >>> binaryToDecimal(111110000)
    496
    >>> binaryToDecimal(10100)
    20
    >>> binaryToDecimal(101011)
    43
    """"""
    decimal, i, n = 0, 0, 0
    while binary != 0:
        dec = binary % 10
        decimal = decimal + dec * pow(2, i)
        binary = binary // 10
        i += 1
    print(decimal)


binaryToDecimal(100)
",2,0,2,0.0,1,1.0,9,0
biqukan.py,"# -*- coding:UTF-8 -*-
from urllib import request
from bs4 import BeautifulSoup
import collections
import re
import os
import time
import sys
import types

""""""
类说明:下载《笔趣看》网小说: url:https://www.biqukan.com/

Parameters:
	target - 《笔趣看》网指定的小说目录地址(string)

Returns:
	无

Modify:
	2017-05-06
""""""
class download(object):
	def __init__(self, target):
		self.__target_url = target
		self.__head = {'User-Agent':'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19',}

	""""""
	函数说明:获取下载链接

	Parameters:
		无

	Returns:
		novel_name + '.txt' - 保存的小说名(string)
		numbers - 章节数(int)
		download_dict - 保存章节名称和下载链接的字典(dict)

	Modify:
		2017-05-06
	""""""
	def get_download_url(self):
		charter = re.compile(u'[第弟](.+)章', re.IGNORECASE)
		target_req = request.Request(url = self.__target_url, headers = self.__head)
		target_response = request.urlopen(target_req)
		target_html = target_response.read().decode('gbk','ignore')
		listmain_soup = BeautifulSoup(target_html,'lxml')
		chapters = listmain_soup.find_all('div',class_ = 'listmain')
		download_soup = BeautifulSoup(str(chapters), 'lxml')
		novel_name = str(download_soup.dl.dt).split(""》"")[0][5:]
		flag_name = ""《"" + novel_name + ""》"" + ""正文卷""
		numbers = (len(download_soup.dl.contents) - 1) / 2 - 8
		download_dict = collections.OrderedDict()
		begin_flag = False
		numbers = 1
		for child in download_soup.dl.children:
			if child != '\n':
				if child.string == u""%s"" % flag_name:
					begin_flag = True
				if begin_flag == True and child.a != None:
					download_url = ""https://www.biqukan.com"" + child.a.get('href')
					download_name = child.string
					names = str(download_name).split('章')
					name = charter.findall(names[0] + '章')
					if name:
							download_dict['第' + str(numbers) + '章 ' + names[1]] = download_url
							numbers += 1
		return novel_name + '.txt', numbers, download_dict
	
	""""""
	函数说明:爬取文章内容

	Parameters:
		url - 下载连接(string)

	Returns:
		soup_text - 章节内容(string)

	Modify:
		2017-05-06
	""""""
	def Downloader(self, url):
		download_req = request.Request(url = url, headers = self.__head)
		download_response = request.urlopen(download_req)
		download_html = download_response.read().decode('gbk','ignore')
		soup_texts = BeautifulSoup(download_html, 'lxml')
		texts = soup_texts.find_all(id = 'content', class_ = 'showtxt')
		soup_text = BeautifulSoup(str(texts), 'lxml').div.text.replace('\xa0','')
		return soup_text

	""""""
	函数说明:将爬取的文章内容写入文件

	Parameters:
		name - 章节名称(string)
		path - 当前路径下,小说保存名称(string)
		text - 章节内容(string)

	Returns:
		无

	Modify:
		2017-05-06
	""""""
	def Writer(self, name, path, text):
		write_flag = True
		with open(path, 'a', encoding='utf-8') as f:
			f.write(name + '\n\n')
			for each in text:
				if each == 'h':
					write_flag = False
				if write_flag == True and each != ' ':
					f.write(each)
				if write_flag == True and each == '\r':
					f.write('\n')			
			f.write('\n\n')

if __name__ == ""__main__"":
	print(""\n\t\t欢迎使用《笔趣看》小说下载小工具\n\n\t\t作者:Jack-Cui\t时间:2017-05-06\n"")
	print(""*************************************************************************"")
	
	#小说地址
	target_url = str(input(""请输入小说目录下载地址:\n""))

	#实例化下载类
	d = download(target = target_url)
	name, numbers, url_dict = d.get_download_url()
	if name in os.listdir():
		os.remove(name)
	index = 1

	#下载中
	print(""《%s》下载中:"" % name[:-4])
	for key, value in url_dict.items():
		d.Writer(key, name, d.Downloader(value))
		sys.stdout.write(""已下载:%.3f%%"" %  float(index/numbers) + '\r')
		sys.stdout.flush()
		index += 1	

	print(""《%s》下载完成！"" % name[:-4])

	
",19,0,3,0.0,28,1.0,127,0
BruteForce.py,"from itertools import product


def findPassword(chars, function, show=50, format_=""%s""):

    password = None
    attempts = 0
    size = 1
    stop = False

    while not stop:

        # Obtém todas as combinações possíveis com os dígitos do parâmetro ""chars"".
        for pw in product(chars, repeat=size):

            password = """".join(pw)

            # Imprime a senha que será tentada.
            if attempts % show == 0:
                print(format_ % password)

            # Verifica se a senha é a correta.
            if function(password):
                stop = True
                break
            else:
                attempts += 1
        size += 1

    return password, attempts


def getChars():
    """"""
    Método para obter uma lista contendo todas as
    letras do alfabeto e números.
    """"""
    chars = []

    # Acrescenta à lista todas as letras maiúsculas
    for id_ in range(ord(""A""), ord(""Z"") + 1):
        chars.append(chr(id_))

    # Acrescenta à lista todas as letras minúsculas
    for id_ in range(ord(""a""), ord(""z"") + 1):
        chars.append(chr(id_))

    # Acrescenta à lista todos os números
    for number in range(10):
        chars.append(str(number))

    return chars


# Se este módulo não for importado, o programa será testado.
# Para realizar o teste, o usuário deverá inserir uma senha para ser encontrada.

if __name__ == ""__main__"":

    import datetime
    import time

    # Pede ao usuário uma senha
    pw = input(""\n Type a password: "")
    print(""\n"")

    def testFunction(password):
        global pw
        if password == pw:
            return True
        else:
            return False

    # Obtém os dígitos que uma senha pode ter
    chars = getChars()

    t = time.process_time()

    # Obtém a senha encontrada e o múmero de tentativas
    password, attempts = findPassword(
        chars, testFunction, show=1000, format_="" Trying %s""
    )

    t = datetime.timedelta(seconds=int(time.process_time() - t))
    input(f""\n\n Password found: {password}\n Attempts: {attempts}\n Time: {t}\n"")
",12,0,3,0.0,16,1.0,45,0
build.py,"#!/usr/bin/env python3

import sys
from pathlib import Path

path = Path(__file__).resolve().parent / "".github"" / ""workflows"" / ""scripts""
sys.path.insert(0, str(path))

import ti_build.entry

sys.exit(ti_build.entry.main())
",0,0,0,0.0,0,1.0,6,0
Caesar Cipher Encoder  & Decoder.py,"# PROJECT1
# CAESAR CIPHER ENCODER/DECODER

# Author: InTruder
# Cloned from: https://github.com/InTruder-Sec/caesar-cipher

# Improved by: OfficialAhmed (https://github.com/OfficialAhmed)

def get_int() -> int:
    """"""
    Get integer, otherwise redo
    """"""

    try:
        key = int(input(""Enter number of characters you want to shift: ""))
    except:
        print(""Enter an integer"")
        key = get_int()

    return key

def main():

    print(""[>] CAESAR CIPHER DECODER!!! \n"")
    print(""[1] Encrypt\n[2] Decrypt"")

    match input(""Choose one of the above(example for encode enter 1): ""):

        case ""1"":
            encode()

        case ""2"":
            decode()

        case _:
            print(""\n[>] Invalid input. Choose 1 or 2"")
            main()


def encode():

    encoded_cipher = """"
    text = input(""Enter text to encode: "")
    key = get_int()
        
    for char in text:
        
        ascii = ord(char) + key
        encoded_cipher += chr(ascii)

    print(f""Encoded text: {encoded_cipher}"")


def decode():

    decoded_cipher = """"
    cipher = input(""\n[>] Enter your cipher text: "")
    key = get_int()

    for character in cipher:
        ascii = ord(character) - key
        decoded_cipher += chr(ascii)

    print(decoded_cipher)


if __name__ == '__main__':
    main()
",7,0,3,0.0,4,1.0,36,0
Calculate resistance.py,"def res(R1, R2): 
       sum = R1 + R2 
       if option ==""series"": 
           return sum 
       elif option ==""parallel"" : 
           return (R1 * R2)/sum
       return 0
Resistance1 = int(input(""Enter R1 : "")) 
Resistance2 = int(input(""Enter R2 : "")) 
option = input(""Enter series or parallel :"") 
print(""\n"") 
R = res(Resistance1,Resistance2 ) 
if R==0:
    print('Wrong Input!!' )
else:
    print(""The total resistance is"", R)
",3,0,3,0.0,4,1.0,16,0
Calculator with simple ui.py,"# Program make a simple calculator


class Calculator:
    def __init__(self):
        pass

    def add(self, num1, num2):
        """"""
        This function adds two numbers.

        Examples:
        >>> add(2, 3)
        5
        >>> add(5, 9)
        14
        >>> add(-1, 2)
        1
        """"""
        return num1 + num2

    def subtract(self, num1, num2):
        """"""
        This function subtracts two numbers.

        Examples:
        >>> subtract(5, 3)
        2
        >>> subtract(9, 5)
        4
        >>> subtract(4, 9)
        -5
        """"""
        return num1 - num2

    def multiply(self, num1, num2):
        """"""
        This function multiplies two numbers.

        Examples:
        >>> multiply(4, 2)
        8
        >>> multiply(3, 3)
        9
        >>> multiply(9, 9)
        81
        """"""
        return num1 * num2

    def divide(self, num1, num2):
        """"""
        This function divides two numbers.

        Examples:
        >>> divide(4, 4)
        1
        >>> divide(6, 3)
        2
        >>> divide(9, 1)
        9
        """"""
        if num2 == 0:
            print(""Cannot divide by zero"")
        else:
            return num1 / num2


calculator = Calculator()


print(""1.Add"")
print(""2.Subtract"")
print(""3.Multiply"")
print(""4.Divide"")

while True:
    # Take input from the user
    choice = input(""Enter choice(1/2/3/4): "")

    # Check if choice is one of the four options
    if choice in (""1"", ""2"", ""3"", ""4""):
        num1 = float(input(""Enter first number: ""))
        num2 = float(input(""Enter second number: ""))

        if choice == ""1"":
            print(calculator.add(num1, num2))

        elif choice == ""2"":
            print(calculator.subtract(num1, num2))

        elif choice == ""3"":
            print(calculator.multiply(num1, num2))

        elif choice == ""4"":
            print(calculator.divide(num1, num2))
        break
    else:
        print(""Invalid Input"")
",9,0,1,0.0,12,1.0,35,0
Calendar (GUI).py,"from tkinter import *
import calendar

root = Tk()
# root.geometry(""400x300"")
root.title(""Calendar"")

# Function

def text():
    month_int = int(month.get())
    year_int = int(year.get())
    cal = calendar.month(year_int, month_int)
    textfield.delete(0.0, END)
    textfield.insert(INSERT, cal)


# Creating Labels
label1 = Label(root, text=""Month:"")
label1.grid(row=0, column=0)

label2 = Label(root, text=""Year:"")
label2.grid(row=0, column=1)

# Creating spinbox
month = Spinbox(root, from_=1, to=12, width=8)
month.grid(row=1, column=0, padx=5)

year = Spinbox(root, from_=2000, to=2100, width=10)
year.grid(row=1, column=1, padx=10)

# Creating Button
button = Button(root, text=""Go"", command=text)
button.grid(row=1, column=2, padx=10)

# Creating Textfield
textfield = Text(root, width=25, height=10, fg=""red"")
textfield.grid(row=2, columnspan=2)


root.mainloop()
",1,0,1,0.0,0,1.0,23,0
changelog.py,"from git_changelog.cli import build_and_render

# 运行这段脚本自动生成CHANGELOG.md文件

build_and_render(
    repository=""."",
    output=""CHANGELOG.md"",
    convention=""angular"",
    provider=""github"",
    template=""keepachangelog"",
    parse_trailers=True,
    parse_refs=False,
    sections=[""build"", ""deps"", ""feat"", ""fix"", ""refactor""],
    versioning=""pep440"",
    bump=""1.1.2"",  # 指定bump版本
    in_place=True,
)
",0,0,0,0.0,0,1.0,14,0
Chrome Dino Automater.py,"import pyautogui  # pip install pyautogui
from PIL import Image, ImageGrab  # pip install pillow

# from numpy import asarray
import time


def hit(key):
    pyautogui.press(key)
    return


def isCollide(data):

    # for cactus
    for i in range(329, 425):
        for j in range(550, 650):
            if data[i, j] < 100:
                hit(""up"")
                return

    # Draw the rectangle for birds
    # for i in range(310, 425):
    #     for j in range(390, 550):
    #         if data[i, j] < 100:
    #             hit(""down"")
    #             return

    # return


if __name__ == ""__main__"":
    print(""Hey.. Dino game about to start in 3 seconds"")
    time.sleep(2)
    # hit('up')

    while True:
        image = ImageGrab.grab().convert(""L"")
        data = image.load()
        isCollide(data)

        # print(aarray(image))

        # Draw the rectangle for cactus
        # for i in range(315, 425):
        #     for j in range(550, 650):
        #         data[i, j] = 0

        # # # # # Draw the rectangle for birds
        # for i in range(310, 425):
        #     for j in range(390, 550):
        #         data[i, j] = 171

        # image.show()
        # break
",7,0,3,0.0,9,1.0,19,0
cleaner.py,"import pandas as pd


dataset_path = ""github_code_dataset_with_all_sonar_metrics.csv""
df_final = pd.read_csv(dataset_path)


columns_to_drop = [""repo_name"", ""file_path"", ""sonar_component_key""]


df_final = df_final.drop(columns=[col for col in columns_to_drop if col in df_final.columns], errors=""ignore"")
df_final = df_final[df_final[""short_code_snippet""].notna() & (df_final[""short_code_snippet""].str.strip() != """")]

updated_file_path = ""sonarcloud_metrics.csv""
df_final.to_csv(updated_file_path, index=False)

df = pd.read_csv(""sonarcloud_metrics.csv"")

# Create blank rows
blank_row = pd.DataFrame([[""""] * len(df.columns)], columns=df.columns)

# Insert a blank row after every row
df_spaced = pd.concat([pd.concat([df.iloc[[i]], blank_row]) for i in range(len(df))], ignore_index=True)

# Save the updated dataset
df_spaced.to_csv(""sonar_metrics.csv"", index=False)

",1,0,0,0.0,0,1.0,12,0
cli.py,"""""""
Provides a command line interface for the GPTResearcher class.

Usage:

```shell
python cli.py ""<query>"" --report_type <report_type>
```

""""""
import asyncio
import argparse
from argparse import RawTextHelpFormatter
from uuid import uuid4
import os

from dotenv import load_dotenv

from gpt_researcher import GPTResearcher
from gpt_researcher.utils.enum import ReportType, Tone
from backend.report_type import DetailedReport

# =============================================================================
# CLI
# =============================================================================

cli = argparse.ArgumentParser(
    description=""Generate a research report."",
    # Enables the use of newlines in the help message
    formatter_class=RawTextHelpFormatter)

# =====================================
# Arg: Query
# =====================================

cli.add_argument(
    # Position 0 argument
    ""query"",
    type=str,
    help=""The query to conduct research on."")

# =====================================
# Arg: Report Type
# =====================================

choices = [report_type.value for report_type in ReportType]

report_type_descriptions = {
    ReportType.ResearchReport.value: ""Summary - Short and fast (~2 min)"",
    ReportType.DetailedReport.value: ""Detailed - In depth and longer (~5 min)"",
    ReportType.ResourceReport.value: """",
    ReportType.OutlineReport.value: """",
    ReportType.CustomReport.value: """",
    ReportType.SubtopicReport.value: """"
}

cli.add_argument(
    ""--report_type"",
    type=str,
    help=""The type of report to generate. Options:\n"" + ""\n"".join(
        f""  {choice}: {report_type_descriptions[choice]}"" for choice in choices
    ),
    # Deserialize ReportType as a List of strings:
    choices=choices,
    required=True)

# First, let's see what values are actually in the Tone enum
print([t.value for t in Tone])

cli.add_argument(
    ""--tone"",
    type=str,
    help=""The tone of the report (optional)."",
    choices=[""objective"", ""formal"", ""analytical"", ""persuasive"", ""informative"", 
            ""explanatory"", ""descriptive"", ""critical"", ""comparative"", ""speculative"", 
            ""reflective"", ""narrative"", ""humorous"", ""optimistic"", ""pessimistic""],
    default=""objective""
)

# =============================================================================
# Main
# =============================================================================


async def main(args):
    """""" 
    Conduct research on the given query, generate the report, and write
    it as a markdown file to the output directory.
    """"""
    if args.report_type == 'detailed_report':
        detailed_report = DetailedReport(
            query=args.query,
            report_type=""research_report"",
            report_source=""web_search"",
        )

        report = await detailed_report.run()
    else:
        # Convert the simple keyword to the full Tone enum value
        tone_map = {
            ""objective"": Tone.Objective,
            ""formal"": Tone.Formal,
            ""analytical"": Tone.Analytical,
            ""persuasive"": Tone.Persuasive,
            ""informative"": Tone.Informative,
            ""explanatory"": Tone.Explanatory,
            ""descriptive"": Tone.Descriptive,
            ""critical"": Tone.Critical,
            ""comparative"": Tone.Comparative,
            ""speculative"": Tone.Speculative,
            ""reflective"": Tone.Reflective,
            ""narrative"": Tone.Narrative,
            ""humorous"": Tone.Humorous,
            ""optimistic"": Tone.Optimistic,
            ""pessimistic"": Tone.Pessimistic
        }

        researcher = GPTResearcher(
            query=args.query,
            report_type=args.report_type,
            tone=tone_map[args.tone]
        )

        await researcher.conduct_research()

        report = await researcher.write_report()

    # Write the report to a file
    artifact_filepath = f""outputs/{uuid4()}.md""
    os.makedirs(""outputs"", exist_ok=True)
    with open(artifact_filepath, ""w"") as f:
        f.write(report)

    print(f""Report written to '{artifact_filepath}'"")

if __name__ == ""__main__"":
    load_dotenv()
    args = cli.parse_args()
    asyncio.run(main(args))
",3,0,0,0.0,3,1.0,85,0
conanfile.py,"#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""Conan recipe package for Google FlatBuffers
""""""
import os
import shutil
from conans import ConanFile, CMake, tools


class FlatbuffersConan(ConanFile):
    name = ""flatbuffers""
    license = ""Apache-2.0""
    url = ""https://github.com/google/flatbuffers""
    homepage = ""http://google.github.io/flatbuffers/""
    author = ""Wouter van Oortmerssen""
    topics = (""conan"", ""flatbuffers"", ""serialization"", ""rpc"", ""json-parser"")
    description = ""Memory Efficient Serialization Library""
    settings = ""os"", ""compiler"", ""build_type"", ""arch""
    options = {""shared"": [True, False], ""fPIC"": [True, False]}
    default_options = {""shared"": False, ""fPIC"": True}
    generators = ""cmake""
    exports = ""LICENSE""
    exports_sources = [""CMake/*"", ""include/*"", ""src/*"", ""grpc/*"", ""CMakeLists.txt"", ""conan/CMakeLists.txt""]

    def source(self):
        """"""Wrap the original CMake file to call conan_basic_setup
        """"""
        shutil.move(""CMakeLists.txt"", ""CMakeListsOriginal.txt"")
        shutil.move(os.path.join(""conan"", ""CMakeLists.txt""), ""CMakeLists.txt"")

    def config_options(self):
        """"""Remove fPIC option on Windows platform
        """"""
        if self.settings.os == ""Windows"":
            self.options.remove(""fPIC"")

    def configure_cmake(self):
        """"""Create CMake instance and execute configure step
        """"""
        cmake = CMake(self)
        cmake.definitions[""FLATBUFFERS_BUILD_TESTS""] = False
        cmake.definitions[""FLATBUFFERS_BUILD_SHAREDLIB""] = self.options.shared
        cmake.definitions[""FLATBUFFERS_BUILD_FLATLIB""] = not self.options.shared
        cmake.configure()
        return cmake

    def build(self):
        """"""Configure, build and install FlatBuffers using CMake.
        """"""
        cmake = self.configure_cmake()
        cmake.build()

    def package(self):
        """"""Copy Flatbuffers' artifacts to package folder
        """"""
        cmake = self.configure_cmake()
        cmake.install()
        self.copy(pattern=""LICENSE"", dst=""licenses"")
        self.copy(pattern=""FindFlatBuffers.cmake"", dst=os.path.join(""lib"", ""cmake"", ""flatbuffers""), src=""CMake"")
        self.copy(pattern=""flathash*"", dst=""bin"", src=""bin"")
        self.copy(pattern=""flatc*"", dst=""bin"", src=""bin"")
        if self.settings.os == ""Windows"" and self.options.shared:
            if self.settings.compiler == ""Visual Studio"":
                shutil.move(os.path.join(self.package_folder, ""lib"", ""%s.dll"" % self.name),
                            os.path.join(self.package_folder, ""bin"", ""%s.dll"" % self.name))
            elif self.settings.compiler == ""gcc"":
                shutil.move(os.path.join(self.package_folder, ""lib"", ""lib%s.dll"" % self.name),
                            os.path.join(self.package_folder, ""bin"", ""lib%s.dll"" % self.name))

    def package_info(self):
        """"""Collect built libraries names and solve flatc path.
        """"""
        self.cpp_info.libs = tools.collect_libs(self)
        self.user_info.flatc = os.path.join(self.package_folder, ""bin"", ""flatc"")
",10,0,1,0.0,6,1.0,50,0
config.py,"# In this file, you can set the configurations of the app.

from src.utils.constants import DEBUG, ERROR, LLM_MODEL, OPENAI

#config related to logging must have prefix LOG_
LOG_LEVEL = 'ERROR'
LOG_SELENIUM_LEVEL = ERROR
LOG_TO_FILE = False
LOG_TO_CONSOLE = False

MINIMUM_WAIT_TIME_IN_SECONDS = 60

JOB_APPLICATIONS_DIR = ""job_applications""
JOB_SUITABILITY_SCORE = 7

JOB_MAX_APPLICATIONS = 5
JOB_MIN_APPLICATIONS = 1

LLM_MODEL_TYPE = 'openai'
LLM_MODEL = 'gpt-4o-mini'
# Only required for OLLAMA models
LLM_API_URL = ''
",0,0,0,0.0,0,1.0,13,0
conftest.py,"import os
import shutil
import warnings

import django


def pytest_addoption(parser):
    parser.addoption(
        ""--deprecation"",
        choices=[""all"", ""pending"", ""imminent"", ""none""],
        default=""pending"",
    )
    parser.addoption(""--postgres"", action=""store_true"")
    parser.addoption(""--elasticsearch"", action=""store_true"")


def pytest_configure(config):
    deprecation = config.getoption(""deprecation"")

    only_wagtail = r""^wagtail(\.|$)""
    if deprecation == ""all"":
        # Show all deprecation warnings from all packages
        warnings.simplefilter(""default"", DeprecationWarning)
        warnings.simplefilter(""default"", PendingDeprecationWarning)
    elif deprecation == ""pending"":
        # Show all deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
        warnings.filterwarnings(
            ""default"", category=PendingDeprecationWarning, module=only_wagtail
        )
    elif deprecation == ""imminent"":
        # Show only imminent deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
    elif deprecation == ""none"":
        # Deprecation warnings are ignored by default
        pass

    if config.getoption(""postgres""):
        os.environ[""DATABASE_ENGINE""] = ""django.db.backends.postgresql""

    # Setup django after processing the pytest arguments so that the env
    # variables are available in the settings
    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""wagtail.test.settings"")
    django.setup()

    # Activate a language: This affects HTTP header HTTP_ACCEPT_LANGUAGE sent by
    # the Django test client.
    from django.utils import translation

    translation.activate(""en"")

    from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT

    shutil.rmtree(STATIC_ROOT, ignore_errors=True)
    shutil.rmtree(MEDIA_ROOT, ignore_errors=True)


def pytest_unconfigure(config):
    from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT

    shutil.rmtree(STATIC_ROOT, ignore_errors=True)
    shutil.rmtree(MEDIA_ROOT, ignore_errors=True)
",5,0,0,0.0,5,1.0,44,0
crawl_sourcecode.py,""""""" This script can be used to test the equivalence in parsing between
rustpython and cpython.

Usage example:

$ python crawl_sourcecode.py crawl_sourcecode.py > cpython.txt
$ cargo run crawl_sourcecode.py crawl_sourcecode.py > rustpython.txt
$ diff cpython.txt rustpython.txt
""""""


import ast
import sys
import symtable
import dis

filename = sys.argv[1]
print('Crawling file:', filename)


with open(filename, 'r') as f:
    source = f.read()

t = ast.parse(source)
print(t)

shift = 3
def print_node(node, indent=0):
    indents = ' ' * indent
    if isinstance(node, ast.AST):
        lineno = 'row={}'.format(node.lineno) if hasattr(node, 'lineno') else ''
        print(indents, ""NODE"", node.__class__.__name__, lineno)
        for field in node._fields:
            print(indents,'-', field)
            f = getattr(node, field)
            if isinstance(f, list):
                for f2 in f:
                    print_node(f2, indent=indent+shift)
            else:
                print_node(f, indent=indent+shift)
    else:
        print(indents, 'OBJ', node)

print_node(t)

# print(ast.dump(t))
flag_names = [
    'is_referenced',
    'is_assigned',
    'is_global',
    'is_local',
    'is_parameter',
    'is_free',
]

def print_table(table, indent=0):
    indents = ' ' * indent
    print(indents, 'table:', table.get_name())
    print(indents, ' ', 'name:', table.get_name())
    print(indents, ' ', 'type:', table.get_type())
    print(indents, ' ', 'line:', table.get_lineno())
    print(indents, ' ', 'identifiers:', table.get_identifiers())
    print(indents, ' ', 'Syms:')
    for sym in table.get_symbols():
        flags = []
        for flag_name in flag_names:
            func = getattr(sym, flag_name)
            if func():
                flags.append(flag_name)
        print(indents, '   sym:', sym.get_name(), 'flags:', ' '.join(flags))
    if table.has_children():
        print(indents, ' ', 'Child tables:')
        for child in table.get_children():
            print_table(child, indent=indent+shift)

table = symtable.symtable(source, 'a', 'exec')
print_table(table)

print()
print('======== dis.dis ========')
print()
co = compile(source, filename, 'exec')
dis.dis(co)
",12,0,0,0.0,23,1.0,61,0
daili.py,"# -*- coding:UTF-8 -*-
from bs4 import BeautifulSoup
from selenium import webdriver
import subprocess as sp
from lxml import etree
import requests
import random
import re

""""""
函数说明:获取IP代理
Parameters:
	page - 高匿代理页数,默认获取第一页
Returns:
	proxys_list - 代理列表
Modify:
	2017-05-27
""""""
def get_proxys(page = 1):
	#requests的Session可以自动保持cookie,不需要自己维护cookie内容
	S = requests.Session()
	#西祠代理高匿IP地址
	target_url = 'http://www.xicidaili.com/nn/%d' % page
	#完善的headers
	target_headers = {'Upgrade-Insecure-Requests':'1',
		'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
		'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
		'Referer':'http://www.xicidaili.com/nn/',
		'Accept-Encoding':'gzip, deflate, sdch',
		'Accept-Language':'zh-CN,zh;q=0.8',
	}
	#get请求
	target_response = S.get(url = target_url, headers = target_headers)
	#utf-8编码
	target_response.encoding = 'utf-8'
	#获取网页信息
	target_html = target_response.text
	#获取id为ip_list的table
	bf1_ip_list = BeautifulSoup(target_html, 'lxml')
	bf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id = 'ip_list')), 'lxml')
	ip_list_info = bf2_ip_list.table.contents
	#存储代理的列表
	proxys_list = []
	#爬取每个代理信息
	for index in range(len(ip_list_info)):
		if index % 2 == 1 and index != 1:
			dom = etree.HTML(str(ip_list_info[index]))
			ip = dom.xpath('//td[2]')
			port = dom.xpath('//td[3]')
			protocol = dom.xpath('//td[6]')
			proxys_list.append(protocol[0].text.lower() + '#' + ip[0].text + '#' + port[0].text)
	#返回代理列表
	return proxys_list

""""""
函数说明:检查代理IP的连通性
Parameters:
	ip - 代理的ip地址
	lose_time - 匹配丢包数
	waste_time - 匹配平均时间
Returns:
	average_time - 代理ip平均耗时
Modify:
	2017-05-27
""""""
def check_ip(ip, lose_time, waste_time):
	#命令 -n 要发送的回显请求数 -w 等待每次回复的超时时间(毫秒)
	cmd = ""ping -n 3 -w 3 %s""
	#执行命令
	p = sp.Popen(cmd % ip, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE, shell=True) 
	#获得返回结果并解码
	out = p.stdout.read().decode(""gbk"")
	#丢包数
	lose_time = lose_time.findall(out)
	#当匹配到丢失包信息失败,默认为三次请求全部丢包,丢包数lose赋值为3
	if len(lose_time) == 0:
		lose = 3
	else:
		lose = int(lose_time[0])
	#如果丢包数目大于2个,则认为连接超时,返回平均耗时1000ms
	if lose > 2:
		#返回False
		return 1000
	#如果丢包数目小于等于2个,获取平均耗时的时间
	else:
		#平均时间
		average = waste_time.findall(out)
		#当匹配耗时时间信息失败,默认三次请求严重超时,返回平均好使1000ms
		if len(average) == 0:
			return 1000
		else:
			#
			average_time = int(average[0])
			#返回平均耗时
			return average_time

""""""
函数说明:初始化正则表达式
Parameters:
	无
Returns:
	lose_time - 匹配丢包数
	waste_time - 匹配平均时间
Modify:
	2017-05-27
""""""
def initpattern():
	#匹配丢包数
	lose_time = re.compile(u""丢失 = (\d+)"", re.IGNORECASE)
	#匹配平均时间
	waste_time = re.compile(u""平均 = (\d+)ms"", re.IGNORECASE)
	return lose_time, waste_time

if __name__ == '__main__':
	#初始化正则表达式
	lose_time, waste_time = initpattern()
	#获取IP代理
	proxys_list = get_proxys(1)

	#如果平均时间超过200ms重新选取ip
	while True:
		#从100个IP中随机选取一个IP作为代理进行访问
		proxy = random.choice(proxys_list)
		split_proxy = proxy.split('#')
		#获取IP
		ip = split_proxy[1]
		#检查ip
		average_time = check_ip(ip, lose_time, waste_time)
		if average_time > 200:
			#去掉不能使用的IP
			proxys_list.remove(proxy)
			print(""ip连接超时, 重新获取中!"")
		if average_time < 200:
			break

	#去掉已经使用的IP
	proxys_list.remove(proxy)
	proxy_dict = {split_proxy[0]:split_proxy[1] + ':' + split_proxy[2]}
	print(""使用代理:"", proxy_dict)
",13,0,0,0.0,20,1.0,100,0
dangdang_top_500.py,"import requests
import re
import json


def request_dandan(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
    except requests.RequestException as e:
        print(e)
        return None


def parse_result(html):
    pattern = re.compile(
        '<li.*?list_num.*?(\d+)\.</div>.*?<img src=""(.*?)"".*?class=""name"".*?title=""(.*?)"">.*?class=""star"">.*?class=""tuijian"">(.*?)</span>.*?class=""publisher_info"">.*?target=""_blank"">(.*?)</a>.*?class=""biaosheng"">.*?<span>(.*?)</span></div>.*?<p><span class=""price_n"">(.*?)</span>.*?</li>', re.S)
    items = re.findall(pattern, html)

    for item in items:
        yield {
            'range': item[0],
            'image': item[1],
            'title': item[2],
            'recommend': item[3],
            'author': item[4],
            'times': item[5],
            'price': item[6]
        }


def write_item_to_file(item):
    print('开始写入数据 ====> ' + str(item))
    with open('book.txt', 'a', encoding='UTF-8') as f:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')


def main(page):
    url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page)
    html = request_dandan(url)
    items = parse_result(html)  # 解析过滤我们想要的信息
    for item in items:
        write_item_to_file(item)


if __name__ == ""__main__"":
    for i in range(1, 26):
        main(i)
",9,0,0,0.0,7,1.0,38,0
demo_cli.py,"import argparse
import os
from pathlib import Path

import librosa
import numpy as np
import soundfile as sf
import torch

from encoder import inference as encoder
from encoder.params_model import model_embedding_size as speaker_embedding_size
from synthesizer.inference import Synthesizer
from utils.argutils import print_args
from utils.default_models import ensure_default_models
from vocoder import inference as vocoder


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(""-e"", ""--enc_model_fpath"", type=Path,
                        default=""saved_models/default/encoder.pt"",
                        help=""Path to a saved encoder"")
    parser.add_argument(""-s"", ""--syn_model_fpath"", type=Path,
                        default=""saved_models/default/synthesizer.pt"",
                        help=""Path to a saved synthesizer"")
    parser.add_argument(""-v"", ""--voc_model_fpath"", type=Path,
                        default=""saved_models/default/vocoder.pt"",
                        help=""Path to a saved vocoder"")
    parser.add_argument(""--cpu"", action=""store_true"", help=\
        ""If True, processing is done on CPU, even when a GPU is available."")
    parser.add_argument(""--no_sound"", action=""store_true"", help=\
        ""If True, audio won't be played."")
    parser.add_argument(""--seed"", type=int, default=None, help=\
        ""Optional random number seed value to make toolbox deterministic."")
    args = parser.parse_args()
    arg_dict = vars(args)
    print_args(args, parser)

    # Hide GPUs from Pytorch to force CPU processing
    if arg_dict.pop(""cpu""):
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

    print(""Running a test of your configuration...\n"")

    if torch.cuda.is_available():
        device_id = torch.cuda.current_device()
        gpu_properties = torch.cuda.get_device_properties(device_id)
        ## Print some environment information (for debugging purposes)
        print(""Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with ""
            ""%.1fGb total memory.\n"" %
            (torch.cuda.device_count(),
            device_id,
            gpu_properties.name,
            gpu_properties.major,
            gpu_properties.minor,
            gpu_properties.total_memory / 1e9))
    else:
        print(""Using CPU for inference.\n"")

    ## Load the models one by one.
    print(""Preparing the encoder, the synthesizer and the vocoder..."")
    ensure_default_models(Path(""saved_models""))
    encoder.load_model(args.enc_model_fpath)
    synthesizer = Synthesizer(args.syn_model_fpath)
    vocoder.load_model(args.voc_model_fpath)


    ## Run a test
    print(""Testing your configuration with small inputs."")
    # Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's
    # sampling rate, which may differ.
    # If you're unfamiliar with digital audio, know that it is encoded as an array of floats
    # (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.
    # The sampling rate is the number of values (samples) recorded per second, it is set to
    # 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond
    # to an audio of 1 second.
    print(""\tTesting the encoder..."")
    encoder.embed_utterance(np.zeros(encoder.sampling_rate))

    # Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance
    # returns, but here we're going to make one ourselves just for the sake of showing that it's
    # possible.
    embed = np.random.rand(speaker_embedding_size)
    # Embeddings are L2-normalized (this isn't important here, but if you want to make your own
    # embeddings it will be).
    embed /= np.linalg.norm(embed)
    # The synthesizer can handle multiple inputs with batching. Let's create another embedding to
    # illustrate that
    embeds = [embed, np.zeros(speaker_embedding_size)]
    texts = [""test 1"", ""test 2""]
    print(""\tTesting the synthesizer... (loading the model will output a lot of text)"")
    mels = synthesizer.synthesize_spectrograms(texts, embeds)

    # The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We
    # can concatenate the mel spectrograms to a single one.
    mel = np.concatenate(mels, axis=1)
    # The vocoder can take a callback function to display the generation. More on that later. For
    # now we'll simply hide it like this:
    no_action = lambda *args: None
    print(""\tTesting the vocoder..."")
    # For the sake of making this test short, we'll pass a short target length. The target length
    # is the length of the wav segments that are processed in parallel. E.g. for audio sampled
    # at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of
    # 0.5 seconds which will all be generated together. The parameters here are absurdly short, and
    # that has a detrimental effect on the quality of the audio. The default parameters are
    # recommended in general.
    vocoder.infer_waveform(mel, target=200, overlap=50, progress_callback=no_action)

    print(""All test passed! You can now synthesize speech.\n\n"")


    ## Interactive speech generation
    print(""This is a GUI-less example of interface to SV2TTS. The purpose of this script is to ""
          ""show how you can interface this project easily with your own. See the source code for ""
          ""an explanation of what is happening.\n"")

    print(""Interactive generation loop"")
    num_generated = 0
    while True:
        try:
            # Get the reference audio filepath
            message = ""Reference voice: enter an audio filepath of a voice to be cloned (mp3, "" \
                      ""wav, m4a, flac, ...):\n""
            in_fpath = Path(input(message).replace(""\"""", """").replace(""\'"", """"))

            ## Computing the embedding
            # First, we load the wav using the function that the speaker encoder provides. This is
            # important: there is preprocessing that must be applied.

            # The following two methods are equivalent:
            # - Directly load from the filepath:
            preprocessed_wav = encoder.preprocess_wav(in_fpath)
            # - If the wav is already loaded:
            original_wav, sampling_rate = librosa.load(str(in_fpath))
            preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
            print(""Loaded file succesfully"")

            # Then we derive the embedding. There are many functions and parameters that the
            # speaker encoder interfaces. These are mostly for in-depth research. You will typically
            # only use this function (with its default parameters):
            embed = encoder.embed_utterance(preprocessed_wav)
            print(""Created the embedding"")


            ## Generating the spectrogram
            text = input(""Write a sentence (+-20 words) to be synthesized:\n"")

            # If seed is specified, reset torch seed and force synthesizer reload
            if args.seed is not None:
                torch.manual_seed(args.seed)
                synthesizer = Synthesizer(args.syn_model_fpath)

            # The synthesizer works in batch, so you need to put your data in a list or numpy array
            texts = [text]
            embeds = [embed]
            # If you know what the attention layer alignments are, you can retrieve them here by
            # passing return_alignments=True
            specs = synthesizer.synthesize_spectrograms(texts, embeds)
            spec = specs[0]
            print(""Created the mel spectrogram"")


            ## Generating the waveform
            print(""Synthesizing the waveform:"")

            # If seed is specified, reset torch seed and reload vocoder
            if args.seed is not None:
                torch.manual_seed(args.seed)
                vocoder.load_model(args.voc_model_fpath)

            # Synthesizing the waveform is fairly straightforward. Remember that the longer the
            # spectrogram, the more time-efficient the vocoder.
            generated_wav = vocoder.infer_waveform(spec)


            ## Post-generation
            # There's a bug with sounddevice that makes the audio cut one second earlier, so we
            # pad it.
            generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=""constant"")

            # Trim excess silences to compensate for gaps in spectrograms (issue #53)
            generated_wav = encoder.preprocess_wav(generated_wav)

            # Play the audio (non-blocking)
            if not args.no_sound:
                import sounddevice as sd
                try:
                    sd.stop()
                    sd.play(generated_wav, synthesizer.sample_rate)
                except sd.PortAudioError as e:
                    print(""\nCaught exception: %s"" % repr(e))
                    print(""Continuing without audio playback. Suppress this message with the \""--no_sound\"" flag.\n"")
                except:
                    raise

            # Save it on the disk
            filename = ""demo_output_%02d.wav"" % num_generated
            print(generated_wav.dtype)
            sf.write(filename, generated_wav.astype(np.float32), synthesizer.sample_rate)
            num_generated += 1
            print(""\nSaved output as %s\n\n"" % filename)


        except Exception as e:
            print(""Caught exception: %s"" % repr(e))
            print(""Restarting\n"")
",7,0,2,0.0,28,1.0,120,0
demo_closures.py,"

def foo(x):
    def bar(z):
        return z + x
    return bar

f = foo(9)
g = foo(10)

print(f(2))
print(g(2))

",2,0,0,0.0,0,1.0,8,0
demo_toolbox.py,"import argparse
import os
from pathlib import Path

from toolbox import Toolbox
from utils.argutils import print_args
from utils.default_models import ensure_default_models


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=""Runs the toolbox."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(""-d"", ""--datasets_root"", type=Path, help= \
        ""Path to the directory containing your datasets. See toolbox/__init__.py for a list of ""
        ""supported datasets."", default=None)
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"",
                        help=""Directory containing all saved models"")
    parser.add_argument(""--cpu"", action=""store_true"", help=\
        ""If True, all inference will be done on CPU"")
    parser.add_argument(""--seed"", type=int, default=None, help=\
        ""Optional random number seed value to make toolbox deterministic."")
    args = parser.parse_args()
    arg_dict = vars(args)
    print_args(args, parser)

    # Hide GPUs from Pytorch to force CPU processing
    if arg_dict.pop(""cpu""):
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

    # Remind the user to download pretrained models if needed
    ensure_default_models(args.models_dir)

    # Launch the toolbox
    Toolbox(**arg_dict)
",2,0,0,0.0,3,1.0,27,0
douban_top_250_books.py,"import requests
from bs4 import BeautifulSoup
import xlwt


def request_douban(url):
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/88.0.4324.146 Safari/537.36',
    }

    try:
        response = requests.get(url=url, headers=headers)
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None


book = xlwt.Workbook(encoding='utf-8', style_compression=0)

sheet = book.add_sheet('豆瓣电影Top250', cell_overwrite_ok=True)
sheet.write(0, 0, '名称')
sheet.write(0, 1, '图片')
sheet.write(0, 2, '排名')
sheet.write(0, 3, '评分')
sheet.write(0, 4, '作者')
sheet.write(0, 5, '简介')

n = 1


def save_to_excel(soup):
    list = soup.find(class_='grid_view').find_all('li')

    for item in list:
        item_name = item.find(class_='title').string
        item_img = item.find('a').find('img').get('src')
        item_index = item.find(class_='').string
        item_score = item.find(class_='rating_num').string
        item_author = item.find('p').text
        if item.find(class_='inq') is not None:
            item_intr = item.find(class_='inq').string
        else:
            item_intr = 'NOT AVAILABLE'        

        # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_img +' | ' + item_score +' | ' + item_author +' | ' + item_intr )
        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)

        global n

        sheet.write(n, 0, item_name)
        sheet.write(n, 1, item_img)
        sheet.write(n, 2, item_index)
        sheet.write(n, 3, item_score)
        sheet.write(n, 4, item_author)
        sheet.write(n, 5, item_intr)

        n = n + 1


def main(page):
    url = 'https://movie.douban.com/top250?start=' + str(page * 25) + '&filter='
    html = request_douban(url)
    soup = BeautifulSoup(html, 'lxml')
    save_to_excel(soup)


if __name__ == '__main__':

    for i in range(0, 10):
        main(i)

book.save(u'豆瓣最受欢迎的250部电影.xlsx')
",,0,0,0.0,0,1.0,,0
douban_top_250_books_mul_process.py,"import requests
from bs4 import BeautifulSoup
import xlwt
import multiprocessing
import time
import sys

def request_douban(url):
    try:
        response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None

def main(url):
    sys.setrecursionlimit(1000000)
    data = []
    html = request_douban(url)
    # soup = BeautifulSoup(html, 'lxml')
    soup = BeautifulSoup(html, 'html.parser')
    list = soup.find(class_='grid_view').find_all('li')
    for item in list:
        item_name = item.find(class_='title').string
        item_img = item.find('a').find('img').get('src')
        item_index = item.find(class_='').string
        item_score = item.find(class_='rating_num').string
        item_author = item.find('p').text
        item_intr = ''
        if (item.find(class_='inq') != None):
            item_intr = item.find(class_='inq').string
        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)
        item = {
            'item_index': item_index,
            'item_name': item_name,
            'item_score': item_score,
            'item_intr': item_intr,
            'item_img': item_img,
            'item_author': item_author
        }
        data.append(item)
    return data
    
if __name__ == '__main__':
    startTime = time.time()
    data = []
    urls = []
    pool = multiprocessing.Pool(multiprocessing.cpu_count()-1)
    for i in range(0, 10):
        url = 'https://movie.douban.com/top250?start=' + str(i * 25) + '&filter='
        urls.append(url)
    pool.map(main, urls)
    for pageItem in pool.map(main, urls):
        data.extend(pageItem)
    book = xlwt.Workbook(encoding='utf-8', style_compression=0)
    sheet = book.add_sheet('豆瓣电影Top250-test', cell_overwrite_ok=True)
    sheet.write(0, 0, '名称')
    sheet.write(0, 1, '图片')
    sheet.write(0, 2, '排名')
    sheet.write(0, 3, '评分')
    sheet.write(0, 4, '作者')
    sheet.write(0, 5, '简介')
    for index,item in enumerate(data):
        sheet.write(index+1, 0, item['item_name'])
        sheet.write(index+1, 1, item['item_img'])
        sheet.write(index+1, 2, item['item_index'])
        sheet.write(index+1, 3, item['item_score'])
        sheet.write(index+1, 4, item['item_author'])
        sheet.write(index+1, 5, item['item_intr'])
    book.save(u'豆瓣最受欢迎的250部电影-mul.xlsx')

    endTime = time.time()
    dtime = endTime - startTime
    print(""程序运行时间：%s s"" % dtime)  # 4.036666631698608 s",9,0,2,0.0,12,1.0,69,0
douyin.py,"# -*- coding:utf-8 -*-
from bs4 import BeautifulSoup
from contextlib import closing
import requests, json, time, re, os, sys, time

class DouYin(object):
	def __init__(self):
		""""""
		抖音App视频下载
		""""""
		#SSL认证
		pass

	def get_video_urls(self, user_id):
		""""""
		获得视频播放地址
		Parameters:
			nickname：查询的用户名
		Returns:
			video_names: 视频名字列表
			video_urls: 视频链接列表
			aweme_count: 视频数量
		""""""
		video_names = []
		video_urls = []
		unique_id = ''
		while unique_id != user_id:
			search_url = 'https://api.amemv.com/aweme/v1/discover/search/?cursor=0&keyword=%s&count=10&type=1&retry_type=no_retry&iid=17900846586&device_id=34692364855&ac=wifi&channel=xiaomi&aid=1128&app_name=aweme&version_code=162&version_name=1.6.2&device_platform=android&ssmix=a&device_type=MI+5&device_brand=Xiaomi&os_api=24&os_version=7.0&uuid=861945034132187&openudid=dc451556fc0eeadb&manifest_version_code=162&resolution=1080*1920&dpi=480&update_version_code=1622' % user_id
			req = requests.get(url = search_url, verify = False)
			html = json.loads(req.text)
			aweme_count = html['user_list'][0]['user_info']['aweme_count']
			uid = html['user_list'][0]['user_info']['uid']
			nickname = html['user_list'][0]['user_info']['nickname']
			unique_id = html['user_list'][0]['user_info']['unique_id']
		user_url = 'https://www.douyin.com/aweme/v1/aweme/post/?user_id=%s&max_cursor=0&count=%s' % (uid, aweme_count)
		req = requests.get(url = user_url, verify = False)
		html = json.loads(req.text)
		i = 1
		for each in html['aweme_list']:
			share_desc = each['share_info']['share_desc']
			if '抖音-原创音乐短视频社区' == share_desc:
				video_names.append(str(i) + '.mp4')
				i += 1
			else:
				video_names.append(share_desc + '.mp4')
			video_urls.append(each['share_info']['share_url'])

		return video_names, video_urls, nickname

	def get_download_url(self, video_url):
		""""""
		获得视频播放地址
		Parameters:
			video_url：视频播放地址
		Returns:
			download_url: 视频下载地址
		""""""
		req = requests.get(url = video_url, verify = False)
		bf = BeautifulSoup(req.text, 'lxml')
		script = bf.find_all('script')[-1]
		video_url_js = re.findall('var data = \[(.+)\];', str(script))[0]
		video_html = json.loads(video_url_js)
		download_url = video_html['video']['play_addr']['url_list'][0]
		return download_url

	def video_downloader(self, video_url, video_name):
		""""""
		视频下载
		Parameters:
			None
		Returns:
			None
		""""""
		size = 0
		with closing(requests.get(video_url, stream=True, verify = False)) as response:
			chunk_size = 1024
			content_size = int(response.headers['content-length']) 
			if response.status_code == 200:
				sys.stdout.write('  [文件大小]:%0.2f MB\n' % (content_size / chunk_size / 1024))

				with open(video_name, ""wb"") as file:  
					for data in response.iter_content(chunk_size = chunk_size):
						file.write(data)
						size += len(data)
						file.flush()

					sys.stdout.write('    [下载进度]:%.2f%%' % float(size / content_size * 100))
					sys.stdout.flush()
		time.sleep(1)


	def run(self):
		""""""
		运行函数
		Parameters:
			None
		Returns:
			None
		""""""
		self.hello()
		# user_id = input('请输入ID(例如13978338):')
		user_id = 'sm666888'
		video_names, video_urls, nickname = self.get_video_urls(user_id)
		if nickname not in os.listdir():
			os.mkdir(nickname)
		sys.stdout.write('视频下载中:\n')
		for num in range(len(video_urls)):
			print('  %s\n' % video_urls[num])
			video_url = self.get_download_url(video_urls[num])
			if '\\' in video_names[num]:
				video_name = video_names[num].replace('\\', '')
			elif '/' in video_names[num]:
				video_name = video_names[num].replace('/', '')
			else:
				video_name = video_names[num]
			self.video_downloader(video_url, os.path.join(nickname, video_name))
			print('')

	def hello(self):
		""""""
		打印欢迎界面
		Parameters:
			None
		Returns:
			None
		""""""
		print('*' * 100)
		print('\t\t\t\t抖音App视频下载小助手')
		print('*' * 100)

		
if __name__ == '__main__':
	douyin = DouYin()
	douyin.run()",15,0,1,20.1,15,4.0,79,4
douyin_pro.py,"# -*- coding:utf-8 -*-
from splinter.driver.webdriver.chrome import Options, Chrome
from splinter.browser import Browser
from contextlib import closing
import requests, json, time, re, os, sys, time
from bs4 import BeautifulSoup

class DouYin(object):
	def __init__(self, width = 500, height = 300):
		""""""
		抖音App视频下载
		""""""
		# 无头浏览器
		chrome_options = Options()
		chrome_options.add_argument('user-agent=""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36""')
		self.driver = Browser(driver_name='chrome', executable_path='D:/chromedriver', options=chrome_options, headless=True)

	def get_video_urls(self, user_id):
		""""""
		获得视频播放地址
		Parameters:
			user_id：查询的用户ID
		Returns:
			video_names: 视频名字列表
			video_urls: 视频链接列表
			nickname: 用户昵称
		""""""
		video_names = []
		video_urls = []
		unique_id = ''
		while unique_id != user_id:
			search_url = 'https://api.amemv.com/aweme/v1/discover/search/?cursor=0&keyword=%s&count=10&type=1&retry_type=no_retry&iid=17900846586&device_id=34692364855&ac=wifi&channel=xiaomi&aid=1128&app_name=aweme&version_code=162&version_name=1.6.2&device_platform=android&ssmix=a&device_type=MI+5&device_brand=Xiaomi&os_api=24&os_version=7.0&uuid=861945034132187&openudid=dc451556fc0eeadb&manifest_version_code=162&resolution=1080*1920&dpi=480&update_version_code=1622' % user_id
			req = requests.get(url = search_url, verify = False)
			html = json.loads(req.text)
			aweme_count = html['user_list'][0]['user_info']['aweme_count']
			uid = html['user_list'][0]['user_info']['uid']
			nickname = html['user_list'][0]['user_info']['nickname']
			unique_id = html['user_list'][0]['user_info']['unique_id']
		user_url = 'https://www.douyin.com/aweme/v1/aweme/post/?user_id=%s&max_cursor=0&count=%s' % (uid, aweme_count)
		req = requests.get(url = user_url, verify = False)
		html = json.loads(req.text)
		i = 1
		for each in html['aweme_list']:
			share_desc = each['share_info']['share_desc']
			if '抖音-原创音乐短视频社区' == share_desc:
				video_names.append(str(i) + '.mp4')
				i += 1
			else:
				video_names.append(share_desc + '.mp4')
			video_urls.append(each['share_info']['share_url'])

		return video_names, video_urls, nickname

	def get_download_url(self, video_url):
		""""""
		获得带水印的视频播放地址
		Parameters:
			video_url：带水印的视频播放地址
		Returns:
			download_url: 带水印的视频下载地址
		""""""
		req = requests.get(url = video_url, verify = False)
		bf = BeautifulSoup(req.text, 'lxml')
		script = bf.find_all('script')[-1]
		video_url_js = re.findall('var data = \[(.+)\];', str(script))[0]
		video_html = json.loads(video_url_js)
		download_url = video_html['video']['play_addr']['url_list'][0]
		return download_url

	def video_downloader(self, video_url, video_name, watermark_flag=True):
		""""""
		视频下载
		Parameters:
			video_url: 带水印的视频地址
			video_name: 视频名
			watermark_flag: 是否下载不带水印的视频
		Returns:
			无
		""""""
		size = 0
		if watermark_flag == True:
			video_url = self.remove_watermark(video_url)
		else:
			video_url = self.get_download_url(video_url)
		with closing(requests.get(video_url, stream=True, verify = False)) as response:
			chunk_size = 1024
			content_size = int(response.headers['content-length']) 
			if response.status_code == 200:
				sys.stdout.write('  [文件大小]:%0.2f MB\n' % (content_size / chunk_size / 1024))

				with open(video_name, ""wb"") as file:  
					for data in response.iter_content(chunk_size = chunk_size):
						file.write(data)
						size += len(data)
						file.flush()

						sys.stdout.write('  [下载进度]:%.2f%%' % float(size / content_size * 100) + '\r')
						sys.stdout.flush()


	def remove_watermark(self, video_url):
		""""""
		获得无水印的视频播放地址
		Parameters:
			video_url: 带水印的视频地址
		Returns:
			无水印的视频下载地址
		""""""
		self.driver.visit('http://douyin.iiilab.com/')
		self.driver.find_by_tag('input').fill(video_url)
		self.driver.find_by_xpath('//button[@class=""btn btn-default""]').click()
		html = self.driver.find_by_xpath('//div[@class=""thumbnail""]/div/p')[0].html
		bf = BeautifulSoup(html, 'lxml')
		return bf.find('a').get('href')

	def run(self):
		""""""
		运行函数
		Parameters:
			None
		Returns:
			None
		""""""
		self.hello()
		user_id = input('请输入ID(例如40103580):')
		video_names, video_urls, nickname = self.get_video_urls(user_id)
		if nickname not in os.listdir():
			os.mkdir(nickname)
		print('视频下载中:共有%d个作品!\n' % len(video_urls))
		for num in range(len(video_urls)):
			print('  解析第%d个视频链接 [%s] 中，请稍后!\n' % (num+1, video_urls[num]))
			if '\\' in video_names[num]:
				video_name = video_names[num].replace('\\', '')
			elif '/' in video_names[num]:
				video_name = video_names[num].replace('/', '')
			else:
				video_name = video_names[num]
			self.video_downloader(video_urls[num], os.path.join(nickname, video_name))
			print('\n')

		print('下载完成!')

	def hello(self):
		""""""
		打印欢迎界面
		Parameters:
			None
		Returns:
			None
		""""""
		print('*' * 100)
		print('\t\t\t\t抖音App视频下载小助手')
		print('\t\t作者:Jack Cui')
		print('*' * 100)


if __name__ == '__main__':
	douyin = DouYin()
	douyin.run()
",17,0,0,16.9,17,4.0,94,4
download_checks.py,"{'https://s3.amazonaws.com/fast-ai-imageclas/caltech_101.tgz': (131740031,
                                                                'd673425306e98ee4619fcdeef8a0e876'),
 'https://s3.amazonaws.com/fast-ai-imagelocal/biwi_head_pose.tgz': (452316199,
                                                                    '00f4ccf66e8cba184bc292fdc08fb237'),
 'https://s3.amazonaws.com/fast-ai-imagelocal/camvid.tgz': (598913237,
                                                            '648371e4f3a833682afb39b08a3ce2aa'),
 'https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz': (11784419,
                                                          'b86f328f4dbd072486591cb7a5644dcd'),
 'https://s3.amazonaws.com/fast-ai-nlp/amazon_review_full_csv.tgz': (71606272,
                                                                     '4a1196cf0adaea22f4bc3f592cddde90'),
 'https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz': (688339454,
                                                                         '676f7e5208ec343c8274b4bb085bc938'),
 'https://s3.amazonaws.com/fast-ai-sample/adult_sample.tgz': (968212,
                                                              '64eb9d7e23732de0b138f7372d15492f'),
 'https://s3.amazonaws.com/fast-ai-sample/biwi_sample.tgz': (593774,
                                                             '9179f4c1435f4b291f0d5b072d60c2c9'),
 'https://s3.amazonaws.com/fast-ai-sample/camvid_tiny.tgz': (2314212,
                                                             '2cf6daf91b7a2083ecfa3e9968e9d915')}",0,1,0,0.0,0,1.0,18,0
downloader.py,"#-*- coding: UTF-8 -*-
import requests  
from contextlib import closing

class ProgressBar(object):  
    def __init__(self, title, count=0.0, run_status=None, fin_status=None, total=100.0, unit='', sep='/', chunk_size=1.0):  
        super(ProgressBar, self).__init__()  
        self.info = ""[%s] %s %.2f %s %s %.2f %s""  
        self.title = title  
        self.total = total  
        self.count = count  
        self.chunk_size = chunk_size  
        self.status = run_status or """"  
        self.fin_status = fin_status or "" "" * len(self.status)  
        self.unit = unit  
        self.seq = sep  
  
    def __get_info(self):  
        #[名称] 状态 进度 单位 分割线 总数 单位  
        _info = self.info % (self.title, self.status, self.count/self.chunk_size, self.unit, self.seq, self.total/self.chunk_size, self.unit)  
        return _info  
  
    def refresh(self, count = 1, status = None):  
        self.count += count  
        self.status = status or self.status  
        end_str = ""\r""  
        if self.count >= self.total:  
            end_str = '\n'  
            self.status = status or self.fin_status  
        print(self.__get_info(), end=end_str, )  


if __name__ == '__main__':
	#url = 'http://www.demongan.com/source/game/二十四点.zip'
	#filename = '二十四点.zip'
	print('*' * 100)
	print('\t\t\t\t欢迎使用文件下载小助手')
	print('作者:Jack-Cui\n博客:http://blog.csdn.net/c406495762')
	print('*' * 100)
	url  = input('请输入需要下载的文件链接:\n')
	filename = url.split('/')[-1]
	with closing(requests.get(url, stream=True)) as response:  
		chunk_size = 1024  
		content_size = int(response.headers['content-length'])  
		if response.status_code == 200:
			print('文件大小:%0.2f KB' % (content_size / chunk_size))
			progress = ProgressBar(""%s下载进度"" % filename
			            , total = content_size  
			            , unit = ""KB""  
			            , chunk_size = chunk_size  
			            , run_status = ""正在下载""  
			            , fin_status = ""下载完成"")  

			with open(filename, ""wb"") as file:  
			        for data in response.iter_content(chunk_size=chunk_size):  
			            file.write(data)  
			            progress.refresh(count=len(data))  
		else:
			print('链接异常')",11,0,1,0.0,12,1.0,49,0
encoder_preprocess.py,"from encoder.preprocess import preprocess_librispeech, preprocess_voxceleb1, preprocess_voxceleb2
from utils.argutils import print_args
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
        pass

    parser = argparse.ArgumentParser(
        description=""Preprocesses audio files from datasets, encodes them as mel spectrograms and ""
                    ""writes them to the disk. This will allow you to train the encoder. The ""
                    ""datasets required are at least one of VoxCeleb1, VoxCeleb2 and LibriSpeech. ""
                    ""Ideally, you should have all three. You should extract them as they are ""
                    ""after having downloaded them and put them in a same directory, e.g.:\n""
                    ""-[datasets_root]\n""
                    ""  -LibriSpeech\n""
                    ""    -train-other-500\n""
                    ""  -VoxCeleb1\n""
                    ""    -wav\n""
                    ""    -vox1_meta.csv\n""
                    ""  -VoxCeleb2\n""
                    ""    -dev"",
        formatter_class=MyFormatter
    )
    parser.add_argument(""datasets_root"", type=Path, help=\
        ""Path to the directory containing your LibriSpeech/TTS and VoxCeleb datasets."")
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\
        ""Path to the output directory that will contain the mel spectrograms. If left out, ""
        ""defaults to <datasets_root>/SV2TTS/encoder/"")
    parser.add_argument(""-d"", ""--datasets"", type=str,
                        default=""librispeech_other,voxceleb1,voxceleb2"", help=\
        ""Comma-separated list of the name of the datasets you want to preprocess. Only the train ""
        ""set of these datasets will be used. Possible names: librispeech_other, voxceleb1, ""
        ""voxceleb2."")
    parser.add_argument(""-s"", ""--skip_existing"", action=""store_true"", help=\
        ""Whether to skip existing output files with the same name. Useful if this script was ""
        ""interrupted."")
    parser.add_argument(""--no_trim"", action=""store_true"", help=\
        ""Preprocess audio without trimming silences (not recommended)."")
    args = parser.parse_args()

    # Verify webrtcvad is available
    if not args.no_trim:
        try:
            import webrtcvad
        except:
            raise ModuleNotFoundError(""Package 'webrtcvad' not found. This package enables ""
                ""noise removal and is recommended. Please install and try again. If installation fails, ""
                ""use --no_trim to disable this error message."")
    del args.no_trim

    # Process the arguments
    args.datasets = args.datasets.split("","")
    if not hasattr(args, ""out_dir""):
        args.out_dir = args.datasets_root.joinpath(""SV2TTS"", ""encoder"")
    assert args.datasets_root.exists()
    args.out_dir.mkdir(exist_ok=True, parents=True)

    # Preprocess the datasets
    print_args(args, parser)
    preprocess_func = {
        ""librispeech_other"": preprocess_librispeech,
        ""voxceleb1"": preprocess_voxceleb1,
        ""voxceleb2"": preprocess_voxceleb2,
    }
    args = vars(args)
    for dataset in args.pop(""datasets""):
        print(""Preprocessing %s"" % dataset)
        preprocess_func[dataset](**args)
",4,0,1,0.0,10,1.0,62,0
encoder_train.py,"from utils.argutils import print_args
from encoder.train import train
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Trains the speaker encoder. You must have run encoder_preprocess.py first."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(""run_id"", type=str, help= \
        ""Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state ""
        ""from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved ""
        ""states and restart from scratch."")
    parser.add_argument(""clean_data_root"", type=Path, help= \
        ""Path to the output directory of encoder_preprocess.py. If you left the default ""
        ""output directory when preprocessing, it should be <datasets_root>/SV2TTS/encoder/."")
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"", help=\
        ""Path to the root directory that contains all models. A directory <run_name> will be created under this root.""
        ""It will contain the saved model weights, as well as backups of those weights and plots generated during ""
        ""training."")
    parser.add_argument(""-v"", ""--vis_every"", type=int, default=10, help= \
        ""Number of steps between updates of the loss and the plots."")
    parser.add_argument(""-u"", ""--umap_every"", type=int, default=100, help= \
        ""Number of steps between updates of the umap projection. Set to 0 to never update the ""
        ""projections."")
    parser.add_argument(""-s"", ""--save_every"", type=int, default=500, help= \
        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""
        ""model."")
    parser.add_argument(""-b"", ""--backup_every"", type=int, default=7500, help= \
        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""
        ""model."")
    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \
        ""Do not load any saved model."")
    parser.add_argument(""--visdom_server"", type=str, default=""http://localhost"")
    parser.add_argument(""--no_visdom"", action=""store_true"", help= \
        ""Disable visdom."")
    args = parser.parse_args()

    # Run the training
    print_args(args, parser)
    train(**vars(args))
",1,0,0,0.0,1,1.0,39,0
financical.py,"#-*- coding:UTF-8 -*-
import sys
import pymysql
import requests
import json
import re
from bs4 import BeautifulSoup

""""""
类说明:获取财务数据

Author:
	Jack Cui
Blog:
	http://blog.csdn.net/c406495762
Zhihu:
	https://www.zhihu.com/people/Jack--Cui/
Modify:
	2017-08-31
""""""
class FinancialData():

	def __init__(self):
		#服务器域名
		self.server = 'http://quotes.money.163.com/'
		self.cwnb = 'http://quotes.money.163.com/hkstock/cwsj_'
		#主要财务指标
		self.cwzb_dict = {'EPS':'基本每股收益','EPS_DILUTED':'摊薄每股收益','GROSS_MARGIN':'毛利率',
		'CAPITAL_ADEQUACY':'资本充足率','LOANS_DEPOSITS':'贷款回报率','ROTA':'总资产收益率',
		'ROEQUITY':'净资产收益率','CURRENT_RATIO':'流动比率','QUICK_RATIO':'速动比率',
		'ROLOANS':'存贷比','INVENTORY_TURNOVER':'存货周转率','GENERAL_ADMIN_RATIO':'管理费用比率',
		'TOTAL_ASSET2TURNOVER':'资产周转率','FINCOSTS_GROSSPROFIT':'财务费用比率','TURNOVER_CASH':'销售现金比率','YEAREND_DATE':'报表日期'}
		#利润表
		self.lrb_dict = {'TURNOVER':'总营收','OPER_PROFIT':'经营利润','PBT':'除税前利润',
		'NET_PROF':'净利润','EPS':'每股基本盈利','DPS':'每股派息',
		'INCOME_INTEREST':'利息收益','INCOME_NETTRADING':'交易收益','INCOME_NETFEE':'费用收益','YEAREND_DATE':'报表日期'}
		#资产负债表
		self.fzb_dict = {
			'FIX_ASS':'固定资产','CURR_ASS':'流动资产','CURR_LIAB':'流动负债',
			'INVENTORY':'存款','CASH':'现金及银行存结','OTHER_ASS':'其他资产',
			'TOTAL_ASS':'总资产','TOTAL_LIAB':'总负债','EQUITY':'股东权益',
			'CASH_SHORTTERMFUND':'库存现金及短期资金','DEPOSITS_FROM_CUSTOMER':'客户存款',
			'FINANCIALASSET_SALE':'可供出售之证券','LOAN_TO_BANK':'银行同业存款及贷款',
			'DERIVATIVES_LIABILITIES':'金融负债','DERIVATIVES_ASSET':'金融资产','YEAREND_DATE':'报表日期'}
		#现金流表
		self.llb_dict = {
			'CF_NCF_OPERACT':'经营活动产生的现金流','CF_INT_REC':'已收利息','CF_INT_PAID':'已付利息',
			'CF_INT_REC':'已收股息','CF_DIV_PAID':'已派股息','CF_INV':'投资活动产生现金流',
			'CF_FIN_ACT':'融资活动产生现金流','CF_BEG':'期初现金及现金等价物','CF_CHANGE_CSH':'现金及现金等价物净增加额',
			'CF_END':'期末现金及现金等价物','CF_EXCH':'汇率变动影响','YEAREND_DATE':'报表日期'}
		#总表
		self.table_dict = {'cwzb':self.cwzb_dict,'lrb':self.lrb_dict,'fzb':self.fzb_dict,'llb':self.llb_dict}
		#请求头
		self.headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
			'Accept-Encoding': 'gzip, deflate',
			'Accept-Language': 'zh-CN,zh;q=0.8',
			'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'}
	
	""""""
	函数说明:获取股票页面信息

	Author:
		Jack Cui
	Parameters:
	    url - 股票财务数据界面地址
	Returns:
	    name - 股票名
	    table_name_list - 财务报表名称
	    table_date_list - 财务报表年限
	    url_list - 财务报表查询连接
	Blog:
		http://blog.csdn.net/c406495762
	Zhihu:
		https://www.zhihu.com/people/Jack--Cui/
	Modify:
		2017-08-31
	""""""
	def get_informations(self, url):
		req = requests.get(url = url, headers = self.headers)
		req.encoding = 'utf-8'
		html = req.text
		page_bf = BeautifulSoup(html, 'lxml')
		#股票名称，股票代码
		name = page_bf.find_all('span', class_ = 'name')[0].string
		# code = page_bf.find_all('span', class_ = 'code')[0].string
		# code = re.findall('\d+',code)[0]

		#存储各个表名的列表
		table_name_list = []
		table_date_list = []
		each_date_list = []
		url_list = []
		#表名和表时间
		table_name = page_bf.find_all('div', class_ = 'titlebar3')
		for each_table_name in table_name:
			#表名
			table_name_list.append(each_table_name.span.string)
			#表时间
			for each_table_date in each_table_name.div.find_all('select', id = re.compile('.+1$')):
				url_list.append(re.findall('(\w+)1',each_table_date.get('id'))[0])
				for each_date in each_table_date.find_all('option'):
					each_date_list.append(each_date.string)
				table_date_list.append(each_date_list)
				each_date_list = []
		return name,table_name_list,table_date_list,url_list

	""""""
	函数说明:财务报表入库

	Author:
		Jack Cui
	Parameters:
	    name - 股票名
	    table_name_list - 财务报表名称
	    table_date_list - 财务报表年限
	    url_list - 财务报表查询连接
	Returns:
		无
	Blog:
		http://blog.csdn.net/c406495762
	Zhihu:
		https://www.zhihu.com/people/Jack--Cui/
	Modify:
		2017-08-31
	""""""
	def insert_tables(self, name, table_name_list,table_date_list, url_list):
		#打开数据库连接:host-连接主机地址,port-端口号,user-用户名,passwd-用户密码,db-数据库名,charset-编码
		conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='yourpasswd',db='financialdata',charset='utf8')
		#使用cursor()方法获取操作游标
		cursor = conn.cursor()  
		#插入信息
		for i in range(len(table_name_list)):
			sys.stdout.write('    [正在下载       ]    %s' % table_name_list[i] + '\r')
			#获取数据地址
			url = self.server + 'hk/service/cwsj_service.php?symbol={}&start={}&end={}&type={}&unit=yuan'.format(code,table_date_list[i][-1],table_date_list[i][0],url_list[i])
			req_table = requests.get(url = url, headers = self.headers)
			table = req_table.json()
			nums = len(table)
			value_dict = {}
			for num in range(nums):
				sys.stdout.write('    [正在下载 %.2f%%]   ' % (((num+1) / nums)*100) + '\r')
				sys.stdout.flush()
				value_dict['股票名'] = name
				value_dict['股票代码'] = code
				for key, value in table[i].items():
					if key in self.table_dict[url_list[i]]:
						value_dict[self.table_dict[url_list[i]][key]] = value

				sql1 = """"""
				INSERT INTO %s (`股票名`,`股票代码`,`报表日期`) VALUES ('%s','%s','%s')"""""" % (url_list[i],value_dict['股票名'],value_dict['股票代码'],value_dict['报表日期'])
				try:
					cursor.execute(sql1)
					# 执行sql语句
					conn.commit()
				except:
					# 发生错误时回滚
					conn.rollback()

				for key, value in value_dict.items():
					if key not in ['股票名','股票代码','报表日期']:
						sql2 = """"""
						UPDATE %s SET %s='%s' WHERE `股票名`='%s' AND `报表日期`='%s'"""""" % (url_list[i],key,value,value_dict['股票名'],value_dict['报表日期'])
						try:
							cursor.execute(sql2)
							# 执行sql语句
							conn.commit()
						except:
							# 发生错误时回滚
							conn.rollback()
				value_dict = {}
			print('    [下载完成 ')

		# 关闭数据库连接
		cursor.close()  
		conn.close()

if __name__ == '__main__':
	print('*' * 100)
	print('\t\t\t\t\t财务数据下载助手\n')
	print('作者:Jack-Cui\n')
	print('About Me:\n')
	print('  知乎:https://www.zhihu.com/people/Jack--Cui')
	print('  Blog:http://blog.csdn.net/c406495762')
	print('  Gihub:https://github.com/Jack-Cherish\n')
	print('*' * 100)
	fd = FinancialData()
	#上市股票地址
	code = input('请输入股票代码:')

	name,table_name_list,table_date_list,url_list = fd.get_informations(fd.cwnb + code + '.html')
	print('\n  %s:(%s)财务数据下载中！\n' % (name,code))
	fd.insert_tables(name,table_name_list,table_date_list,url_list)
	print('\n  %s:(%s)财务数据下载完成！' % (name,code))",13,0,5,0.0,32,1.0,158,0
fuck_bilibili_captcha.py,"import time
import requests
from PIL import Image
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
from io import BytesIO

driver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver')
WAIT = WebDriverWait(driver, 10)
url = 'https://passport.bilibili.com/login'


def mergy_Image(image_file, location_list):
    """"""
    将原始图片进行合成
    :param image_file: 图片文件
    :param location_list: 图片位置
    :return: 合成新的图片
    """"""

    # 存放上下部分的各个小块
    upper_half_list = []
    down_half_list = []

    image = Image.open(image_file)

    # 通过 y 的位置来判断是上半部分还是下半部分,然后切割
    for location in location_list:
        if location['y'] == -58:
            # 间距为10，y：58-116
            im = image.crop((abs(location['x']), 58, abs(location['x'])+10, 116))
            upper_half_list.append(im)
        if location['y'] == 0:
            # 间距为10，y：0-58
            im = image.crop((abs(location['x']), 0, abs(location['x']) + 10, 58))
            down_half_list.append(im)

    # 创建一张大小一样的图片
    new_image = Image.new('RGB', (260, 116))

    # 粘贴好上半部分 y坐标是从上到下（0-116）
    offset = 0
    for im in upper_half_list:
        new_image.paste(im, (offset, 0))
        offset += 10

    # 粘贴好下半部分
    offset = 0
    for im in down_half_list:
        new_image.paste(im, (offset, 58))
        offset += 10

    return new_image


def get_distance(bg_Image, fullbg_Image):

    # 阈值
    threshold = 200

    print(bg_Image.size[0])
    print(bg_Image.size[1])


    for i in range(60, bg_Image.size[0]):
        for j in range(bg_Image.size[1]):
            bg_pix = bg_Image.getpixel((i, j))
            fullbg_pix = fullbg_Image.getpixel((i, j))
            r = abs(bg_pix[0] - fullbg_pix[0])
            g = abs(bg_pix[1] - fullbg_pix[1])
            b = abs(bg_pix[2] - fullbg_pix[2])

            if r + g + b > threshold:
               return i




def get_path(distance):
        result = []
        current = 0
        mid = distance * 4 / 5
        t = 0.2
        v = 0
        while current < (distance - 10):
            if current < mid:
                a = 2
            else:
                a = -3
            v0 = v
            v = v0 + a * t
            s = v0 * t + 0.5 * a * t * t
            current += s
            result.append(round(s))
        return result


def start_drag(driver, distance):

    # 被妖怪吃掉了
    # knob =  WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, ""#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show"")))
    # ActionChains(driver).click_and_hold(knob).perform()
    # ActionChains(driver).move_by_offset(xoffset=distance, yoffset=0.1).perform()
    # time.sleep(0.5)
    # ActionChains(driver).release(knob).perform()

    # 被妖怪吃掉了
    # ActionChains(driver).drag_and_drop_by_offset(knob, distance-10, 0).perform()

    knob = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, ""#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show"")))
    result = get_path(distance)
    ActionChains(driver).click_and_hold(knob).perform()

    for x in result:
        ActionChains(driver).move_by_offset(xoffset=x, yoffset=0).perform()

    time.sleep(0.5)
    ActionChains(driver).release(knob).perform()


def recognize_code(driver):
    """"""
    识别滑动验证码
    :param driver: selenium驱动
    :return:
    """"""

    bs = BeautifulSoup(driver.page_source,'lxml')
    # 找到背景图片和缺口图片的div
    bg_div = bs.find_all(class_='gt_cut_bg_slice')
    fullbg_div = bs.find_all(class_='gt_cut_fullbg_slice')

    # 获取缺口背景图片url
    bg_url = re.findall('background-image:\surl\(""(.*?)""\)',bg_div[0].get('style'))
    # 获取背景图片url
    fullbg_url = re.findall('background-image:\surl\(""(.*?)""\)',fullbg_div[0].get('style'))

    # 存放每个合成缺口背景图片的位置
    bg_location_list = []
    # 存放每个合成背景图片的位置
    fullbg_location_list = []

    for bg in bg_div:
        location = {}
        location['x'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', bg.get('style'))[0][0])
        location['y'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', bg.get('style'))[0][1])
        bg_location_list.append(location)

    for fullbg in fullbg_div:
        location = {}
        location['x'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', fullbg.get('style'))[0][0])
        location['y'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', fullbg.get('style'))[0][1])
        fullbg_location_list.append(location)

    print(bg_location_list)
    print(fullbg_location_list)

    # 将图片格式存为 jpg 格式
    bg_url = bg_url[0].replace('webp', 'jpg')
    fullbg_url = fullbg_url[0].replace('webp', 'jpg')
    # print(bg_url)
    # print(fullbg_url)

    # 下载图片
    bg_image = requests.get(bg_url).content
    fullbg_image = requests.get(fullbg_url).content
    print('完成图片下载')

    # 写入图片
    bg_image_file = BytesIO(bg_image)
    fullbg_image_file = BytesIO(fullbg_image)

    # 合成图片
    bg_Image = mergy_Image(bg_image_file, bg_location_list)
    fullbg_Image = mergy_Image(fullbg_image_file, fullbg_location_list)
    # bg_Image.show()
    # fullbg_Image.show()

    # 计算缺口偏移距离
    distance = get_distance(bg_Image, fullbg_Image)
    print('得到距离：%s' % str(distance))

    start_drag(driver, distance)




if __name__ == '__main__':

    # 获取滑块按钮
    driver.get(url)
    slider = WAIT.until(EC.element_to_be_clickable(
        (By.CSS_SELECTOR, ""#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show"")))

    recognize_code(driver)


    # driver.close()

",19,0,10,0.0,21,1.0,110,0
geetest.py,"# -*-coding:utf-8 -*-
import random
import re
import time
# 图片转换
import base64
from urllib.request import urlretrieve

from bs4 import BeautifulSoup

import PIL.Image as image
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

def save_base64img(data_str, save_name):
    """"""
    将 base64 数据转化为图片保存到指定位置
    :param data_str: base64 数据，不包含类型
    :param save_name: 保存的全路径
    """"""
    img_data = base64.b64decode(data_str)
    file = open(save_name, 'wb')
    file.write(img_data)
    file.close()


def get_base64_by_canvas(driver, class_name, contain_type):
    """"""
    将 canvas 标签内容转换为 base64 数据
    :param driver: webdriver 对象
    :param class_name: canvas 标签的类名
    :param contain_type: 返回的数据是否包含类型
    :return: base64 数据
    """"""
    # 防止图片未加载完就下载一张空图
    bg_img = ''
    while len(bg_img) < 5000:
        getImgJS = 'return document.getElementsByClassName(""' + class_name + '"")[0].toDataURL(""image/png"");'
        bg_img = driver.execute_script(getImgJS)
        time.sleep(0.5)
    # print(bg_img)
    if contain_type:
        return bg_img
    else:
        return bg_img[bg_img.find(',') + 1:]


def save_bg(driver, bg_path=""bg.png"", bg_class=""geetest_canvas_bg geetest_absolute""):
    """"""
    保存包含缺口的背景图
    :param driver: webdriver 对象
    :param bg_path: 保存路径
    :param bg_class: 背景图的 class 属性
    :return: 保存路径
    """"""
    bg_img_data = get_base64_by_canvas(driver, bg_class, False)
    save_base64img(bg_img_data, bg_path)
    return bg_path


def save_full_bg(driver, full_bg_path=""fbg.png"", full_bg_class=""geetest_canvas_fullbg geetest_fade geetest_absolute""):
    """"""
    保存完整的的背景图
    :param driver: webdriver 对象
    :param full_bg_path: 保存路径
    :param full_bg_class: 完整背景图的 class 属性
    :return: 保存路径
    """"""
    bg_img_data = get_base64_by_canvas(driver, full_bg_class, False)
    save_base64img(bg_img_data, full_bg_path)
    return full_bg_path

class Crack():
	def __init__(self,keyword):
		self.url = '*'
		self.browser = webdriver.Chrome('D:\\chromedriver.exe')
		self.wait = WebDriverWait(self.browser, 100)
		self.keyword = keyword
		self.BORDER = 6

	def open(self):
		""""""
		打开浏览器,并输入查询内容
		""""""
		self.browser.get(self.url)
		keyword = self.wait.until(EC.presence_of_element_located((By.ID, 'keyword_qycx')))
		bowton = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'btn')))
		keyword.send_keys(self.keyword)
		bowton.click()

	def get_images(self, bg_filename = 'bg.jpg', fullbg_filename = 'fullbg.jpg'):
		""""""
		获取验证码图片
		:return: 图片的location信息
		""""""
		bg = []
		fullgb = []
		while bg == [] and fullgb == []:
			bf = BeautifulSoup(self.browser.page_source, 'lxml')
			bg = bf.find_all('div', class_ = 'gt_cut_bg_slice')
			fullgb = bf.find_all('div', class_ = 'gt_cut_fullbg_slice')
		bg_url = re.findall('url\(\""(.*)\""\);', bg[0].get('style'))[0].replace('webp', 'jpg')
		fullgb_url = re.findall('url\(\""(.*)\""\);', fullgb[0].get('style'))[0].replace('webp', 'jpg')
		bg_location_list = []
		fullbg_location_list = []
		for each_bg in bg:
			location = {}
			location['x'] = int(re.findall('background-position: (.*)px (.*)px;',each_bg.get('style'))[0][0])
			location['y'] = int(re.findall('background-position: (.*)px (.*)px;',each_bg.get('style'))[0][1])
			bg_location_list.append(location)
		for each_fullgb in fullgb:
			location = {}
			location['x'] = int(re.findall('background-position: (.*)px (.*)px;',each_fullgb.get('style'))[0][0])
			location['y'] = int(re.findall('background-position: (.*)px (.*)px;',each_fullgb.get('style'))[0][1])
			fullbg_location_list.append(location)

		urlretrieve(url = bg_url, filename = bg_filename)
		print('缺口图片下载完成')
		urlretrieve(url = fullgb_url, filename = fullbg_filename)
		print('背景图片下载完成')
		return bg_location_list, fullbg_location_list

	def get_merge_image(self, filename, location_list):
		""""""
		根据位置对图片进行合并还原
		:filename:图片
		:location_list:图片位置
		""""""
		im = image.open(filename)
		new_im = image.new('RGB', (260,116))
		im_list_upper=[]
		im_list_down=[]

		for location in location_list:
			if location['y'] == -58:
				im_list_upper.append(im.crop((abs(location['x']),58,abs(location['x']) + 10, 166)))
			if location['y'] == 0:
				im_list_down.append(im.crop((abs(location['x']),0,abs(location['x']) + 10, 58)))

		new_im = image.new('RGB', (260,116))

		x_offset = 0
		for im in im_list_upper:
			new_im.paste(im, (x_offset,0))
			x_offset += im.size[0]

		x_offset = 0
		for im in im_list_down:
			new_im.paste(im, (x_offset,58))
			x_offset += im.size[0]

		new_im.save(filename)

		return new_im

	def get_merge_image(self, filename, location_list):
		""""""
		根据位置对图片进行合并还原
		:filename:图片
		:location_list:图片位置
		""""""
		im = image.open(filename)
		new_im = image.new('RGB', (260,116))
		im_list_upper=[]
		im_list_down=[]

		for location in location_list:
			if location['y']==-58:
				im_list_upper.append(im.crop((abs(location['x']),58,abs(location['x'])+10,166)))
			if location['y']==0:
				im_list_down.append(im.crop((abs(location['x']),0,abs(location['x'])+10,58)))

		new_im = image.new('RGB', (260,116))

		x_offset = 0
		for im in im_list_upper:
			new_im.paste(im, (x_offset,0))
			x_offset += im.size[0]

		x_offset = 0
		for im in im_list_down:
			new_im.paste(im, (x_offset,58))
			x_offset += im.size[0]

		new_im.save(filename)

		return new_im

	def is_pixel_equal(self, img1, img2, x, y):
		""""""
		判断两个像素是否相同
		:param image1: 图片1
		:param image2: 图片2
		:param x: 位置x
		:param y: 位置y
		:return: 像素是否相同
		""""""
		# 取两个图片的像素点
		pix1 = img1.load()[x, y]
		pix2 = img2.load()[x, y]
		threshold = 60
		if (abs(pix1[0] - pix2[0] < threshold) and abs(pix1[1] - pix2[1] < threshold) and abs(pix1[2] - pix2[2] < threshold)):
			return True
		else:
			return False

	def get_gap(self, img1, img2):
		""""""
		获取缺口偏移量
		:param img1: 不带缺口图片
		:param img2: 带缺口图片
		:return:
		""""""
		left = 43
		for i in range(left, img1.size[0]):
			for j in range(img1.size[1]):
				if not self.is_pixel_equal(img1, img2, i, j):
					left = i
					return left
		return left	

	def get_track(self, distance):
		""""""
		根据偏移量获取移动轨迹
		:param distance: 偏移量
		:return: 移动轨迹
		""""""
		# 移动轨迹
		track = []
		# 当前位移
		current = 0
		# 减速阈值
		mid = distance * 4 / 5
		# 计算间隔
		t = 0.2
		# 初速度
		v = 0
        
		while current < distance:
			if current < mid:
				# 加速度为正2
				a = 2
			else:	
				# 加速度为负3
				a = -3
			# 初速度v0
			v0 = v
			# 当前速度v = v0 + at
			v = v0 + a * t
			# 移动距离x = v0t + 1/2 * a * t^2
			move = v0 * t + 1 / 2 * a * t * t
			# 当前位移
			current += move
			# 加入轨迹
			track.append(round(move))
		return track

	def get_slider(self):
		""""""
		获取滑块
		:return: 滑块对象
		""""""
		while True:
			try:
				slider = self.browser.find_element_by_xpath(""//div[@class='gt_slider_knob gt_show']"")
				break
			except:
				time.sleep(0.5)
		return slider

	def move_to_gap(self, slider, track):
		""""""
		拖动滑块到缺口处
		:param slider: 滑块
		:param track: 轨迹
		:return:
		""""""
		ActionChains(self.browser).click_and_hold(slider).perform()
		while track:
			x = random.choice(track)
			ActionChains(self.browser).move_by_offset(xoffset=x, yoffset=0).perform()
			track.remove(x)
		time.sleep(0.5)
		ActionChains(self.browser).release().perform()

	def crack(self):
		# 打开浏览器
		self.open()

		# 保存的图片名字
		bg_filename = 'bg.jpg'
		fullbg_filename = 'fullbg.jpg'

		# 获取图片
		bg_location_list, fullbg_location_list = self.get_images(bg_filename, fullbg_filename)

		# 根据位置对图片进行合并还原
		# 方法1
		# bg_img = self.get_merge_image(bg_filename, bg_location_list)
		# fullbg_img = self.get_merge_image(fullbg_filename, fullbg_location_list)
		# 方法2
		bg_img = save_bg(self.browser)
		full_bg_img = save_full_bg(self.browser)

		# 获取缺口位置
		# 方法1
		# gap = self.get_gap(fullbg_img, bg_img)
		# 方法2
		gap = self.get_gap(image.open(full_bg_img), image.open(bg_img))
		print('缺口位置', gap)

		track = self.get_track(gap-self.BORDER)
		print('滑动滑块')
		print(track)

		# # 点按呼出缺口
		# slider = self.get_slider()
		# # 拖动滑块到缺口处
		# self.move_to_gap(slider, track)

if __name__ == '__main__':
	print('开始验证')
	crack = Crack(u'中国移动')
	crack.crack()
	print('验证成功')
",42,0,13,19.5,39,1.0,182,0
generate_changelog.py,"#!/usr/bin/env python3

import os
import subprocess
import sys

github_api_token = (
    os.getenv(""CHANGELOG_GITHUB_TOKEN"") if os.getenv(""CHANGELOG_GITHUB_TOKEN"") else input(""Enter Github API token: "")
)

if len(sys.argv) < 2:
    raise Exception(""Provide a version number as parameter (--future-release argument)"")

version = sys.argv[1]

cmd = [
    ""github_changelog_generator"",
    ""-t"",
    github_api_token,
    ""-u"",
    ""locustio"",
    ""-p"",
    ""locust"",
    ""--exclude-labels"",
    ""duplicate,question,invalid,wontfix,cantfix,stale,no-changelog"",
    ""--header-label"",
    ""# Detailed changelog\nThe most important changes can also be found in [the documentation](https://docs.locust.io/en/latest/changelog.html)."",
    ""--since-tag"",
    ""2.27.0"",
    # ""--since-commit"", # these cause issues
    # ""2020-07-01 00:00:00"",
    ""--future-release"",
    version,
]

print(f""Running command: {' '.join(cmd)}\n"")
subprocess.run(cmd)
",2,0,2,0.0,2,1.0,28,0
generate_pxi.py,"import argparse
import os

from Cython import Tempita


def process_tempita(pxifile, outfile) -> None:
    with open(pxifile, encoding=""utf-8"") as f:
        tmpl = f.read()
    pyxcontent = Tempita.sub(tmpl)

    with open(outfile, ""w"", encoding=""utf-8"") as f:
        f.write(pyxcontent)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(""infile"", type=str, help=""Path to the input file"")
    parser.add_argument(""-o"", ""--outdir"", type=str, help=""Path to the output directory"")
    args = parser.parse_args()

    if not args.infile.endswith("".in""):
        raise ValueError(f""Unexpected extension: {args.infile}"")

    outdir_abs = os.path.join(os.getcwd(), args.outdir)
    outfile = os.path.join(
        outdir_abs, os.path.splitext(os.path.split(args.infile)[1])[0]
    )

    process_tempita(args.infile, outfile)


main()
",3,0,0,0.0,1,1.0,22,0
generate_stubs.py,"import inspect
import pathlib
import re

from collections import defaultdict
from typing import Any, Dict, List, Optional, Set, Tuple, Type, get_overloads, get_type_hints

import faker.proxy

from faker import Factory, Faker
from faker.config import AVAILABLE_LOCALES, PROVIDERS

BUILTIN_MODULES_TO_IGNORE = [""builtins""]
GENERIC_MANGLE_TYPES_TO_IGNORE = [""builtin_function_or_method"", ""mappingproxy""]
MODULES_TO_FULLY_QUALIFY = [""datetime""]


imports: Dict[str, Optional[Set[str]]] = defaultdict(lambda: None)
imports[""collections""] = {""OrderedDict""}
imports[""json""] = {""encoder""}
imports[""typing""] = {""Callable"", ""Collection"", ""TypeVar"", ""overload""}
imports[""uuid""] = {""UUID""}
imports[""enum""] = {""Enum""}
imports[""faker.typing""] = {""*""}
imports[""faker.generator""] = {""Generator""}


def get_module_and_member_to_import(cls: Type, locale: Optional[str] = None) -> Tuple[str, str]:
    cls_name = getattr(cls, ""__name__"", getattr(cls, ""_name"", str(cls)))
    module, member = cls.__module__, cls_name
    if cls_name is None:
        qualified_type = re.findall(r""([a-zA-Z_0-9]+)\.([a-zA-Z_0-9]+)"", str(cls))
        if len(qualified_type) > 0:
            if imports[qualified_type[0][0]] is None or qualified_type[0][1] not in imports[qualified_type[0][0]]:
                module, member = qualified_type[0]
        else:
            unqualified_type = re.findall(r""[^\.a-zA-Z0-9_]([A-Z][a-zA-Z0-9_]+)[^\.a-zA-Z0-9_]"", "" "" + str(cls) + "" "")
            if len(unqualified_type) > 0 and unqualified_type[0] != ""NoneType"":
                cls_str = str(cls).replace("".en_US"", """").replace(""faker."", ""."")
                if ""<class '"" in cls_str:
                    cls_str = cls_str.split(""'"")[1]
                if locale is not None:
                    cls_str = cls_str.replace(""."" + locale, """")

                if imports[cls_str] is None or unqualified_type[0] not in imports[cls_str]:
                    module, member = cls_str, unqualified_type[0]
    if module in MODULES_TO_FULLY_QUALIFY:
        member = None
    return module, member


seen_funcs = set()
seen_vars = set()


class UniqueMemberFunctionsAndVariables:
    def __init__(self, cls: type, funcs: Dict[str, Any], vars: Dict[str, Any]):
        global seen_funcs, seen_vars
        self.cls = cls
        self.funcs = funcs
        for func_name in seen_funcs:
            self.funcs.pop(func_name, None)
        seen_funcs = seen_funcs.union(self.funcs.keys())

        self.vars = vars
        for var_name in seen_vars.union(seen_funcs):
            self.vars.pop(var_name, None)
        seen_vars = seen_vars.union(self.vars.keys())


def get_member_functions_and_variables(cls: object, include_mangled: bool = False) -> UniqueMemberFunctionsAndVariables:
    members = [
        (name, value)
        for (name, value) in inspect.getmembers(cls)
        if ((include_mangled and name.startswith(""__"")) or not name.startswith(""_""))
    ]
    funcs: Dict[str, Any] = {}
    vars: Dict[str, Any] = {}
    for name, value in members:
        attr = getattr(cls, name, None)
        if attr is not None and (inspect.isfunction(attr) or inspect.ismethod(attr)):
            funcs[name] = value
        elif inspect.isgetsetdescriptor(attr) or inspect.ismethoddescriptor(attr):
            # I haven't implemented logic
            # for generating descriptor signatures yet
            continue
        elif not include_mangled or type(value).__name__ not in GENERIC_MANGLE_TYPES_TO_IGNORE:
            vars[name] = value

    return UniqueMemberFunctionsAndVariables(cls, funcs, vars)


def get_signatures_for_func(func_value, func_name, locale, is_overload: bool = False, comment: Optional[str] = None):
    """"""Return the signatures for the given function, recursing as necessary to handle overloads.""""""
    signatures = []

    if comment is None:
        comment = inspect.getdoc(func_value)

    if not is_overload:
        try:
            overloads = get_overloads(func_value)
        except Exception as e:
            raise TypeError(f""Can't parse overloads for {func_name}{sig}."") from e

        if overloads:
            for overload in overloads:
                signatures.extend(
                    get_signatures_for_func(overload, func_name, locale, is_overload=True, comment=comment)
                )
            return signatures

    sig = inspect.signature(func_value)
    try:
        hints = get_type_hints(func_value)
    except Exception as e:
        raise TypeError(f""Can't parse {func_name}{sig}."") from e
    ret_annot_module = getattr(sig.return_annotation, ""__module__"", None)
    if sig.return_annotation not in [
        None,
        inspect.Signature.empty,
        prov_cls.__name__,
    ] and ret_annot_module not in [
        None,
        *BUILTIN_MODULES_TO_IGNORE,
    ]:
        module, member = get_module_and_member_to_import(sig.return_annotation, locale)
        if module not in [None, ""types""]:
            if imports[module] is None:
                imports[module] = set() if member is None else {member}
            elif member is not None:
                imports[module].add(member)

    new_parms = []
    for key, parm_val in sig.parameters.items():
        new_parm = parm_val
        annotation = hints.get(key, new_parm.annotation)
        if parm_val.default is not inspect.Parameter.empty:
            new_parm = parm_val.replace(default=...)
        if annotation is not inspect.Parameter.empty and annotation.__module__ not in BUILTIN_MODULES_TO_IGNORE:
            module, member = get_module_and_member_to_import(annotation, locale)
            if module not in [None, ""types""]:
                if imports[module] is None:
                    imports[module] = set() if member is None else {member}
                elif member is not None:
                    imports[module].add(member)
        new_parms.append(new_parm)

    sig = sig.replace(parameters=new_parms)
    sig_str = str(sig).replace(""Ellipsis"", ""..."").replace(""NoneType"", ""None"").replace(""~"", """")
    for module in imports.keys():
        if module in MODULES_TO_FULLY_QUALIFY:
            continue
        sig_str = sig_str.replace(f""{module}."", """")

    decorator = """"
    if is_overload:
        decorator += ""@overload\n""
    if list(sig.parameters)[0] == ""cls"":
        decorator += ""@classmethod\n""
    elif list(sig.parameters)[0] != ""self"":
        decorator += ""@staticmethod\n""
    signatures.append(
        (
            f""{decorator}def {func_name}{sig_str}: ..."",
            None if comment == """" else comment,
            False,
        )
    )
    return signatures


classes_and_locales_to_use_for_stub: List[Tuple[object, str]] = []
for locale in AVAILABLE_LOCALES:
    for provider in PROVIDERS:
        if provider == ""faker.providers"":
            continue
        prov_cls, _, _ = Factory._find_provider_class(provider, locale)
        classes_and_locales_to_use_for_stub.append((prov_cls, locale))

all_members: List[Tuple[UniqueMemberFunctionsAndVariables, str]] = [
    (get_member_functions_and_variables(cls), locale) for cls, locale in classes_and_locales_to_use_for_stub
] + [(get_member_functions_and_variables(Faker, include_mangled=True), None)]

# Use the accumulated seen_funcs and seen_vars to remove all variables that have the same name as a function somewhere
overlapping_var_names = seen_vars.intersection(seen_funcs)
for mbr_funcs_and_vars, _ in all_members:
    for var_name_to_remove in overlapping_var_names:
        mbr_funcs_and_vars.vars.pop(var_name_to_remove, None)

# list of tuples. First elem of tuple is the signature string,
#  second is the comment string,
#  third is a boolean which is True if the comment precedes the signature
signatures_with_comments: List[Tuple[str, str, bool]] = []

for mbr_funcs_and_vars, locale in all_members:
    for func_name, func_value in mbr_funcs_and_vars.funcs.items():
        signatures_with_comments.extend(get_signatures_for_func(func_value, func_name, locale))

signatures_with_comments_as_str = []
for sig, comment, is_preceding_comment in signatures_with_comments:
    if comment is not None and is_preceding_comment:
        signatures_with_comments_as_str.append(f""# {comment}\n    {sig}"")
    elif comment is not None:
        sig_without_final_ellipsis = sig.strip("" ."")
        signatures_with_comments_as_str.append(
            sig_without_final_ellipsis + '\n    """"""\n    ' + comment.replace(""\n"", ""\n    "") + '\n    """"""\n    ...'
        )
    else:
        signatures_with_comments_as_str.append(sig)


def get_import_str(module: str, members: Optional[Set[str]]) -> str:
    if members is None or len(members) == 0:
        return f""import {module}""
    else:
        return f""from {module} import {', '.join(members)}""


imports_block = ""\n"".join([get_import_str(module, names) for module, names in imports.items()])
member_signatures_block = ""    "" + ""\n    "".join(
    [sig.replace(""\n"", ""\n    "") for sig in signatures_with_comments_as_str]
)

body = f""""""# This file is auto-generated by generate_stubs.py.
# Please do not edit this file directly.

{imports_block}

class Faker:
{member_signatures_block}
""""""

faker_proxy_path = pathlib.Path(inspect.getfile(faker.proxy))
stub_file_path = faker_proxy_path.with_name(""proxy.pyi"").resolve()
with open(stub_file_path, ""w"", encoding=""utf-8"") as fh:
    fh.write(body)
",60,0,7,0.0,108,1.0,191,0
generate_version.py,"#!/usr/bin/env python3

# Note: This file has to live next to setup.py or versioneer will not work
import argparse
import os
import sys

import versioneer

sys.path.insert(0, """")


def write_version_info(path) -> None:
    version = None
    git_version = None

    try:
        import _version_meson

        version = _version_meson.__version__
        git_version = _version_meson.__git_version__
    except ImportError:
        version = versioneer.get_version()
        git_version = versioneer.get_versions()[""full-revisionid""]
    if os.environ.get(""MESON_DIST_ROOT""):
        path = os.path.join(os.environ.get(""MESON_DIST_ROOT""), path)
    with open(path, ""w"", encoding=""utf-8"") as file:
        file.write(f'__version__=""{version}""\n')
        file.write(f'__git_version__=""{git_version}""\n')


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""-o"",
        ""--outfile"",
        type=str,
        help=""Path to write version info to"",
        required=False,
    )
    parser.add_argument(
        ""--print"",
        default=False,
        action=""store_true"",
        help=""Whether to print out the version"",
        required=False,
    )
    args = parser.parse_args()

    if args.outfile:
        if not args.outfile.endswith("".py""):
            raise ValueError(
                f""Output file must be a Python file. ""
                f""Got: {args.outfile} as filename instead""
            )

        write_version_info(args.outfile)

    if args.print:
        try:
            import _version_meson

            version = _version_meson.__version__
        except ImportError:
            version = versioneer.get_version()
        print(version)


main()
",6,0,0,0.0,8,1.0,52,0
github-scraper.py,"import requests
import os
from github import Github
import re
GITHUB_TOKEN = ""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""
GITHUB_API_URL = ""https://api.github.com/search/repositories""
g = Github(GITHUB_TOKEN)

MAX_FILE_SIZE = 20000
MAX_REPOS = 200
MAX_FILES = 20

os.makedirs(""smaller-git-dataset"", exist_ok=True)

HEADERS = {""Authorization"": f""token {GITHUB_TOKEN}""}

params = {
    ""q"": ""language: python stars:>10"",
    ""sort"": ""stars"",
    ""order"": ""desc"",
    ""per_page"": 200,
    ""page"": 1
}



local_import_pattern = re.compile(r""import (\w+)|from (\w+) import"")
def is_independent(code, repo_files):
    matches = local_import_pattern.findall(code)
    for match in matches:
        imported_module = match[0] or match[1]
        if f""{imported_module}.py"" in repo_files:
            return False
    return True

repositories = []
while len(repositories) < MAX_REPOS and params[""page""] <= 3:  # Limit to 200 repos (2 pages)
    response = requests.get(GITHUB_API_URL, headers=HEADERS, params=params)
    if response.status_code != 200:
        print(f""⚠️ API Error: {response.status_code}"")
        break

    data = response.json()
    repositories.extend(data[""items""])  # Add repos to list

    params[""page""] += 1  # Go to the next page

# 🔹 Limit to 200 repositories
repositories = repositories[:MAX_REPOS]
print(f""✅ Fetched {len(repositories)} Python repositories."")


# 🔹 Function to fetch Python files from a repository
def fetch_python_files(repo):
    repo_name = repo[""full_name""]
    files_url = repo[""url""] + ""/contents""

    try:
        response = requests.get(files_url, headers=HEADERS)
        if response.status_code != 200:
            print(f""⚠️ Error fetching files for {repo_name}"")
            return

        files = response.json()
        repo_files = {file[""name""] for file in files if file[""type""] == ""file""}
        count = 0  # Limit files per repo

        for file in files:
            if count >= MAX_FILES:
                break  # Stop after 10 files

            if file[""type""] == ""file"" and file[""name""].endswith("".py"") and file[""size""] < MAX_FILE_SIZE:
                file_url = file[""download_url""]

                # 🔹 Download and save the file
                response = requests.get(file_url)
                code = response.text
                if is_independent(code, repo_files):
                    with open(f""smaller-git-dataset/{file['name']}"", ""w"", encoding=""utf-8"") as f:
                        f.write(code)

                    print(f""✅ Saved: {file['name']} ({file['size']} bytes) from {repo_name}"")
                    count += 1

    except Exception as e:
        print(f""⚠️ Error processing {repo_name}: {e}"")


# 🔹 Process each repository
for i, repo in enumerate(repositories):
    print(f""🔍 Processing {i + 1}/{len(repositories)}: {repo['full_name']}"")
    fetch_python_files(repo)

print(""✅ Done processing repositories."")",0,0,0,0.0,0,1.0,0,0
gunicorn.config.py,"#!/usr/bin/env python3

import logging
import os
import socket
import struct
import sys
import threading
import time

import structlog
from prometheus_client import CollectorRegistry, Gauge, multiprocess, start_http_server

loglevel = ""error""
keepalive = 120

# Set the timeout to something lower than any downstreams, such that if the
# timeout is hit, then the worker will be killed and respawned, which will then
# we able to pick up any connections that were previously pending on the socket
# and serve the requests before the downstream timeout.
timeout = 15

grateful_timeout = 120


METRICS_UPDATE_INTERVAL_SECONDS = int(os.getenv(""GUNICORN_METRICS_UPDATE_SECONDS"", 5))


def when_ready(server):
    """"""
    To ease being able to hide the /metrics endpoint when running in production,
    we serve the metrics on a separate port, using the
    prometheus_client.multiprocess Collector to pull in data from the worker
    processes.
    """"""
    registry = CollectorRegistry()
    multiprocess.MultiProcessCollector(registry)
    port = int(os.environ.get(""PROMETHEUS_METRICS_EXPORT_PORT"", 8001))
    start_http_server(port=port, registry=registry)

    # Start a thread in the Arbiter that will monitor the backlog on the sockets
    # Gunicorn is listening on.
    socket_monitor = SocketMonitor(server=server, registry=registry)
    socket_monitor.start()


def post_fork(server, worker):
    """"""
    Within each worker process, start a thread that will monitor the thread and
    connection pool.
    """"""
    worker_monitor = WorkerMonitor(worker=worker)
    worker_monitor.start()


def worker_exit(server, worker):
    """"""
    Ensure that we mark workers as dead with the prometheus_client such that
    any cleanup can happen.
    """"""
    multiprocess.mark_process_dead(worker.pid)


class SocketMonitor(threading.Thread):
    """"""
    We have enabled the statsd collector for Gunicorn, but this doesn't include
    the backlog due to concerns over portability, see
    https://github.com/benoitc/gunicorn/pull/2407

    Instead, we expose to Prometheus a gauge that will report the backlog size.

    We can then:

     1. use this to monitor how well the Gunicorn instances are keeping up with
        requests.
     2. use this metric to handle HPA scaling e.g. in Kubernetes

    """"""

    def __init__(self, server, registry):
        super().__init__()
        self.daemon = True
        self.server = server
        self.registry = registry

    def run(self):
        """"""
        Every X seconds, check to see how many connections are pending for each
        server socket.

        We label each individually, as limits such as `--backlog` will apply to
        each individually.
        """"""
        if sys.platform != ""linux"":
            # We use the assumption that we are on Linux to be able to get the
            # socket backlog, so if we're not on Linux, we return immediately.
            return

        backlog_gauge = Gauge(
            ""gunicorn_pending_connections"",
            ""The number of pending connections on all sockets. Linux only."",
            registry=self.registry,
            labelnames=[""listener""],
        )

        while True:
            for sock in self.server.LISTENERS:
                backlog = self.get_backlog(sock=sock)
                backlog_gauge.labels(listener=str(sock)).set(backlog)

            time.sleep(METRICS_UPDATE_INTERVAL_SECONDS)

    def get_backlog(self, sock):
        # tcp_info struct from include/uapi/linux/tcp.h
        fmt = ""B"" * 8 + ""I"" * 24
        tcp_info_struct = sock.getsockopt(socket.IPPROTO_TCP, socket.TCP_INFO, 104)
        # 12 is tcpi_unacked
        return struct.unpack(fmt, tcp_info_struct)[12]


class WorkerMonitor(threading.Thread):
    """"""
    There is a statsd logger support in Gunicorn that allows us to gather
    metrics e.g. on the number of workers, requests, request duration etc. See
    https://docs.gunicorn.org/en/stable/instrumentation.html for details.

    To get a better understanding of the pool utilization, number of accepted
    connections, we start a thread in head worker to report these via prometheus
    metrics.
    """"""

    def __init__(self, worker):
        super().__init__()
        self.daemon = True
        self.worker = worker

    def run(self):
        """"""
        Every X seconds, check the status of the Thread pool, as well as the
        """"""
        active_worker_connections = Gauge(
            ""gunicorn_active_worker_connections"",
            ""Number of active connections."",
            labelnames=[""pid""],
        )
        max_worker_connections = Gauge(
            ""gunicorn_max_worker_connections"",
            ""Maximum worker connections."",
            labelnames=[""pid""],
        )

        total_threads = Gauge(
            ""gunicorn_max_worker_threads"",
            ""Size of the thread pool per worker."",
            labelnames=[""pid""],
        )
        active_threads = Gauge(
            ""gunicorn_active_worker_threads"",
            ""Number of threads actively processing requests."",
            labelnames=[""pid""],
        )

        pending_requests = Gauge(
            ""gunicorn_pending_requests"",
            ""Number of requests that have been read from a connection but have not completed yet"",
            labelnames=[""pid""],
        )

        max_worker_connections.labels(pid=self.worker.pid).set(self.worker.cfg.worker_connections)
        total_threads.labels(pid=self.worker.pid).set(self.worker.cfg.threads)

        while True:
            active_worker_connections.labels(pid=self.worker.pid).set(self.worker.nr_conns)
            active_threads.labels(pid=self.worker.pid).set(min(self.worker.cfg.threads, len(self.worker.futures)))
            pending_requests.labels(pid=self.worker.pid).set(len(self.worker.futures))

            time.sleep(METRICS_UPDATE_INTERVAL_SECONDS)


LOGGING_FORMATTER_NAME = os.getenv(""LOGGING_FORMATTER_NAME"", ""default"")


# Setup stdlib logging to be handled by Structlog
def add_pid_and_tid(
    logger: logging.Logger, method_name: str, event_dict: structlog.types.EventDict
) -> structlog.types.EventDict:
    event_dict[""pid""] = os.getpid()
    event_dict[""tid""] = threading.get_ident()
    return event_dict


pre_chain = [
    # Add the log level and a timestamp to the event_dict if the log entry
    # is not from structlog.
    structlog.stdlib.add_log_level,
    structlog.stdlib.add_logger_name,
    add_pid_and_tid,
    structlog.processors.TimeStamper(fmt=""iso""),
]


# This is a copy the default logging config for gunicorn but with additions to:
#
#  1. non propagate loggers to the root handlers (otherwise we get duplicate log
#     lines)
#  2. use structlog for processing of log records
#
# See
# https://github.com/benoitc/gunicorn/blob/0b953b803786997d633d66c0f7c7b290df75e07c/gunicorn/glogging.py#L48
# for the default log settings.
logconfig_dict = {
    ""version"": 1,
    ""disable_existing_loggers"": True,
    ""formatters"": {
        ""default"": {
            ""()"": structlog.stdlib.ProcessorFormatter,
            ""processor"": structlog.dev.ConsoleRenderer(colors=True),
            ""foreign_pre_chain"": pre_chain,
        },
        ""json"": {
            ""()"": structlog.stdlib.ProcessorFormatter,
            ""processor"": structlog.processors.JSONRenderer(),
            ""foreign_pre_chain"": pre_chain,
        },
    },
    ""root"": {""level"": ""INFO"", ""handlers"": [""console""]},
    ""loggers"": {
        ""gunicorn.error"": {
            ""level"": ""INFO"",
            ""handlers"": [""error_console""],
            ""propagate"": False,
            ""qualname"": ""gunicorn.error"",
        },
        ""gunicorn.access"": {
            ""level"": ""INFO"",
            ""handlers"": [""console""],
            ""propagate"": False,
            ""qualname"": ""gunicorn.access"",
        },
    },
    ""handlers"": {
        ""error_console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": LOGGING_FORMATTER_NAME,
            ""stream"": ""ext://sys.stderr"",
        },
        ""console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": LOGGING_FORMATTER_NAME,
            ""stream"": ""ext://sys.stdout"",
        },
    },
}
",13,0,2,0.0,5,1.0,144,0
hatch_build.py,"import os

from hatchling.builders.hooks.plugin.interface import BuildHookInterface


class CustomBuildHook(BuildHookInterface):
    def initialize(self, version, build_data):
        from babel.messages.frontend import compile_catalog

        for theme in 'mkdocs', 'readthedocs':
            cmd = compile_catalog()
            cmd.directory = os.path.join('mkdocs', 'themes', theme, 'locales')
            cmd.finalize_options()
            cmd.run()
",2,0,0,0.0,1,1.0,10,0
hero.py,"#-*- coding: UTF-8 -*-
from urllib.request import urlretrieve
import requests
import os

""""""
函数说明:下载《英雄联盟盒子》中的英雄图片

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_imgs_download(url, header):
    req = requests.get(url = url, headers = header).json()
    hero_num = len(req['list'])
    print('一共有%d个英雄' % hero_num)
    hero_images_path = 'hero_images'
    for each_hero in req['list']:
        hero_photo_url = each_hero['cover']
        hero_name = each_hero['name'] + '.jpg'
        filename = hero_images_path + '/' + hero_name
        if hero_images_path not in os.listdir():
            os.makedirs(hero_images_path)
        urlretrieve(url = hero_photo_url, filename = filename)

""""""
函数说明:打印所有英雄的名字和ID

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_list(url, header):
	print('*' * 100)
	print('\t\t\t\t欢迎使用《王者荣耀》出装下助手！')
	print('*' * 100)
	req = requests.get(url = url, headers = header).json()
	flag = 0
	for each_hero in req['list']:
		flag += 1
		print('%s的ID为:%-7s' % (each_hero['name'], each_hero['hero_id']), end = '\t\t')
		if flag == 3:
			print('\n', end = '')
			flag = 0

""""""
函数说明:根据equip_id查询武器名字和价格

Parameters:
    equip_id - 武器的ID
    weapon_info - 存储所有武器的字典
Returns:
    weapon_name - 武器的名字
    weapon_price - 武器的价格
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def seek_weapon(equip_id, weapon_info):
	for each_weapon in weapon_info:
		if each_weapon['equip_id'] == str(equip_id):
			weapon_name = each_weapon['name']
			weapon_price = each_weapon['price']
			return weapon_name, weapon_price


""""""
函数说明:获取并打印出装信息

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
    weapon_info - 存储所有武器的字典
Returns:
	无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_info(url, header, weapon_info):
	req = requests.get(url = url, headers = header).json()
	print('\n历史上的%s:\n    %s' % (req['info']['name'], req['info']['history_intro']))
	for each_equip_choice in req['info']['equip_choice']:
		print('\n%s:\n   %s' % (each_equip_choice['title'], each_equip_choice['description']))
		total_price = 0
		flag = 0
		for each_weapon in each_equip_choice['list']:
			flag += 1
			weapon_name, weapon_price = seek_weapon(each_weapon['equip_id'], weapon_info)
			print('%s:%s' % (weapon_name, weapon_price), end = '\t')
			if flag == 3:
				print('\n', end = '')
				flag = 0
			total_price += int(weapon_price)
		print('神装套件价格共计:%d' % total_price)


""""""
函数说明:获取武器信息

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
Returns:
    weapon_info_dict - 武器信息
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_weapon(url, header):
    req = requests.get(url = url, headers = header).json()
    weapon_info_dict = req['list']
    return weapon_info_dict


if __name__ == '__main__':
    headers = {'Accept-Charset': 'UTF-8',
            'Accept-Encoding': 'gzip,deflate',
            'User-Agent': 'Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI 5 MIUI/V8.1.6.0.MAACNDI)',
            'X-Requested-With': 'XMLHttpRequest',
            'Content-type': 'application/x-www-form-urlencoded',
            'Connection': 'Keep-Alive',
            'Host': 'gamehelper.gm825.com'}
    weapon_url = ""http://gamehelper.gm825.com/wzry/equip/list?channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8""
    heros_url = ""http://gamehelper.gm825.com/wzry/hero/list?channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8""
    hero_list(heros_url, headers)
    hero_id = input(""请输入要查询的英雄ID:"")
    hero_url = ""http://gamehelper.gm825.com/wzry/hero/detail?hero_id={}&channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8"".format(hero_id)
    weapon_info_dict = hero_weapon(weapon_url, headers)
    hero_info(hero_url, headers, weapon_info_dict)",15,0,0,0.0,16,1.0,145,0
hubconf.py,"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""isort:skip_file""""""

import functools
import importlib


dependencies = [
    ""dataclasses"",
    ""hydra"",
    ""numpy"",
    ""omegaconf"",
    ""regex"",
    ""requests"",
    ""torch"",
]


# Check for required dependencies and raise a RuntimeError if any are missing.
missing_deps = []
for dep in dependencies:
    try:
        importlib.import_module(dep)
    except ImportError:
        # Hack: the hydra package is provided under the ""hydra-core"" name in
        # pypi. We don't want the user mistakenly calling `pip install hydra`
        # since that will install an unrelated package.
        if dep == ""hydra"":
            dep = ""hydra-core""
        missing_deps.append(dep)
if len(missing_deps) > 0:
    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))


# only do fairseq imports after checking for dependencies
from fairseq.hub_utils import (  # noqa; noqa
    BPEHubInterface as bpe,
    TokenizerHubInterface as tokenizer,
)
from fairseq.models import MODEL_REGISTRY  # noqa


# torch.hub doesn't build Cython components, so if they are not found then try
# to build them here
try:
    import fairseq.data.token_block_utils_fast  # noqa
except ImportError:
    try:
        import cython  # noqa
        import os
        from setuptools import sandbox

        sandbox.run_setup(
            os.path.join(os.path.dirname(__file__), ""setup.py""),
            [""build_ext"", ""--inplace""],
        )
    except ImportError:
        print(
            ""Unable to build Cython components. Please make sure Cython is ""
            ""installed if the torch.hub model you are loading depends on it.""
        )


# automatically expose models defined in FairseqModel::hub_models
for _model_type, _cls in MODEL_REGISTRY.items():
    for model_name in _cls.hub_models().keys():
        globals()[model_name] = functools.partial(
            _cls.from_pretrained,
            model_name,
        )
",5,0,1,0.0,13,1.0,47,0
idc.py,"import pandas as pd
dataset_path = ""github_code_dataset_with_efficiency_scores.csv""
df = pd.read_csv(dataset_path)


columns_to_remove = [""efficiency_score""]
df = df.drop(columns=[col for col in columns_to_remove if col in df.columns], errors=""ignore"")


final_file_path = ""github_code_dataset_no_comments.csv""
df.to_csv(final_file_path, index=False)",1,0,0,0.0,0,1.0,7,0
idk.py,"import requests
import pandas as pd
import os

SONARCLOUD_TOKEN = ""82a8fe6df3b0391119aa62fd413df6db3707e9b1""
ORGANIZATION_KEY = ""gamify""
PROJECT_KEY = ""dimp170_refined-sonar-analysis""
HEADERS = {""Authorization"": f""Bearer {SONARCLOUD_TOKEN}""}

full_results = []
snippet_results = []

def get_sonar_files():
    sonar_files = {}
    page = 1

    while True:
        url = f""https://sonarcloud.io/api/components/tree""
        params = {
            ""component"": PROJECT_KEY,
            ""qualifiers"": ""FIL"",
            ""organization"": ORGANIZATION_KEY,
            ""p"": page,
            ""ps"": 500
        }

        response = requests.get(url, headers=HEADERS, params=params)

        if response.status_code == 200:
            components = response.json().get(""components"", [])
            file_paths = [c[""path""] for c in components]
            print(f""SonarCloud Retrieved Files (First 5): {file_paths[:5]}"")
            return {c[""path""]: c[""key""] for c in components}
        else:
            print(f""API Error: {response.status_code} - {response.text}"")
            break



sonar_files = get_sonar_files()
print(f""Retrieved {len(sonar_files)} indexed files from SonarCloud."")


dataset_path = ""github_code_dataset_no_comments.csv""
try:
    df = pd.read_csv(dataset_path)
except FileNotFoundError:
    print(f""Error: Dataset file '{dataset_path}' not found."")
    exit()


df[""file_path""] = df[""file_path""].apply(lambda x: x.split(""/"")[-1])
df[""file_path""] = df[""file_path""].apply(lambda x: f""{PROJECT_KEY}:{x}"")
df[""file_path""] = df[""file_path""].apply(lambda x: x.replace(f""{PROJECT_KEY}:"", """"))


df[""file_path""] = df[""file_path""].apply(lambda x: f""{PROJECT_KEY}:{x}"" if not x.startswith(PROJECT_KEY) else x)
print(f""Updated Dataset File Paths (First 5): {df['file_path'].head().tolist()}"")

df[""sonar_component_key""] = df[""file_path""].map(sonar_files)


df_filtered = df.dropna(subset=[""sonar_component_key""])
print(f""Matched {len(df_filtered)} files to SonarCloud."")



def get_all_sonar_metrics(component_key):
    if not component_key or pd.isna(component_key):
        return {}

    url = ""https://sonarcloud.io/api/measures/component""
    params = {
        ""component"": component_key,
        ""organization"": ORGANIZATION_KEY,
        ""metricKeys"": ""code_smells,complexity,security_rating,cognitive_complexity,duplicated_lines_density,bugs,vulnerabilities,ncloc""

    }

    response = requests.get(url, headers=HEADERS, params=params)

    if response.status_code == 200:
        measures = response.json().get(""component"", {}).get(""measures"", [])
        metrics = {m[""metric""]: m[""value""] for m in measures}


        print(f""SonarCloud Metrics for {component_key}: {metrics}"")

        return metrics
    else:
        print(f""API Error ({component_key}): {response.status_code} - {response.text}"")
        return {}
def get_code(file_path):
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        full_code = f.read()
        snippet = ""\n"".join(full_code.split(""\n"")[:5]) + ""...""
        return full_code, snippet




sonar_results_all_metrics = {}
for file_path, component_key in sonar_files.items():
    clean_file_name = os.path.basename(file_path)
    full_code, snippet = get_code(file_path)
    metrics = get_all_sonar_metrics(component_key)

    full_row = {
        ""file_name"": clean_file_name,
        ""full_code"": full_code
    }
    full_row.update(metrics)
    full_results.append(full_row)

    snippet_row = {
        ""file_name"": clean_file_name,
        ""snippet"": snippet
    }
    snippet_row.update(metrics)
    snippet_results.append(snippet_row)

df_full = pd.DataFrame(full_results)
df_snip = pd.DataFrame(snippet_results)

df_full.to_csv(""refined-sonar-metrics-for-ai.csv"", index=False)
df_snip.to_csv(""refined-sonar-metrics.csv"", index=False)







",13,0,1,0.0,22,1.0,80,0
ikun_basketball.py,"# coding=utf-8

# 最新版的selenium(4.x.x)已经不支持PhantomJS。如要用PhantomJS，可用旧版本selenium。如pip install selenium==3.8.0。
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import xlwt

# browser = webdriver.PhantomJS()
browser = webdriver.Chrome()
WAIT = WebDriverWait(browser, 10)
browser.set_window_size(1400, 900)

book = xlwt.Workbook(encoding='utf-8', style_compression=0)

sheet = book.add_sheet('蔡徐坤篮球', cell_overwrite_ok=True)
sheet.write(0, 0, '名称')
sheet.write(0, 1, '地址')
sheet.write(0, 2, '描述')
sheet.write(0, 3, '观看次数')
sheet.write(0, 4, '弹幕数')
sheet.write(0, 5, '发布时间')

n = 1


def search():
    try:
        print('开始访问b站....')
        browser.get(""https://www.bilibili.com/"")

        # 被那个破登录遮住了
        # index = WAIT.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ""#primary_menu > ul > li.home > a"")))
        # index.click()

        input = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, ""#nav_searchform > input"")))
        submit = WAIT.until(EC.element_to_be_clickable(
            (By.XPATH, '/html/body/div[2]/div/div[1]/div[1]/div/div[2]/div/form/div/button')))

        input.send_keys('蔡徐坤 篮球')
        submit.click()

        # 跳转到新的窗口
        print('跳转到新窗口')
        all_h = browser.window_handles
        browser.switch_to.window(all_h[1])
        get_source()

        total = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR,
                                                           ""#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.last > button"")))
        return int(total.text)
    except TimeoutException:
        return search()


def next_page(page_num):
    try:
        print('获取下一页数据')
        next_btn = WAIT.until(EC.element_to_be_clickable((By.CSS_SELECTOR,
                                                          '#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.next > button')))
        next_btn.click()
        WAIT.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,
                                                     '#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.active > button'),
                                                    str(page_num)))
        get_source()
    except TimeoutException:
        browser.refresh()
        return next_page(page_num)


def save_to_excel(soup):
    list = soup.find(class_='video-list clearfix').find_all(class_='video-item matrix')

    for item in list:
        item_title = item.find('a').get('title')
        item_link = item.find('a').get('href')
        item_dec = item.find(class_='des hide').text
        item_view = item.find(class_='so-icon watch-num').text
        item_biubiu = item.find(class_='so-icon hide').text
        item_date = item.find(class_='so-icon time').text

        print('爬取：' + item_title)

        global n

        sheet.write(n, 0, item_title)
        sheet.write(n, 1, item_link)
        sheet.write(n, 2, item_dec)
        sheet.write(n, 3, item_view)
        sheet.write(n, 4, item_biubiu)
        sheet.write(n, 5, item_date)

        n = n + 1


def get_source():
    WAIT.until(EC.presence_of_element_located(
        (By.CSS_SELECTOR, '#all-list > div.flow-loader > div.filter-wrap')))

    html = browser.page_source
    soup = BeautifulSoup(html, 'lxml')
    print('到这')

    save_to_excel(soup)


def main():
    try:
        total = search()
        print(total)

        for i in range(2, int(total + 1)):
            next_page(i)

    finally:
        browser.close()


if __name__ == '__main__':
    main()
    book.save('蔡徐坤篮球.xlsx')
",8,0,4,0.0,5,1.0,86,0
input_data.py,"""""""Functions for downloading and reading MNIST data.""""""
from __future__ import print_function
import gzip
import os
import urllib
import numpy
SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
def maybe_download(filename, work_directory):
  """"""Download the data from Yann's website, unless it's already here.""""""
  if not os.path.exists(work_directory):
    os.mkdir(work_directory)
  filepath = os.path.join(work_directory, filename)
  if not os.path.exists(filepath):
    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)
    statinfo = os.stat(filepath)
    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')
  return filepath
def _read32(bytestream):
  dt = numpy.dtype(numpy.uint32).newbyteorder('>')
  return numpy.frombuffer(bytestream.read(4), dtype=dt)
def extract_images(filename):
  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""
  print('Extracting', filename)
  with gzip.open(filename) as bytestream:
    magic = _read32(bytestream)
    if magic != 2051:
      raise ValueError(
          'Invalid magic number %d in MNIST image file: %s' %
          (magic, filename))
    num_images = _read32(bytestream)
    rows = _read32(bytestream)
    cols = _read32(bytestream)
    buf = bytestream.read(rows * cols * num_images)
    data = numpy.frombuffer(buf, dtype=numpy.uint8)
    data = data.reshape(num_images, rows, cols, 1)
    return data
def dense_to_one_hot(labels_dense, num_classes=10):
  """"""Convert class labels from scalars to one-hot vectors.""""""
  num_labels = labels_dense.shape[0]
  index_offset = numpy.arange(num_labels) * num_classes
  labels_one_hot = numpy.zeros((num_labels, num_classes))
  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
  return labels_one_hot
def extract_labels(filename, one_hot=False):
  """"""Extract the labels into a 1D uint8 numpy array [index].""""""
  print('Extracting', filename)
  with gzip.open(filename) as bytestream:
    magic = _read32(bytestream)
    if magic != 2049:
      raise ValueError(
          'Invalid magic number %d in MNIST label file: %s' %
          (magic, filename))
    num_items = _read32(bytestream)
    buf = bytestream.read(num_items)
    labels = numpy.frombuffer(buf, dtype=numpy.uint8)
    if one_hot:
      return dense_to_one_hot(labels)
    return labels
class DataSet(object):
  def __init__(self, images, labels, fake_data=False):
    if fake_data:
      self._num_examples = 10000
    else:
      assert images.shape[0] == labels.shape[0], (
          ""images.shape: %s labels.shape: %s"" % (images.shape,
                                                 labels.shape))
      self._num_examples = images.shape[0]
      # Convert shape from [num examples, rows, columns, depth]
      # to [num examples, rows*columns] (assuming depth == 1)
      assert images.shape[3] == 1
      images = images.reshape(images.shape[0],
                              images.shape[1] * images.shape[2])
      # Convert from [0, 255] -> [0.0, 1.0].
      images = images.astype(numpy.float32)
      images = numpy.multiply(images, 1.0 / 255.0)
    self._images = images
    self._labels = labels
    self._epochs_completed = 0
    self._index_in_epoch = 0
  @property
  def images(self):
    return self._images
  @property
  def labels(self):
    return self._labels
  @property
  def num_examples(self):
    return self._num_examples
  @property
  def epochs_completed(self):
    return self._epochs_completed
  def next_batch(self, batch_size, fake_data=False):
    """"""Return the next `batch_size`examples from this data set.""""""
    if fake_data:
      fake_image = [1.0 for _ in xrange(784)]
      fake_label = 0
      return [fake_image for _ in xrange(batch_size)], [
          fake_label for _ in xrange(batch_size)]
    start = self._index_in_epoch
    self._index_in_epoch += batch_size
    if self._index_in_epoch > self._num_examples:
      # Finished epoch
      self._epochs_completed += 1
      # Shuffle the data
      perm = numpy.arange(self._num_examples)
      numpy.random.shuffle(perm)
      self._images = self._images[perm]
      self._labels = self._labels[perm]
      # Start next epoch
      start = 0
      self._index_in_epoch = batch_size
      assert batch_size <= self._num_examples
    end = self._index_in_epoch
    return self._images[start:end], self._labels[start:end]
def read_data_sets(train_dir, fake_data=False, one_hot=False):
  class DataSets(object):
    pass
  data_sets = DataSets()
  if fake_data:
    data_sets.train = DataSet([], [], fake_data=True)
    data_sets.validation = DataSet([], [], fake_data=True)
    data_sets.test = DataSet([], [], fake_data=True)
    return data_sets
  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'
  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'
  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'
  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'
  VALIDATION_SIZE = 5000
  local_file = maybe_download(TRAIN_IMAGES, train_dir)
  train_images = extract_images(local_file)
  local_file = maybe_download(TRAIN_LABELS, train_dir)
  train_labels = extract_labels(local_file, one_hot=one_hot)
  local_file = maybe_download(TEST_IMAGES, train_dir)
  test_images = extract_images(local_file)
  local_file = maybe_download(TEST_LABELS, train_dir)
  test_labels = extract_labels(local_file, one_hot=one_hot)
  validation_images = train_images[:VALIDATION_SIZE]
  validation_labels = train_labels[:VALIDATION_SIZE]
  train_images = train_images[VALIDATION_SIZE:]
  train_labels = train_labels[VALIDATION_SIZE:]
  data_sets.train = DataSet(train_images, train_labels)
  data_sets.validation = DataSet(validation_images, validation_labels)
  data_sets.test = DataSet(test_images, test_labels)
  return data_sets
",21,0,1,0.0,10,1.0,132,0
linter_plugin.py,"""""""
Certbot PyLint plugin.

The built-in ImportChecker of Pylint does a similar job to ForbidStandardOsModule to detect
deprecated modules. You can check its behavior as a reference to what is coded here.
See https://github.com/PyCQA/pylint/blob/b20a2984c94e2946669d727dbda78735882bf50a/pylint/checkers/imports.py#L287
See https://docs.pytest.org/en/latest/writing_plugins.html
""""""
import os.path
import re

from pylint.checkers import BaseChecker

# Modules whose file is matching one of these paths can import the os module.
WHITELIST_PATHS = [
    '/acme/acme/',
    '/certbot-ci/',
    '/certbot-compatibility-test/',
]


class ForbidStandardOsModule(BaseChecker):
    """"""
    This checker ensures that standard os module (and submodules) is not imported by certbot
    modules. Otherwise an 'os-module-forbidden' error will be registered for the faulty lines.
    """"""

    name = 'forbid-os-module'
    msgs = {
        'E5001': (
            'Forbidden use of os module, certbot.compat.os must be used instead',
            'os-module-forbidden',
            'Some methods from the standard os module cannot be used for security reasons on '
            'Windows: the safe wrapper certbot.compat.os must be used instead in Certbot.'
        )
    }
    priority = -1

    def visit_import(self, node):
        os_used = any(name for name in node.names if name[0] == 'os' or name[0].startswith('os.'))
        if os_used and not _check_disabled(node):
            self.add_message('os-module-forbidden', node=node)

    def visit_importfrom(self, node):
        if node.modname == 'os' or node.modname.startswith('os.') and not _check_disabled(node):
            self.add_message('os-module-forbidden', node=node)


def register(linter):
    """"""Pylint hook to auto-register this linter""""""
    linter.register_checker(ForbidStandardOsModule(linter))


def _check_disabled(node):
    module = node.root()
    return any(path for path in WHITELIST_PATHS
               if os.path.normpath(path) in os.path.normpath(module.file))
",12,0,0,0.0,6,1.0,32,0
main.py,"from dotenv import load_dotenv
import logging
from pathlib import Path

# Create logs directory if it doesn't exist
logs_dir = Path(""logs"")
logs_dir.mkdir(exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        # File handler for general application logs
        logging.FileHandler('logs/app.log'),
        # Stream handler for console output
        logging.StreamHandler()
    ]
)

# Suppress verbose fontTools logging
logging.getLogger('fontTools').setLevel(logging.WARNING)
logging.getLogger('fontTools.subset').setLevel(logging.WARNING)
logging.getLogger('fontTools.ttLib').setLevel(logging.WARNING)

# Create logger instance
logger = logging.getLogger(__name__)

load_dotenv()

from backend.server.server import app

if __name__ == ""__main__"":
    import uvicorn
    
    logger.info(""Starting server..."")
    uvicorn.run(app, host=""0.0.0.0"", port=8000)",1,0,0,0.0,1,1.0,23,0
makeHosts.py,"#!/usr/bin/env python

# Script by gfyoung
# https://github.com/gfyoung
#
# This Python script will generate hosts files and update the readme file.

from __future__ import print_function

import argparse
import subprocess
import sys


def print_failure(msg):
    """"""
    Print a failure message.

    Parameters
    ----------
    msg : str
        The failure message to print.
    """"""

    print(""\033[91m"" + msg + ""\033[0m"")


def update_hosts_file(*flags):
    """"""
    Wrapper around running updateHostsFile.py

    Parameters
    ----------
    flags : varargs
        Commandline flags to pass into updateHostsFile.py. For more info, run
        the following command in the terminal or command prompt:

        ```
        python updateHostsFile.py -h
        ```
    """"""

    if subprocess.call([sys.executable, ""updateHostsFile.py""] + list(flags)):
        print_failure(""Failed to update hosts file"")


def update_readme_file():
    """"""
    Wrapper around running updateReadme.py
    """"""

    if subprocess.call([sys.executable, ""updateReadme.py""]):
        print_failure(""Failed to update readme file"")


def recursively_loop_extensions(extension, extensions, current_extensions):
    """"""
    Helper function that recursively calls itself to prevent manually creating
    all possible combinations of extensions.

    Will call update_hosts_file for all combinations of extensions
    """"""

    c_extensions = extensions.copy()
    c_current_extensions = current_extensions.copy()
    c_current_extensions.append(extension)

    name = ""-"".join(c_current_extensions)

    params = (""-a"", ""-n"", ""-o"", ""alternates/""+name, ""-e"") + tuple(c_current_extensions)
    update_hosts_file(*params)

    params = (""-a"", ""-n"", ""-s"", ""--nounifiedhosts"", ""-o"", ""alternates/""+name+""-only"", ""-e"") + tuple(c_current_extensions)
    update_hosts_file(*params)

    while len(c_extensions) > 0:
        recursively_loop_extensions(c_extensions.pop(0), c_extensions, c_current_extensions)


def main():
    parser = argparse.ArgumentParser(
        description=""Creates custom hosts ""
        ""file from hosts stored in ""
        ""data subfolders.""
    )
    parser.parse_args()

    # Update the unified hosts file
    update_hosts_file(""-a"")

    # List of extensions we want to generate, we will loop over them recursively to prevent manual definitions
    # Only add new extensions to the end of the array, to avoid relocating existing hosts-files
    extensions = [""fakenews"", ""gambling"", ""porn"", ""social""]

    while len(extensions) > 0:
        recursively_loop_extensions(extensions.pop(0), extensions, [])

    # Update the readme files.
    update_readme_file()


if __name__ == ""__main__"":
    main()
",10,0,0,0.0,5,1.0,37,0
manage.py,"#!/usr/bin/env python3
import os
import sys

if __name__ == ""__main__"":
    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""saleor.settings"")

    from django.core.management import execute_from_command_line

    execute_from_command_line(sys.argv)
",1,0,0,0.0,1,1.0,6,0
meizitu.py,"# encoding = utf-8
import concurrent
import os
from concurrent.futures import ThreadPoolExecutor
import requests
from bs4 import BeautifulSoup


def header(referer):

    headers = {
        'Host': 'i.meizitu.net',
        'Pragma': 'no-cache',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        'Referer': '{}'.format(referer),
    }

    return headers


def request_page(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None


def get_page_urls():

    for i in range(1, 2):
        baseurl = 'https://www.mzitu.com/page/{}'.format(i)
        html = request_page(baseurl)
        soup = BeautifulSoup(html, 'lxml')
        elements = soup.find(class_='postlist').find_all('li')
        urls = []
        for item in elements:
            url = item.find('span').find('a').get('href')
            print('页面链接：%s' % url)
            urls.append(url)

    return urls


def download_Pic(title, image_list):
    # 新建文件夹
    os.mkdir(title)
    j = 1
    # 下载图片
    for item in image_list:
        filename = '%s/%s.jpg' % (title, str(j))
        print('downloading....%s : NO.%s' % (title, str(j)))
        with open(filename, 'wb') as f:
            img = requests.get(item, headers=header(item)).content
            f.write(img)
        j += 1

def download(url):
    html = request_page(url)
    soup = BeautifulSoup(html, 'lxml')
    total = soup.find(class_='pagenavi').find_all('a')[-2].find('span').string
    title = soup.find('h2').string
    image_list = []

    for i in range(int(total)):
        html = request_page(url + '/%s' % (i + 1))
        soup = BeautifulSoup(html, 'lxml')
        img_url = soup.find('img').get('src')
        image_list.append(img_url)

    download_Pic(title, image_list)


def download_all_images(list_page_urls):
    # 获取每一个详情妹纸
    # works = len(list_page_urls)
    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as exector:
        for url in list_page_urls:
            exector.submit(download, url)


if __name__ == '__main__':
    # 获取每一页的链接和名称
    list_page_urls = get_page_urls()
    download_all_images(list_page_urls)",13,0,3,0.0,9,1.0,66,0
mixed_tabs_and_spaces.py,"def square(x):
    sum_so_far = 0
    for _ in range(x):
        sum_so_far += x
	return sum_so_far  # noqa: E999 # pylint: disable=mixed-indentation Python 3 will raise a TabError here

print(square(10))
",2,0,0,0.0,1,1.0,6,0
mymodule.py,"def generate_full_name(firstname, lastname):
      space = ' '
      fullname = firstname + space + lastname
      return fullname

def sum_two_nums (num1, num2):
    return num1 + num2
gravity = 9.81
person = {
    ""firstname"": ""Asabeneh"",
    ""age"": 250,
    ""country"": ""Finland"",
    ""city"":'Helsinki'
}


",2,0,0,0.0,0,1.0,13,0
noxfile.py,"""""""
The file for Nox
""""""
from typing import Any

import nox
from nox.sessions import Session

locations = ""ciphey/"", ""tests/"", ""docs/""
nox.options.sessions = [""tests""]
package = ""ciphey""


def install_with_constraints(session: Session, *args: str, **kwargs: Any) -> None:
    """"""Install packages constrained by Poetry's lock file.
    This function is a wrapper for nox.sessions.Session.install. It
    invokes pip to install packages inside of the session's virtualenv.
    Additionally, pip is passed a constraints file generated from
    Poetry's lock file, to ensure that the packages are pinned to the
    versions specified in poetry.lock. This allows you to manage the
    packages as Poetry development dependencies.
    Arguments:
        session: The Session object.
        args: Command-line arguments for pip.
        kwargs: Additional keyword arguments for Session.install.
    """"""
    session.run(
        ""poetry"",
        ""export"",
        ""--dev"",
        ""--format=requirements.txt"",
        ""--output=requirements.txt"",
        external=True,
    )
    session.install(""--constraint=requirements.txt"", *args, **kwargs)


# noxfile.py
@nox.session
def black(session):
    args = session.posargs or locations
    session.install(""black"")
    session.run(""black"", *args)


@nox.session(python=""3.8"")
def coverage(session: Session) -> None:
    """"""Upload coverage data.""""""
    install_with_constraints(session, ""coverage[toml]"", ""codecov"")
    session.run(""pip3"", ""install"", ""cipheydists"")
    session.run(""coverage"", ""xml"", ""--fail-under=0"")
    session.run(""codecov"", *session.posargs)


# noxfile.py
@nox.session
def docs(session: Session) -> None:
    """"""Build the documentation.""""""
    install_with_constraints(session, ""sphinx"")
    session.run(""sphinx-build"", ""docs"", ""docs/_build"")


@nox.session
def tests(session):
    session.run(""pip3"", ""install"", ""cipheydists"")
    session.run(""poetry"", ""install"", external=True)
    session.run(""poetry"", ""run"", ""pytest"", ""--cov=ciphey"")
",6,0,0,0.0,1,1.0,36,0
optimized-code.py,"from transformers import AutoModelForCasualLM, AutoTokenizer
import torch
import pandas as pd

model_name = ""bigcode/starcoder""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCasualLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"")

df = pd.read_csv(""sonarcloud_metrics.csv"")

if ""optimized_code"" not in df.columns:
    df[""optimized_code""] = """"

def op_code(code_snippet):
    prompt = f""Optimize the following Python code for better readability and efficiency:\n\n{code_snippet}""
    inputs = tokenizer(prompt, return_tensors=""pt"").to(""cuda"")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length)",0,0,0,0.0,0,1.0,0,0
pavement.py,"r""""""
This paver file is intended to help with the release process as much as
possible. It relies on virtualenv to generate 'bootstrap' environments as
independent from the user system as possible (e.g. to make sure the sphinx doc
is built against the built numpy, not an installed one).

Building changelog + notes
==========================

Assumes you have git and the binaries/tarballs in installers/::

    paver write_release
    paver write_note

This automatically put the checksum into README.rst, and writes the Changelog.

TODO
====
    - the script is messy, lots of global variables
    - make it more easily customizable (through command line args)
    - missing targets: install & test, sdist test, debian packaging
    - fix bdist_mpkg: we build the same source twice -> how to make sure we use
      the same underlying python for egg install in venv and for bdist_mpkg
""""""
import os
import hashlib
import textwrap

# The paver package needs to be installed to run tasks
import paver
from paver.easy import Bunch, options, task, sh


#-----------------------------------
# Things to be changed for a release
#-----------------------------------

# Path to the release notes
RELEASE_NOTES = 'doc/source/release/2.3.0-notes.rst'


#-------------------------------------------------------
# Hardcoded build/install dirs, virtualenv options, etc.
#-------------------------------------------------------

# Where to put the release installers
options(installers=Bunch(releasedir=""release"",
                         installersdir=os.path.join(""release"", ""installers"")),)


#-------------
# README stuff
#-------------

def _compute_hash(idirs, hashfunc):
    """"""Hash files using given hashfunc.

    Parameters
    ----------
    idirs : directory path
        Directory containing files to be hashed.
    hashfunc : hash function
        Function to be used to hash the files.

    """"""
    released = paver.path.path(idirs).listdir()
    checksums = []
    for fpath in sorted(released):
        with open(fpath, 'rb') as fin:
            fhash = hashfunc(fin.read())
            checksums.append(
                '%s  %s' % (fhash.hexdigest(), os.path.basename(fpath)))
    return checksums


def compute_md5(idirs):
    """"""Compute md5 hash of files in idirs.

    Parameters
    ----------
    idirs : directory path
        Directory containing files to be hashed.

    """"""
    return _compute_hash(idirs, hashlib.md5)


def compute_sha256(idirs):
    """"""Compute sha256 hash of files in idirs.

    Parameters
    ----------
    idirs : directory path
        Directory containing files to be hashed.

    """"""
    # better checksum so gpg signed README.rst containing the sums can be used
    # to verify the binaries instead of signing all binaries
    return _compute_hash(idirs, hashlib.sha256)


def write_release_task(options, filename='README'):
    """"""Append hashes of release files to release notes.

    This appends file hashes to the release notes and creates
    four README files of the result in various formats:

    - README.rst
    - README.rst.gpg
    - README.md
    - README.md.gpg

    The md file are created using `pandoc` so that the links are
    properly updated. The gpg files are kept separate, so that
    the unsigned files may be edited before signing if needed.

    Parameters
    ----------
    options :
        Set by ``task`` decorator.
    filename : str
        Filename of the modified notes. The file is written
        in the release directory.

    """"""
    idirs = options.installers.installersdir
    notes = paver.path.path(RELEASE_NOTES)
    rst_readme = paver.path.path(filename + '.rst')
    md_readme = paver.path.path(filename + '.md')

    # append hashes
    with open(rst_readme, 'w') as freadme:
        with open(notes) as fnotes:
            freadme.write(fnotes.read())

        freadme.writelines(textwrap.dedent(
            """"""
            Checksums
            =========

            MD5
            ---
            ::

            """"""))
        freadme.writelines([f'    {c}\n' for c in compute_md5(idirs)])

        freadme.writelines(textwrap.dedent(
            """"""
            SHA256
            ------
            ::

            """"""))
        freadme.writelines([f'    {c}\n' for c in compute_sha256(idirs)])

    # generate md file using pandoc before signing
    sh(f""pandoc -s -o {md_readme} {rst_readme}"")

    # Sign files
    if hasattr(options, 'gpg_key'):
        cmd = f'gpg --clearsign --armor --default_key {options.gpg_key}'
    else:
        cmd = 'gpg --clearsign --armor'

    sh(cmd + f' --output {rst_readme}.gpg {rst_readme}')
    sh(cmd + f' --output {md_readme}.gpg {md_readme}')


@task
def write_release(options):
    """"""Write the README files.

    Two README files are generated from the release notes, one in ``rst``
    markup for the general release, the other in ``md`` markup for the github
    release notes.

    Parameters
    ----------
    options :
        Set by ``task`` decorator.

    """"""
    rdir = options.installers.releasedir
    write_release_task(options, os.path.join(rdir, 'README'))
",7,0,0,0.0,3,1.0,59,0
pdm_build.py,"import os
from typing import Any, Dict

from pdm.backend.hooks import Context

TIANGOLO_BUILD_PACKAGE = os.getenv(""TIANGOLO_BUILD_PACKAGE"", ""fastapi"")


def pdm_build_initialize(context: Context) -> None:
    metadata = context.config.metadata
    # Get custom config for the current package, from the env var
    config: Dict[str, Any] = context.config.data[""tool""][""tiangolo""][
        ""_internal-slim-build""
    ][""packages""].get(TIANGOLO_BUILD_PACKAGE)
    if not config:
        return
    project_config: Dict[str, Any] = config[""project""]
    # Override main [project] configs with custom configs for this package
    for key, value in project_config.items():
        metadata[key] = value
",3,0,0,0.0,2,1.0,14,0
pip_build.py,"""""""Script to create (and optionally install) a `.whl` archive for Keras 3.

Usage:

1. Create a `.whl` file in `dist/`:

```
python3 pip_build.py
```

2. Also install the new package immediately after:

```
python3 pip_build.py --install
```
""""""

import argparse
import datetime
import glob
import os
import pathlib
import re
import shutil

# Needed because importing torch after TF causes the runtime to crash
import torch  # noqa: F401

package = ""keras""
build_directory = ""tmp_build_dir""
dist_directory = ""dist""
to_copy = [""pyproject.toml"", ""README.md""]


def export_version_string(version, is_nightly=False, rc_index=None):
    """"""Export Version and Package Name.""""""
    if is_nightly:
        date = datetime.datetime.now()
        version += f"".dev{date:%Y%m%d%H}""
        # Update `name = ""keras""` with ""keras-nightly""
        pyproj_pth = pathlib.Path(""pyproject.toml"")
        pyproj_str = pyproj_pth.read_text().replace(
            'name = ""keras""', 'name = ""keras-nightly""'
        )
        pyproj_pth.write_text(pyproj_str)
    elif rc_index is not None:
        version += ""rc"" + str(rc_index)

    # Make sure to export the __version__ string
    with open(os.path.join(package, ""src"", ""version.py"")) as f:
        init_contents = f.read()
    with open(os.path.join(package, ""src"", ""version.py""), ""w"") as f:
        init_contents = re.sub(
            ""\n__version__ = .*\n"",
            f'\n__version__ = ""{version}""\n',
            init_contents,
        )
        f.write(init_contents)


def ignore_files(_, filenames):
    return [f for f in filenames if f.endswith(""_test.py"")]


def copy_source_to_build_directory(root_path):
    # Copy sources (`keras/` directory and setup files) to build
    # directory
    os.chdir(root_path)
    os.mkdir(build_directory)
    shutil.copytree(
        package, os.path.join(build_directory, package), ignore=ignore_files
    )
    for fname in to_copy:
        shutil.copy(fname, os.path.join(f""{build_directory}"", fname))
    os.chdir(build_directory)


def build(root_path, is_nightly=False, rc_index=None):
    if os.path.exists(build_directory):
        raise ValueError(f""Directory already exists: {build_directory}"")

    try:
        copy_source_to_build_directory(root_path)
        move_tf_keras_directory()

        from keras.src.version import __version__  # noqa: E402

        export_version_string(__version__, is_nightly, rc_index)
        return build_and_save_output(root_path, __version__)
    finally:
        # Clean up: remove the build directory (no longer needed)
        shutil.rmtree(build_directory)


def move_tf_keras_directory():
    """"""Move `keras/api/_tf_keras` to `keras/_tf_keras`, update references.""""""
    shutil.move(os.path.join(package, ""api"", ""_tf_keras""), ""keras"")
    with open(os.path.join(package, ""api"", ""__init__.py"")) as f:
        contents = f.read()
        contents = contents.replace(""from keras.api import _tf_keras"", """")
    with open(os.path.join(package, ""api"", ""__init__.py""), ""w"") as f:
        f.write(contents)
    # Replace `keras.api._tf_keras` with `keras._tf_keras`.
    for root, _, fnames in os.walk(os.path.join(package, ""_tf_keras"")):
        for fname in fnames:
            if fname.endswith("".py""):
                tf_keras_fpath = os.path.join(root, fname)
                with open(tf_keras_fpath) as f:
                    contents = f.read()
                    contents = contents.replace(
                        ""keras.api._tf_keras"", ""keras._tf_keras""
                    )
                with open(tf_keras_fpath, ""w"") as f:
                    f.write(contents)


def build_and_save_output(root_path, __version__):
    # Build the package
    os.system(""python3 -m build"")

    # Save the dist files generated by the build process
    os.chdir(root_path)
    if not os.path.exists(dist_directory):
        os.mkdir(dist_directory)
    for fpath in glob.glob(
        os.path.join(build_directory, dist_directory, ""*.*"")
    ):
        shutil.copy(fpath, dist_directory)

    # Find the .whl file path
    whl_path = None
    for fname in os.listdir(dist_directory):
        if __version__ in fname and fname.endswith("".whl""):
            whl_path = os.path.abspath(os.path.join(dist_directory, fname))
    if whl_path:
        print(f""Build successful. Wheel file available at {whl_path}"")
    else:
        print(""Build failed."")
    return whl_path


def install_whl(whl_fpath):
    print(f""Installing wheel file: {whl_fpath}"")
    os.system(f""pip3 install {whl_fpath} --force-reinstall --no-dependencies"")


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--install"",
        action=""store_true"",
        help=""Whether to install the generated wheel file."",
    )
    parser.add_argument(
        ""--nightly"",
        action=""store_true"",
        help=""Whether to generate nightly wheel file."",
    )
    parser.add_argument(
        ""--rc"",
        type=int,
        help=""Specify `[0-9] when generating RC wheels."",
    )
    args = parser.parse_args()
    root_path = pathlib.Path(__file__).parent.resolve()
    whl_path = build(root_path, args.nightly, args.rc)
    if whl_path and args.install:
        install_whl(whl_path)
",23,0,0,0.0,22,1.0,115,0
pip_deploy.py,"import subprocess



subprocess.call('python setup.py sdist')

subprocess.call('python setup.py bdist_wheel --universal')

subprocess.call('twine upload dist/*')

",0,0,0,0.0,0,1.0,4,0
release.py,"#!/usr/bin/env python
from subprocess import call
import os
import re


version = None


def get_new_setup_py_lines():
    global version
    with open('setup.py', 'r') as sf:
        current_setup = sf.readlines()
    for line in current_setup:
        if line.startswith('VERSION = '):
            major, minor = re.findall(r""VERSION = '(\d+)\.(\d+)'"", line)[0]
            version = ""{}.{}"".format(major, int(minor) + 1)
            yield ""VERSION = '{}'\n"".format(version)
        else:
            yield line


lines = list(get_new_setup_py_lines())
with open('setup.py', 'w') as sf:
    sf.writelines(lines)

call('git pull', shell=True)
call('git commit -am ""Bump to {}""'.format(version), shell=True)
call('git tag {}'.format(version), shell=True)
call('git push', shell=True)
call('git push --tags', shell=True)

env = os.environ
env['CONVERT_README'] = 'true'
call('rm -rf dist/*', shell=True, env=env)
call('python setup.py sdist bdist_wheel', shell=True, env=env)
call('twine upload dist/*', shell=True, env=env)
",3,0,0,0.0,4,1.0,28,0
release_utils.py,"import argparse
from typing import Tuple


def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:
    current_ver = find_version(""fairseq/version.txt"")
    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]
    major, minor, patch = version_list[0], version_list[1], version_list[2]
    if release_type == ""patch"":
        patch += 1
    elif release_type == ""minor"":
        minor += 1
        patch = 0
    elif release_type == ""major"":
        major += 1
        minor = patch = 0
    else:
        raise ValueError(
            ""Incorrect release type specified. Acceptable types are major, minor and patch.""
        )

    new_version_tuple = (major, minor, patch)
    new_version_str = ""."".join([str(x) for x in new_version_tuple])
    new_tag_str = ""v"" + new_version_str
    return new_version_tuple, new_version_str, new_tag_str


def find_version(version_file_path) -> str:
    with open(version_file_path) as f:
        version = f.read().strip()
        return version


def update_version(new_version_str) -> None:
    """"""
    given the current version, update the version to the
    next version depending on the type of release.
    """"""

    with open(""fairseq/version.txt"", ""w"") as writer:
        writer.write(new_version_str)


def main(args):
    if args.release_type in [""major"", ""minor"", ""patch""]:
        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)
    else:
        raise ValueError(""Incorrect release type specified"")

    if args.update_version:
        update_version(new_version)

    print(new_version, new_tag)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Versioning utils"")
    parser.add_argument(
        ""--release-type"",
        type=str,
        required=True,
        help=""type of release = major/minor/patch"",
    )
    parser.add_argument(
        ""--update-version"",
        action=""store_true"",
        required=False,
        help=""updates the version in fairseq/version.txt"",
    )

    args = parser.parse_args()
    main(args)
",8,0,1,0.0,8,1.0,53,0
render_readme.py,"#!/usr/bin/env python

import re
from pathlib import Path

README_TEMPLATE_FILEPATH = ""readme_template.md""
GETTING_STARTED_TEMPLATE_FILEPATH = ""guides/01_getting-started/01_quickstart.md""

readme_template = Path(README_TEMPLATE_FILEPATH).read_text()
getting_started_template = Path(GETTING_STARTED_TEMPLATE_FILEPATH).read_text()
getting_started_template = getting_started_template.replace(""# Quickstart"", """")
getting_started_template = getting_started_template.replace(""Tip:"", ""> [!TIP]\n >"")

# Extract all the code and demo tags from the getting started template
code_tags = re.findall(r""\$code_([^\s]+)"", getting_started_template)
demo_tags = re.findall(r""\$demo_([^\s]+)"", getting_started_template)
codes = {}
demos = {}

for src in code_tags:
    context = Path(f""demo/{src}/run.py"").read_text()
    # Replace the condition to run the demo directly with actual launch code
    context = re.sub(r""if __name__(.*[\n$]*)*"", ""demo.launch()"", context)
    codes[src] = f""```python\n{context}\n```\n""  # Convert to Markdown code block

for src in demo_tags:
    demos[src] = f""![`{src}` demo](demo/{src}/screenshot.gif)""

# Replace the headers in the getting started template with a smaller header (e.g. H3 instead of H2) to
# make the README more readable and less cluttered.
getting_started_template = re.sub(r""^(#+)"", r""#\1"", getting_started_template, flags=re.MULTILINE)
readme_template = readme_template.replace(""$getting_started"", getting_started_template)

# Now put the codes and the screenshots in the README template
readme_template = re.sub(r""\$code_([^\s]+)"", lambda x: codes[x.group(1)], readme_template)
readme_template = re.sub(r""\$demo_([^\s]+)"", lambda x: demos[x.group(1)], readme_template)

# Save the README template to the actual README.md file (with a note about the editing)
EDITING_NOTE = (""<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR ""
                ""`guides/1)getting_started/1)quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->"")
Path(""README.md"").write_text(f""{EDITING_NOTE}\n\n{readme_template}"")
",2,1,0,0.0,2,1.0,25,0
run-venv.py,"#!./venv/bin/python
import re
import sys

from glances import main

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())
",1,0,0,0.0,1,1.0,6,0
run.py,"#!/usr/bin/python
import re
import sys

from glances import main

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())
",1,0,0,0.0,1,1.0,6,0
run_example.py,"# import tutorials.keras.text_NER as ft
# import tutorials.keras.brat_tag as ft
import tutorials.RecommenderSystems.rs_rating_demo as ft
# from middleware.utils import TimeStat, Chart
# import matplotlib.pyplot as plt
# import matplotlib.gridspec as gridspec
# from matplotlib.font_manager import FontProperties
# plt.rcParams['font.sans-serif'] = ['SimHei']
# plt.rcParams['axes.unicode_minus'] = False


def main():
    ft.main()
    # x=y=[1,2,3]
    # plt.plot(x, y, color='g', linestyle='-')  # 绘制
    # plt.grid(True, ls = '--')
    # plt.show()

if __name__ == ""__main__"":
    main()
",2,0,3,0.0,1,1.0,5,0
runtests.py,"#!/usr/bin/env python

import argparse
import os
import shutil
import sys
import warnings

from django.core.management import execute_from_command_line

os.environ[""DJANGO_SETTINGS_MODULE""] = ""wagtail.test.settings""


def make_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--deprecation"",
        choices=[""all"", ""pending"", ""imminent"", ""none""],
        default=""imminent"",
    )
    parser.add_argument(""--postgres"", action=""store_true"")
    parser.add_argument(""--elasticsearch7"", action=""store_true"")
    parser.add_argument(""--elasticsearch8"", action=""store_true"")
    parser.add_argument(""--emailuser"", action=""store_true"")
    parser.add_argument(""--disabletimezone"", action=""store_true"")
    parser.add_argument(""--bench"", action=""store_true"")
    return parser


def parse_args(args=None):
    return make_parser().parse_known_args(args)


def runtests():
    args, rest = parse_args()

    only_wagtail = r""^wagtail(\.|$)""
    if args.deprecation == ""all"":
        # Show all deprecation warnings from all packages
        warnings.simplefilter(""default"", DeprecationWarning)
        warnings.simplefilter(""default"", PendingDeprecationWarning)
    elif args.deprecation == ""pending"":
        # Show all deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
        warnings.filterwarnings(
            ""default"", category=PendingDeprecationWarning, module=only_wagtail
        )
    elif args.deprecation == ""imminent"":
        # Show only imminent deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
    elif args.deprecation == ""none"":
        # Deprecation warnings are ignored by default
        pass

    if args.postgres:
        os.environ[""DATABASE_ENGINE""] = ""django.db.backends.postgresql""

    elif args.elasticsearch7:
        os.environ.setdefault(""ELASTICSEARCH_URL"", ""http://localhost:9200"")
        os.environ.setdefault(""ELASTICSEARCH_VERSION"", ""7"")
    elif args.elasticsearch8:
        os.environ.setdefault(""ELASTICSEARCH_URL"", ""http://localhost:9200"")
        os.environ.setdefault(""ELASTICSEARCH_VERSION"", ""8"")

    elif ""ELASTICSEARCH_URL"" in os.environ:
        # forcibly delete the ELASTICSEARCH_URL setting to skip those tests
        del os.environ[""ELASTICSEARCH_URL""]

    if args.emailuser:
        os.environ[""USE_EMAIL_USER_MODEL""] = ""1""

    if args.disabletimezone:
        os.environ[""DISABLE_TIMEZONE""] = ""1""

    if args.bench:
        benchmarks = [
            ""wagtail.admin.tests.benches"",
        ]

        argv = [sys.argv[0], ""test"", ""-v2""] + benchmarks + rest
    else:
        argv = [sys.argv[0], ""test""] + rest

    try:
        execute_from_command_line(argv)
    finally:
        from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT

        shutil.rmtree(STATIC_ROOT, ignore_errors=True)
        shutil.rmtree(MEDIA_ROOT, ignore_errors=True)


if __name__ == ""__main__"":
    runtests()
",9,0,0,0.0,13,1.0,71,0
setting.py,"# -*- coding: utf-8 -*-
""""""
-------------------------------------------------
   File Name：     setting.py
   Description :   配置文件
   Author :        JHao
   date：          2019/2/15
-------------------------------------------------
   Change Activity:
                   2019/2/15:
-------------------------------------------------
""""""

BANNER = r""""""
****************************************************************
*** ______  ********************* ______ *********** _  ********
*** | ___ \_ ******************** | ___ \ ********* | | ********
*** | |_/ / \__ __   __  _ __   _ | |_/ /___ * ___  | | ********
*** |  __/|  _// _ \ \ \/ /| | | ||  __// _ \ / _ \ | | ********
*** | |   | | | (_) | >  < \ |_| || |  | (_) | (_) || |___  ****
*** \_|   |_|  \___/ /_/\_\ \__  |\_|   \___/ \___/ \_____/ ****
****                       __ / /                          *****
************************* /___ / *******************************
*************************       ********************************
****************************************************************
""""""

VERSION = ""2.4.0""

# ############### server config ###############
HOST = ""0.0.0.0""

PORT = 5010

# ############### database config ###################
# db connection uri
# example:
#      Redis: redis://:password@ip:port/db
#      Ssdb:  ssdb://:password@ip:port
DB_CONN = 'redis://:pwd@127.0.0.1:6379/0'

# proxy table name
TABLE_NAME = 'use_proxy'


# ###### config the proxy fetch function ######
PROXY_FETCHER = [
    ""freeProxy01"",
    ""freeProxy02"",
    ""freeProxy03"",
    ""freeProxy04"",
    ""freeProxy05"",
    ""freeProxy06"",
    ""freeProxy07"",
    ""freeProxy08"",
    ""freeProxy09"",
    ""freeProxy10"",
    ""freeProxy11""
]

# ############# proxy validator #################
# 代理验证目标网站
HTTP_URL = ""http://httpbin.org""

HTTPS_URL = ""https://www.qq.com""

# 代理验证时超时时间
VERIFY_TIMEOUT = 10

# 近PROXY_CHECK_COUNT次校验中允许的最大失败次数,超过则剔除代理
MAX_FAIL_COUNT = 0

# 近PROXY_CHECK_COUNT次校验中允许的最大失败率,超过则剔除代理
# MAX_FAIL_RATE = 0.1

# proxyCheck时代理数量少于POOL_SIZE_MIN触发抓取
POOL_SIZE_MIN = 20

# ############# proxy attributes #################
# 是否启用代理地域属性
PROXY_REGION = True

# ############# scheduler config #################

# Set the timezone for the scheduler forcely (optional)
# If it is running on a VM, and
#   ""ValueError: Timezone offset does not match system offset""
#   was raised during scheduling.
# Please uncomment the following line and set a timezone for the scheduler.
# Otherwise it will detect the timezone from the system automatically.

TIMEZONE = ""Asia/Shanghai""
",0,0,1,0.0,0,1.0,38,0
setup.py,"#!/usr/bin/env python

from pathlib import Path

from setuptools import find_packages, setup

here = Path(__file__).resolve().parent
README = (here / ""README.rst"").read_text(encoding=""utf-8"")
VERSION = (here / ""VERSION"").read_text(encoding=""utf-8"").strip()

excluded_packages = [""docs"", ""tests"", ""tests.*""]


# this module can be zip-safe if the zipimporter implements iter_modules or if
# pkgutil.iter_importer_modules has registered a dispatch for the zipimporter.
try:
    import pkgutil
    import zipimport

    zip_safe = (
        hasattr(zipimport.zipimporter, ""iter_modules"")
        or zipimport.zipimporter in pkgutil.iter_importer_modules.registry.keys()
    )
except AttributeError:
    zip_safe = False

setup(
    name=""Faker"",
    version=VERSION,
    description=""Faker is a Python package that generates fake data for you."",
    long_description=README,
    entry_points={
        ""console_scripts"": [""faker=faker.cli:execute_from_command_line""],
        ""pytest11"": [""faker = faker.contrib.pytest.plugin""],
    },
    classifiers=[
        # See https://pypi.org/pypi?%3Aaction=list_classifiers
        ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Intended Audience :: Developers"",
        ""Programming Language :: Python :: 3.9"",
        ""Programming Language :: Python :: 3.10"",
        ""Programming Language :: Python :: 3.11"",
        ""Programming Language :: Python :: 3.12"",
        ""Programming Language :: Python :: 3.13"",
        ""Programming Language :: Python :: Implementation :: CPython"",
        ""Programming Language :: Python :: Implementation :: PyPy"",
        ""Topic :: Software Development :: Libraries :: Python Modules"",
        ""Topic :: Software Development :: Testing"",
        ""Topic :: Utilities"",
        ""License :: OSI Approved :: MIT License"",
    ],
    keywords=""faker fixtures data test mock generator"",
    author=""joke2k"",
    author_email=""joke2k@gmail.com"",
    url=""https://github.com/joke2k/faker"",
    project_urls={
        ""Bug Tracker"": ""https://github.com/joke2k/faker/issues"",
        ""Changes"": ""https://github.com/joke2k/faker/blob/master/CHANGELOG.md"",
        ""Documentation"": ""http://faker.rtfd.org/"",
        ""Source Code"": ""https://github.com/joke2k/faker"",
    },
    license=""MIT License"",
    packages=find_packages(exclude=excluded_packages),
    package_data={
        ""faker"": [""py.typed"", ""proxy.pyi""],
    },
    platforms=[""any""],
    zip_safe=zip_safe,
    install_requires=[""tzdata""],
    python_requires="">=3.9"",
)
",1,0,0,0.0,2,1.0,60,0
shuaia.py,"# -*- coding:UTF-8 -*-
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import requests
import os
import time

if __name__ == '__main__':
	list_url = []
	for num in range(1,3):
		if num == 1:
			url = 'http://www.shuaia.net/index.html'
		else:
			url = 'http://www.shuaia.net/index_%d.html' % num
		headers = {
				""User-Agent"":""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36""
		}
		req = requests.get(url = url,headers = headers)
		req.encoding = 'utf-8'
		html = req.text
		bf = BeautifulSoup(html, 'lxml')
		targets_url = bf.find_all(class_='item-img')
		
		for each in targets_url:
			list_url.append(each.img.get('alt') + '=' + each.get('href'))

	print('连接采集完成')

	for each_img in list_url:
		img_info = each_img.split('=')
		target_url = img_info[1]
		filename = img_info[0] + '.jpg'
		print('下载：' + filename)
		headers = {
			""User-Agent"":""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36""
		}
		img_req = requests.get(url = target_url,headers = headers)
		img_req.encoding = 'utf-8'
		img_html = img_req.text
		img_bf_1 = BeautifulSoup(img_html, 'lxml')
		img_url = img_bf_1.find_all('div', class_='wr-single-content-list')
		img_bf_2 = BeautifulSoup(str(img_url), 'lxml')
		img_url = 'http://www.shuaia.net' + img_bf_2.div.img.get('src')
		if 'images' not in os.listdir():
			os.makedirs('images')
		urlretrieve(url = img_url,filename = 'images/' + filename)
		time.sleep(1)

	print('下载完成！')",6,0,0,0.0,15,1.0,43,0
sort.py,"#!/usr/bin/env python
# coding: utf-8

""""""
    The approach taken is explained below. I decided to do it simply.
    Initially I was considering parsing the data into some sort of
    structure and then generating an appropriate README. I am still
    considering doing it - but for now this should work. The only issue
    I see is that it only sorts the entries at the lowest level, and that
    the order of the top-level contents do not match the order of the actual
    entries.

    This could be extended by having nested blocks, sorting them recursively
    and flattening the end structure into a list of lines. Revision 2 maybe ^.^.
""""""

def sort_blocks():
    # First, we load the current README into memory
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.read()

    # Separating the 'table of contents' from the contents (blocks)
    table_of_contents = ''.join(read_me.split('- - -')[0])
    blocks = ''.join(read_me.split('- - -')[1]).split('\n# ')
    for i in range(len(blocks)):
        if i == 0:
            blocks[i] = blocks[i] + '\n'
        else:
            blocks[i] = '# ' + blocks[i] + '\n'

    # Sorting the libraries
    inner_blocks = sorted(blocks[0].split('##'))
    for i in range(1, len(inner_blocks)):
        if inner_blocks[i][0] != '#':
            inner_blocks[i] = '##' + inner_blocks[i]
    inner_blocks = ''.join(inner_blocks)

    # Replacing the non-sorted libraries by the sorted ones and gathering all at the final_README file
    blocks[0] = inner_blocks
    final_README = table_of_contents + '- - -' + ''.join(blocks)

    with open('README.md', 'w+') as sorted_file:
        sorted_file.write(final_README)

def main():
    # First, we load the current README into memory as an array of lines
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.readlines()

    # Then we cluster the lines together as blocks
    # Each block represents a collection of lines that should be sorted
    # This was done by assuming only links ([...](...)) are meant to be sorted
    # Clustering is done by indentation
    blocks = []
    last_indent = None
    for line in read_me:
        s_line = line.lstrip()
        indent = len(line) - len(s_line)

        if any([s_line.startswith(s) for s in ['* [', '- [']]):
            if indent == last_indent:
                blocks[-1].append(line)
            else:
                blocks.append([line])
            last_indent = indent
        else:
            blocks.append([line])
            last_indent = None

    with open('README.md', 'w+') as sorted_file:
        # Then all of the blocks are sorted individually
        blocks = [
            ''.join(sorted(block, key=str.lower)) for block in blocks
        ]
        # And the result is written back to README.md
        sorted_file.write(''.join(blocks))

    # Then we call the sorting method
    sort_blocks()


if __name__ == ""__main__"":
    main()
",10,0,2,0.0,16,1.0,44,0
synthesizer_preprocess_audio.py,"from synthesizer.preprocess import preprocess_dataset
from synthesizer.hparams import hparams
from utils.argutils import print_args
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Preprocesses audio files from datasets, encodes them as mel spectrograms ""
                    ""and writes them to  the disk. Audio files are also saved, to be used by the ""
                    ""vocoder for training."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(""datasets_root"", type=Path, help=\
        ""Path to the directory containing your LibriSpeech/TTS datasets."")
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\
        ""Path to the output directory that will contain the mel spectrograms, the audios and the ""
        ""embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/"")
    parser.add_argument(""-n"", ""--n_processes"", type=int, default=4, help=\
        ""Number of processes in parallel."")
    parser.add_argument(""-s"", ""--skip_existing"", action=""store_true"", help=\
        ""Whether to overwrite existing files with the same name. Useful if the preprocessing was ""
        ""interrupted."")
    parser.add_argument(""--hparams"", type=str, default="""", help=\
        ""Hyperparameter overrides as a comma-separated list of name-value pairs"")
    parser.add_argument(""--no_alignments"", action=""store_true"", help=\
        ""Use this option when dataset does not include alignments\
        (these are used to split long audio files into sub-utterances.)"")
    parser.add_argument(""--datasets_name"", type=str, default=""LibriSpeech"", help=\
        ""Name of the dataset directory to process."")
    parser.add_argument(""--subfolders"", type=str, default=""train-clean-100,train-clean-360"", help=\
        ""Comma-separated list of subfolders to process inside your dataset directory"")
    args = parser.parse_args()

    # Process the arguments
    if not hasattr(args, ""out_dir""):
        args.out_dir = args.datasets_root.joinpath(""SV2TTS"", ""synthesizer"")

    # Create directories
    assert args.datasets_root.exists()
    args.out_dir.mkdir(exist_ok=True, parents=True)

    # Preprocess the dataset
    print_args(args, parser)
    args.hparams = hparams.parse(args.hparams)
    preprocess_dataset(**vars(args))
",2,0,0,0.0,3,1.0,39,0
synthesizer_preprocess_embeds.py,"from synthesizer.preprocess import create_embeddings
from utils.argutils import print_args
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Creates embeddings for the synthesizer from the LibriSpeech utterances."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(""synthesizer_root"", type=Path, help=\
        ""Path to the synthesizer training data that contains the audios and the train.txt file. ""
        ""If you let everything as default, it should be <datasets_root>/SV2TTS/synthesizer/."")
    parser.add_argument(""-e"", ""--encoder_model_fpath"", type=Path,
                        default=""saved_models/default/encoder.pt"", help=\
        ""Path your trained encoder model."")
    parser.add_argument(""-n"", ""--n_processes"", type=int, default=4, help= \
        ""Number of parallel processes. An encoder is created for each, so you may need to lower ""
        ""this value on GPUs with low memory. Set it to 1 if CUDA is unhappy."")
    args = parser.parse_args()

    # Preprocess the dataset
    print_args(args, parser)
    create_embeddings(**vars(args))
",1,0,0,0.0,1,1.0,21,0
synthesizer_train.py,"from pathlib import Path

from synthesizer.hparams import hparams
from synthesizer.train import train
from utils.argutils import print_args
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""run_id"", type=str, help= \
        ""Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state ""
        ""from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved ""
        ""states and restart from scratch."")
    parser.add_argument(""syn_dir"", type=Path, help= \
        ""Path to the synthesizer directory that contains the ground truth mel spectrograms, ""
        ""the wavs and the embeds."")
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"", help=\
        ""Path to the output directory that will contain the saved model weights and the logs."")
    parser.add_argument(""-s"", ""--save_every"", type=int, default=1000, help= \
        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""
        ""model."")
    parser.add_argument(""-b"", ""--backup_every"", type=int, default=25000, help= \
        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""
        ""model."")
    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \
        ""Do not load any saved model and restart from scratch."")
    parser.add_argument(""--hparams"", default="""", help=\
        ""Hyperparameter overrides as a comma-separated list of name=value pairs"")
    args = parser.parse_args()
    print_args(args, parser)

    args.hparams = hparams.parse(args.hparams)

    # Run the training
    train(**vars(args))
",1,0,0,0.0,1,1.0,30,0
test.py,"#!./kitty/launcher/kitty +launch
# License: GPL v3 Copyright: 2016, Kovid Goyal <kovid at kovidgoyal.net>

import importlib


def main() -> None:
    m = importlib.import_module('kitty_tests.main')
    getattr(m, 'main')()


if __name__ == '__main__':
    main()
",2,0,0,0.0,1,1.0,6,0
train.py,"#!/usr/bin/env python3 -u
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""
Legacy entry point. Use fairseq_cli/train.py or fairseq-train instead.
""""""

from fairseq_cli.train import cli_main


if __name__ == ""__main__"":
    cli_main()
",1,0,0,0.0,1,1.0,3,0
update.py,"import os
import sys
import json
import re
import shutil


def update():
    from Config import config
    config.parse(silent=True)

    if getattr(sys, 'source_update_dir', False):
        if not os.path.isdir(sys.source_update_dir):
            os.makedirs(sys.source_update_dir)
        source_path = sys.source_update_dir.rstrip(""/"")
    else:
        source_path = os.getcwd().rstrip(""/"")

    if config.dist_type.startswith(""bundle_linux""):
        runtime_path = os.path.normpath(os.path.dirname(sys.executable) + ""/../.."")
    else:
        runtime_path = os.path.dirname(sys.executable)

    updatesite_path = config.data_dir + ""/"" + config.updatesite

    sites_json = json.load(open(config.data_dir + ""/sites.json""))
    updatesite_bad_files = sites_json.get(config.updatesite, {}).get(""cache"", {}).get(""bad_files"", {})
    print(
        ""Update site path: %s, bad_files: %s, source path: %s, runtime path: %s, dist type: %s"" %
        (updatesite_path, len(updatesite_bad_files), source_path, runtime_path, config.dist_type)
    )

    updatesite_content_json = json.load(open(updatesite_path + ""/content.json""))
    inner_paths = list(updatesite_content_json.get(""files"", {}).keys())
    inner_paths += list(updatesite_content_json.get(""files_optional"", {}).keys())

    # Keep file only in ZeroNet directory
    inner_paths = [inner_path for inner_path in inner_paths if re.match(""^(core|bundle)"", inner_path)]

    # Checking plugins
    plugins_enabled = []
    plugins_disabled = []
    if os.path.isdir(""%s/plugins"" % source_path):
        for dir in os.listdir(""%s/plugins"" % source_path):
            if dir.startswith(""disabled-""):
                plugins_disabled.append(dir.replace(""disabled-"", """"))
            else:
                plugins_enabled.append(dir)
        print(""Plugins enabled:"", plugins_enabled, ""disabled:"", plugins_disabled)

    update_paths = {}

    for inner_path in inner_paths:
        if "".."" in inner_path:
            continue
        inner_path = inner_path.replace(""\\"", ""/"").strip(""/"")  # Make sure we have unix path
        print(""."", end="" "")
        if inner_path.startswith(""core""):
            dest_path = source_path + ""/"" + re.sub(""^core/"", """", inner_path)
        elif inner_path.startswith(config.dist_type):
            dest_path = runtime_path + ""/"" + re.sub(""^bundle[^/]+/"", """", inner_path)
        else:
            continue

        if not dest_path:
            continue

        # Keep plugin disabled/enabled status
        match = re.match(re.escape(source_path) + ""/plugins/([^/]+)"", dest_path)
        if match:
            plugin_name = match.group(1).replace(""disabled-"", """")
            if plugin_name in plugins_enabled:  # Plugin was enabled
                dest_path = dest_path.replace(""plugins/disabled-"" + plugin_name, ""plugins/"" + plugin_name)
            elif plugin_name in plugins_disabled:  # Plugin was disabled
                dest_path = dest_path.replace(""plugins/"" + plugin_name, ""plugins/disabled-"" + plugin_name)
            print(""P"", end="" "")

        dest_dir = os.path.dirname(dest_path)
        if dest_dir and not os.path.isdir(dest_dir):
            os.makedirs(dest_dir)

        if dest_dir != dest_path.strip(""/""):
            update_paths[updatesite_path + ""/"" + inner_path] = dest_path

    num_ok = 0
    num_rename = 0
    num_error = 0
    for path_from, path_to in update_paths.items():
        print(""-"", path_from, ""->"", path_to)
        if not os.path.isfile(path_from):
            print(""Missing file"")
            continue

        data = open(path_from, ""rb"").read()

        try:
            open(path_to, 'wb').write(data)
            num_ok += 1
        except Exception as err:
            try:
                print(""Error writing: %s. Renaming old file as workaround..."" % err)
                path_to_tmp = path_to + ""-old""
                if os.path.isfile(path_to_tmp):
                    os.unlink(path_to_tmp)
                os.rename(path_to, path_to_tmp)
                num_rename += 1
                open(path_to, 'wb').write(data)
                shutil.copymode(path_to_tmp, path_to)  # Copy permissions
                print(""Write done after rename!"")
                num_ok += 1
            except Exception as err:
                print(""Write error after rename: %s"" % err)
                num_error += 1
    print(""* Updated files: %s, renamed: %s, error: %s"" % (num_ok, num_rename, num_error))


if __name__ == ""__main__"":
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), ""src""))  # Imports relative to src

    update()
",21,0,1,0.0,45,1.0,96,0
update_version.py,"import os
import subprocess


def get_version():
    command = [""git"", ""describe"", ""--tags""]
    try:
        version = subprocess.check_output(command).decode().strip()
        version_parts = version.split(""-"")
        if len(version_parts) > 1 and version_parts[0].startswith(""magic_pdf""):
            return version_parts[1]
        else:
            raise ValueError(f""Invalid version tag {version}. Expected format is magic_pdf-<version>-released."")
    except Exception as e:
        print(e)
        return ""0.0.0""


def write_version_to_commons(version):
    commons_path = os.path.join(os.path.dirname(__file__), 'magic_pdf', 'libs', 'version.py')
    with open(commons_path, 'w') as f:
        f.write(f'__version__ = ""{version}""\n')


if __name__ == '__main__':
    version_name = get_version()
    write_version_to_commons(version_name)
",5,0,0,0.0,5,1.0,21,0
updateReadme.py,"#!/usr/bin/env python3

# Script by Steven Black
# https://github.com/StevenBlack
#
# This Python script will update the readme files in this repo.

import json
import os
import time
from string import Template

# Project Settings
BASEDIR_PATH = os.path.dirname(os.path.realpath(__file__))
README_TEMPLATE = os.path.join(BASEDIR_PATH, ""readme_template.md"")
README_FILENAME = ""readme.md""
README_DATA_FILENAME = ""readmeData.json""


def main():
    s = Template(
        ""${description} | [Readme](https://github.com/StevenBlack/""
        ""hosts/blob/master/${location}readme.md) | ""
        ""[link](https://raw.githubusercontent.com/StevenBlack/""
        ""hosts/master/${location}hosts) | ""
        ""${fmtentries} | ""
        ""[link](http://sbc.io/hosts/${location}hosts)""
    )
    with open(README_DATA_FILENAME, ""r"", encoding=""utf-8"", newline=""\n"") as f:
        data = json.load(f)

    keys = list(data.keys())
    # Sort by the number of en-dashes in the key
    # and then by the key string itself.
    keys.sort(key=lambda item: (item.replace(""-only"", """").count(""-""), item.replace(""-only"", """")))

    toc_rows = """"
    for key in keys:
        data[key][""fmtentries""] = ""{:,}"".format(data[key][""entries""])
        if key == ""base"":
            data[key][""description""] = ""Unified hosts = **(adware + malware)**""
        else:
            if data[key][""no_unified_hosts""]:
                data[key][""description""] = (
                    ""**"" + key.replace(""-only"", """").replace(""-"", "" + "") + ""**""
                )
            else:
                data[key][""description""] = (
                    ""Unified hosts **+ "" + key.replace(""-"", "" + "") + ""**""
                )

        if ""\\"" in data[key][""location""]:
            data[key][""location""] = data[key][""location""].replace(""\\"", ""/"")

        toc_rows += s.substitute(data[key]) + ""\n""

    row_defaults = {
        ""name"": """",
        ""homeurl"": """",
        ""url"": """",
        ""license"": """",
        ""issues"": """",
        ""description"": """",
    }

    t = Template(
        ""${name} |[link](${homeurl})""
        "" | [raw](${url}) | ${license} | [issues](${issues})| ${description}""
    )
    size_history_graph = ""![Size history](https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts_file_size_history.png)""
    for key in keys:
        extensions = key.replace(""-only"", """").replace(""-"", "", "")
        extensions_str = ""* Extensions: **"" + extensions + ""**.""
        if data[key][""no_unified_hosts""]:
            extensions_header = ""Limited to the extensions: "" + extensions
        else:
            extensions_header = ""Unified hosts file with "" + extensions + "" extensions""

        source_rows = """"
        source_list = data[key][""sourcesdata""]

        for source in source_list:
            this_row = {}
            this_row.update(row_defaults)
            this_row.update(source)
            source_rows += t.substitute(this_row) + ""\n""

        with open(
            os.path.join(data[key][""location""], README_FILENAME),
            ""wt"",
            encoding=""utf-8"",
            newline=""\n"",
        ) as out:
            for line in open(README_TEMPLATE, encoding=""utf-8"", newline=""\n""):
                line = line.replace(
                    ""@GEN_DATE@"", time.strftime(""%B %d %Y"", time.gmtime())
                )
                line = line.replace(""@EXTENSIONS@"", extensions_str)
                line = line.replace(""@EXTENSIONS_HEADER@"", extensions_header)
                line = line.replace(
                    ""@NUM_ENTRIES@"", ""{:,}"".format(data[key][""entries""])
                )
                line = line.replace(
                    ""@SUBFOLDER@"", os.path.join(data[key][""location""], """")
                )
                line = line.replace(""@TOCROWS@"", toc_rows)
                line = line.replace(""@SOURCEROWS@"", source_rows)
                # insert the size graph on the home readme only, for now.
                if key == ""base"":
                    line = line.replace(
                        ""@SIZEHISTORY@"", size_history_graph
                    )
                else:
                    line = line.replace(
                        ""@SIZEHISTORY@"", ""![Size history](stats.png)"")

                out.write(line)


if __name__ == ""__main__"":
    main()
",11,0,1,0.0,23,1.0,95,0
utils.py,"# Numpy and pandas by default assume a narrow screen - this fixes that
from fastai.vision.all import *
from nbdev.showdoc import *
from ipywidgets import widgets
from pandas.api.types import CategoricalDtype

import matplotlib as mpl
import json

# mpl.rcParams['figure.dpi']= 200
mpl.rcParams['savefig.dpi']= 200
mpl.rcParams['font.size']=12

set_seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
pd.set_option('display.max_columns',999)
np.set_printoptions(linewidth=200)
torch.set_printoptions(linewidth=200)

import graphviz
def gv(s): return graphviz.Source('digraph G{ rankdir=""LR""' + s + '; }')

def get_image_files_sorted(path, recurse=True, folders=None): return get_image_files(path, recurse, folders).sorted()


# +
# pip install azure-cognitiveservices-search-imagesearch

from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api
from msrest.authentication import CognitiveServicesCredentials as auth

def search_images_bing(key, term, min_sz=128, max_images=150):    
     params = {'q':term, 'count':max_images, 'minHeight':min_sz, 'minWidth':min_sz, 'imageType':'photo'}
     headers = {""Ocp-Apim-Subscription-Key"":key}
     search_url = ""https://api.bing.microsoft.com/v7.0/images/search""
     response = requests.get(search_url, headers=headers, params=params)
     response.raise_for_status()
     search_results = response.json()
     return L(search_results['value'])


# -

def search_images_ddg(key,max_n=200):
     """"""Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images
        (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api)
     """"""
     url        = 'https://duckduckgo.com/'
     params     = {'q':key}
     res        = requests.post(url,data=params)
     searchObj  = re.search(r'vqd=([\d-]+)\&',res.text)
     if not searchObj: print('Token Parsing Failed !'); return
     requestUrl = url + 'i.js'
     headers    = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'}
     params     = (('l','us-en'),('o','json'),('q',key),('vqd',searchObj.group(1)),('f',',,,'),('p','1'),('v7exp','a'))
     urls       = []
     while True:
         try:
             res  = requests.get(requestUrl,headers=headers,params=params)
             data = json.loads(res.text)
             for obj in data['results']:
                 urls.append(obj['image'])
                 max_n = max_n - 1
                 if max_n < 1: return L(set(urls))     # dedupe
             if 'next' not in data: return L(set(urls))
             requestUrl = url + data['next']
         except:
             pass


def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):
    x = torch.linspace(min,max)
    fig,ax = plt.subplots(figsize=figsize)
    ax.plot(x,f(x))
    if tx is not None: ax.set_xlabel(tx)
    if ty is not None: ax.set_ylabel(ty)
    if title is not None: ax.set_title(title)

# +
from sklearn.tree import export_graphviz

def draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs):
    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,
                      special_characters=True, rotate=False, precision=precision, **kwargs)
    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))


# +
from scipy.cluster import hierarchy as hc

def cluster_columns(df, figsize=(10,6), font_size=12):
    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)
    corr_condensed = hc.distance.squareform(1-corr)
    z = hc.linkage(corr_condensed, method='average')
    fig = plt.figure(figsize=figsize)
    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)
    plt.show()
",15,0,8,0.0,14,1.0,69,0
version.py,"import os

ZULIP_VERSION = ""10.0-dev+git""

# Add information on number of commits and commit hash to version, if available
zulip_git_version_file = os.path.join(
    os.path.dirname(os.path.abspath(__file__)), ""zulip-git-version""
)
lines = [ZULIP_VERSION, """"]
if os.path.exists(zulip_git_version_file):
    with open(zulip_git_version_file) as f:
        lines = [*f, """", """"]
ZULIP_VERSION = lines.pop(0).strip()
ZULIP_MERGE_BASE = lines.pop(0).strip()

LATEST_MAJOR_VERSION = ""9.0""
LATEST_RELEASE_VERSION = ""9.4""
LATEST_RELEASE_ANNOUNCEMENT = ""https://blog.zulip.com/2023/12/15/zulip-8-0-released/""

# Versions of the desktop app below DESKTOP_MINIMUM_VERSION will be
# prevented from connecting to the Zulip server.  Versions above
# DESKTOP_MINIMUM_VERSION but below DESKTOP_WARNING_VERSION will have
# a banner at the top of the page asking the user to upgrade.
DESKTOP_MINIMUM_VERSION = ""5.4.3""
DESKTOP_WARNING_VERSION = ""5.9.3""

# Bump the API_FEATURE_LEVEL whenever an API change is made
# that clients might want to condition on.  If we forget at
# the time we make the change, then bump it later as soon
# as we notice; clients using API_FEATURE_LEVEL will just not
# use the new feature/API until the bump.
#
# Changes should be accompanied by documentation explaining what the
# new level means in api_docs/changelog.md, as well as ""**Changes**""
# entries in the endpoint's documentation in `zulip.yaml`.

API_FEATURE_LEVEL = 353  # Last bumped for Zoom server to server video chat option.

# Bump the minor PROVISION_VERSION to indicate that folks should provision
# only when going from an old version of the code to a newer version. Bump
# the major version to indicate that folks should provision in both
# directions.

# Typically,
# * adding a dependency only requires a minor version bump;
# * removing a dependency requires a major version bump;
# * upgrading a dependency requires a major version bump, unless the
#   upgraded dependency is backwards compatible with all of our
#   historical commits sharing the same major version, in which case a
#   minor version bump suffices.

PROVISION_VERSION = (312, 0)  # bumped 2024-02-14 to upgrade asgiref
",1,0,0,0.0,1,1.0,18,0
vocoder_preprocess.py,"import argparse
import os
from pathlib import Path

from synthesizer.hparams import hparams
from synthesizer.synthesize import run_synthesis
from utils.argutils import print_args



if __name__ == ""__main__"":
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
        pass

    parser = argparse.ArgumentParser(
        description=""Creates ground-truth aligned (GTA) spectrograms from the vocoder."",
        formatter_class=MyFormatter
    )
    parser.add_argument(""datasets_root"", type=Path, help=\
        ""Path to the directory containing your SV2TTS directory. If you specify both --in_dir and ""
        ""--out_dir, this argument won't be used."")
    parser.add_argument(""-s"", ""--syn_model_fpath"", type=Path,
                        default=""saved_models/default/synthesizer.pt"",
                        help=""Path to a saved synthesizer"")
    parser.add_argument(""-i"", ""--in_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the synthesizer directory that contains the mel spectrograms, the wavs and the ""
        ""embeds. Defaults to  <datasets_root>/SV2TTS/synthesizer/."")
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the output vocoder directory that will contain the ground truth aligned mel ""
        ""spectrograms. Defaults to <datasets_root>/SV2TTS/vocoder/."")
    parser.add_argument(""--hparams"", default="""", help=\
        ""Hyperparameter overrides as a comma-separated list of name=value pairs"")
    parser.add_argument(""--cpu"", action=""store_true"", help=\
        ""If True, processing is done on CPU, even when a GPU is available."")
    args = parser.parse_args()
    print_args(args, parser)
    modified_hp = hparams.parse(args.hparams)

    if not hasattr(args, ""in_dir""):
        args.in_dir = args.datasets_root / ""SV2TTS"" / ""synthesizer""
    if not hasattr(args, ""out_dir""):
        args.out_dir = args.datasets_root / ""SV2TTS"" / ""vocoder""

    if args.cpu:
        # Hide GPUs from Pytorch to force CPU processing
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

    run_synthesis(args.in_dir, args.out_dir, args.syn_model_fpath, modified_hp)
",4,0,0,0.0,7,1.0,39,0
vocoder_train.py,"import argparse
from pathlib import Path

from utils.argutils import print_args
from vocoder.train import train


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Trains the vocoder from the synthesizer audios and the GTA synthesized mels, ""
                    ""or ground truth mels."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(""run_id"", type=str, help= \
        ""Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state ""
        ""from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved ""
        ""states and restart from scratch."")
    parser.add_argument(""datasets_root"", type=Path, help= \
        ""Path to the directory containing your SV2TTS directory. Specifying --syn_dir or --voc_dir ""
        ""will take priority over this argument."")
    parser.add_argument(""--syn_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the synthesizer directory that contains the ground truth mel spectrograms, ""
        ""the wavs and the embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/."")
    parser.add_argument(""--voc_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the vocoder directory that contains the GTA synthesized mel spectrograms. ""
        ""Defaults to <datasets_root>/SV2TTS/vocoder/. Unused if --ground_truth is passed."")
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"", help=\
        ""Path to the directory that will contain the saved model weights, as well as backups ""
        ""of those weights and wavs generated during training."")
    parser.add_argument(""-g"", ""--ground_truth"", action=""store_true"", help= \
        ""Train on ground truth spectrograms (<datasets_root>/SV2TTS/synthesizer/mels)."")
    parser.add_argument(""-s"", ""--save_every"", type=int, default=1000, help= \
        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""
        ""model."")
    parser.add_argument(""-b"", ""--backup_every"", type=int, default=25000, help= \
        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""
        ""model."")
    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \
        ""Do not load any saved model and restart from scratch."")
    args = parser.parse_args()

    # Process the arguments
    if not hasattr(args, ""syn_dir""):
        args.syn_dir = args.datasets_root / ""SV2TTS"" / ""synthesizer""
    if not hasattr(args, ""voc_dir""):
        args.voc_dir = args.datasets_root / ""SV2TTS"" / ""vocoder""
    del args.datasets_root
    args.models_dir.mkdir(exist_ok=True)

    # Run the training
    print_args(args, parser)
    train(**vars(args))
",3,0,0,0.0,5,1.0,45,0
wechat_moment.py,"import time

from appium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class Wechat_Moment():
    def __init__(self):
        desired_caps = {}
        desired_caps['platformName'] = 'Android'
        desired_caps['platformVersion'] = '5.1'
        desired_caps['deviceName'] = '88CKBM622PAM'
        desired_caps['appPackage'] = 'com.tencent.mm'
        desired_caps['appActivity'] = '.ui.LauncherUI'

        # 定义在朋友圈的时候滑动位置
        self.start_x = 300
        self.start_y = 800
        self.end_x = 300
        self.end_y = 300

        # 启动微信
        self.driver = webdriver.Remote('http://localhost:4723/wd/hub', desired_caps)
        # 设置等待
        self.wait = WebDriverWait(self.driver, 300)
        print('微信启动...')


    def login(self):
        # 获取到登录按钮后点击
        login_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/e4g"")))
        login_btn.click()
        # 获取使用微信号登录按钮
        change_login_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/cou"")))
        change_login_btn.click()
        # 获取输入账号元素并输入
        account = self.wait.until(EC.presence_of_element_located((By.XPATH, '//*[@resource-id=""com.tencent.mm:id/cos""]/android.widget.EditText')))
        account.send_keys(""xxxxxxxx"")
        # 获取密码元素并输入
        password = self.wait.until(EC.presence_of_element_located((By.XPATH,  '//*[@resource-id=""com.tencent.mm:id/cot""]/android.widget.EditText')))
        password.send_keys(""xxxxxx"")
        # 登录
        login = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/cov"")))
        login.click()
        # 点击去掉通讯录提示框
        no_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/az9"")))
        no_btn.click()
        print('登录成功...')


    def find_xiaoshuaib(self):
        # 获取到搜索按钮后点击
        search_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/iq"")))
        # 等搜索建立索引再点击
        time.sleep(10)
        search_btn.click()
        # 获取搜索框并输入
        search_input = self.wait.until(EC.presence_of_element_located((By.ID, ""com.tencent.mm:id/kh"")))
        search_input.send_keys(""wistbean"")
        print('搜索小帅b...')
        # 点击头像进入
        xiaoshuaib_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/py"")))
        xiaoshuaib_btn.click()
        # 点击右上角...进入
        menu_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/jy"")))
        menu_btn.click()
        # 再点击头像
        icon_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/e0c"")))
        icon_btn.click()
        # 点击朋友圈
        moment_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/d86"")))
        moment_btn.click()
        print('进入朋友圈...')

    def get_data(self):
        while True:
            # 获取 ListView
            items = self.wait.until(EC.presence_of_all_elements_located((By.ID, 'com.tencent.mm:id/eew')))
            # 滑动
            self.driver.swipe(self.start_x, self.start_y, self.end_x, self.end_y, 2000)
            #遍历获取每个List数据
            for item in items:
                moment_text = item.find_element_by_id('com.tencent.mm:id/kt').text
                day_text = item.find_element_by_id('com.tencent.mm:id/eke').text
                month_text = item.find_element_by_id('com.tencent.mm:id/ekf').text
                print('抓取到小帅b朋友圈数据： %s' % moment_text)
                print('抓取到小帅b发布时间： %s月%s' % (month_text, day_text))

if __name__ == '__main__':
    wc_moment = Wechat_Moment()
    wc_moment.login()
    wc_moment.find_xiaoshuaib()
    wc_moment.get_data()











",7,0,1,0.0,4,1.0,65,0
wechat_public_account.py,"#-*- coding:UTF-8 -*-
import json
import time
import pdfkit

import requests

base_url = 'https://mp.weixin.qq.com/mp/profile_ext'


# 这些信息不能抄我的，要用你自己的才有效
headers = {
    'Connection': 'keep - alive',
    'Accept': '* / *',
    'User-Agent': '写你自己的',
    'Referer': '写你自己的',
    'Accept-Encoding': 'br, gzip, deflate'
}

cookies = {
    'devicetype': 'iOS12.2',
    'lang': 'zh_CN',
    'pass_ticket': '写你自己的',
    'version': '1700042b',
    'wap_sid2': '写你自己的',
    'wxuin': '3340537333'
}



def get_params(offset):
    params = {
        'action': 'getmsg',
        '__biz': '写你自己的',
        'f': 'json',
        'offset': '{}'.format(offset),
        'count': '10',
        'is_ok': '1',
        'scene': '126',
        'uin': '777',
        'key': '777',
        'pass_ticket': '写你自己的',
        'appmsg_token': '写你自己的',
        'x5': '0',
        'f': 'json',
    }

    return params


def get_list_data(offset):
    res = requests.get(base_url, headers=headers, params=get_params(offset), cookies=cookies)
    data = json.loads(res.text)
    can_msg_continue = data['can_msg_continue']
    next_offset = data['next_offset']

    general_msg_list = data['general_msg_list']
    list_data = json.loads(general_msg_list)['list']

    for data in list_data:
        try:
            if data['app_msg_ext_info']['copyright_stat'] == 11:
                msg_info = data['app_msg_ext_info']
                title = msg_info['title']
                content_url = msg_info['content_url']
                # 自己定义存储路径
                pdfkit.from_url(content_url, '/home/wistbean/wechat_article/'+title+'.pdf')
                print('获取到原创文章：%s ： %s' % (title, content_url))
        except:
            print('不是图文')

    if can_msg_continue == 1:
        time.sleep(1)
        get_list_data(next_offset)


if __name__ == '__main__':
    get_list_data(0)",6,0,3,0.0,7,1.0,59,0
whats_left.py,"#!/usr/bin/env -S python3 -I

# This script generates Lib/snippets/whats_left_data.py with these variables defined:
# expected_methods - a dictionary mapping builtin objects to their methods
# cpymods - a dictionary mapping module names to their contents
# libdir - the location of RustPython's Lib/ directory.

#
# TODO: include this:
# which finds all modules it has available and
# creates a Python dictionary mapping module names to their contents, which is
# in turn used to generate a second Python script that also finds which modules
# it has available and compares that against the first dictionary we generated.
# We then run this second generated script with RustPython.

import argparse
import re
import os
import re
import sys
import json
import warnings
import inspect
import subprocess
import platform
from pydoc import ModuleScanner

if not sys.flags.isolated:
    print(""running without -I option."")
    print(""python -I whats_left.py"")
    exit(1)

GENERATED_FILE = ""extra_tests/not_impl.py""

implementation = platform.python_implementation()
if implementation != ""CPython"":
    sys.exit(f""whats_left.py must be run under CPython, got {implementation} instead"")
if sys.version_info[:2] < (3, 13):
    sys.exit(f""whats_left.py must be run under CPython 3.13 or newer, got {implementation} {sys.version} instead"")

def parse_args():
    parser = argparse.ArgumentParser(description=""Process some integers."")
    parser.add_argument(
        ""--signature"",
        action=""store_true"",
        help=""print functions whose signatures don't match CPython's"",
    )
    parser.add_argument(
        ""--doc"",
        action=""store_true"",
        help=""print elements whose __doc__ don't match CPython's"",
    )
    parser.add_argument(
        ""--json"",
        action=""store_true"",
        help=""print output as JSON (instead of line by line)"",
    )

    args = parser.parse_args()
    return args


args = parse_args()


# modules suggested for deprecation by PEP 594 (www.python.org/dev/peps/pep-0594/)
# some of these might be implemented, but they are not a priority
PEP_594_MODULES = {
    ""aifc"",
    ""asynchat"",
    ""asyncore"",
    ""audioop"",
    ""binhex"",
    ""cgi"",
    ""cgitb"",
    ""chunk"",
    ""crypt"",
    ""formatter"",
    ""fpectl"",
    ""imghdr"",
    ""imp"",
    ""macpath"",
    ""msilib"",
    ""nntplib"",
    ""nis"",
    ""ossaudiodev"",
    ""parser"",
    ""pipes"",
    ""smtpd"",
    ""sndhdr"",
    ""spwd"",
    ""sunau"",
    ""telnetlib"",
    ""uu"",
    ""xdrlib"",
}

# CPython specific modules (mostly consisting of templates/tests)
CPYTHON_SPECIFIC_MODS = {
    'xxmodule', 'xxsubtype', 'xxlimited', '_xxtestfuzz'
    '_testbuffer', '_testcapi', '_testimportmultiple', '_testinternalcapi', '_testmultiphase',
}

IGNORED_MODULES = {""this"", ""antigravity""} | PEP_594_MODULES | CPYTHON_SPECIFIC_MODS

sys.path = [
    path
    for path in sys.path
    if (""site-packages"" not in path and ""dist-packages"" not in path)
]


def attr_is_not_inherited(type_, attr):
    """"""
    returns True if type_'s attr is not inherited from any of its base classes
    """"""
    bases = type_.__mro__[1:]
    return getattr(type_, attr) not in (getattr(base, attr, None) for base in bases)


def extra_info(obj):
    if callable(obj) and not inspect._signature_is_builtin(obj):
        doc = inspect.getdoc(obj)
        try:
            sig = str(inspect.signature(obj))
            # remove function memory addresses
            return {
                ""sig"": re.sub("" at 0x[0-9A-Fa-f]+"", "" at 0xdeadbeef"", sig),
                ""doc"": doc,
            }
        except Exception as e:
            exception = repr(e)
            # CPython uses ' RustPython uses ""
            if exception.replace('""', ""'"").startswith(""ValueError('no signature found""):
                return {
                    ""sig"": ""ValueError('no signature found')"",
                    ""doc"": doc,
                }

            return {
                ""sig"": exception,
                ""doc"": doc,
            }

    return {
        ""sig"": None,
        ""doc"": None,
    }


def name_sort_key(name):
    if name == ""builtins"":
        return """"
    if name[0] == ""_"":
        return name[1:] + ""1""
    return name + ""2""


def gen_methods():
    types = [
        bool,
        bytearray,
        bytes,
        complex,
        dict,
        enumerate,
        filter,
        float,
        frozenset,
        int,
        list,
        map,
        memoryview,
        range,
        set,
        slice,
        str,
        super,
        tuple,
        object,
        zip,
        classmethod,
        staticmethod,
        property,
        Exception,
        BaseException,
    ]
    objects = [t.__name__ for t in types]
    objects.append(""type(None)"")

    iters = [
        ""type(bytearray().__iter__())"",
        ""type(bytes().__iter__())"",
        ""type(dict().__iter__())"",
        ""type(dict().values().__iter__())"",
        ""type(dict().items().__iter__())"",
        ""type(dict().values())"",
        ""type(dict().items())"",
        ""type(set().__iter__())"",
        ""type(list().__iter__())"",
        ""type(range(0).__iter__())"",
        ""type(str().__iter__())"",
        ""type(tuple().__iter__())"",
        ""type(memoryview(bytearray(b'0')).__iter__())"",
    ]

    methods = {}
    for typ_code in objects + iters:
        typ = eval(typ_code)
        attrs = []
        for attr in dir(typ):
            if attr_is_not_inherited(typ, attr):
                attrs.append((attr, extra_info(getattr(typ, attr))))
        methods[typ.__name__] = (typ_code, extra_info(typ), attrs)

    output = ""expected_methods = {\n""
    for name in sorted(methods.keys(), key=name_sort_key):
        typ_code, extra, attrs = methods[name]
        output += f"" '{name}': ({typ_code}, {extra!r}, [\n""
        for attr, attr_extra in attrs:
            output += f""    ({attr!r}, {attr_extra!r}),\n""
        output += "" ]),\n""
        if typ_code != objects[-1]:
            output += ""\n""
    output += ""}\n\n""
    return output


def scan_modules():
    """"""taken from the source code of help('modules')

    https://github.com/python/cpython/blob/63298930fb531ba2bb4f23bc3b915dbf1e17e9e1/Lib/pydoc.py#L2178""""""
    modules = {}

    def callback(path, modname, desc, modules=modules):
        if modname and modname[-9:] == "".__init__"":
            modname = modname[:-9] + "" (package)""
        if modname.find(""."") < 0:
            modules[modname] = 1

    def onerror(modname):
        callback(None, modname, None)

    with warnings.catch_warnings():
        # ignore warnings from importing deprecated modules
        warnings.simplefilter(""ignore"")
        ModuleScanner().run(callback, onerror=onerror)
    return list(modules.keys())


def import_module(module_name):
    import io
    from contextlib import redirect_stdout

    # Importing modules causes ('Constant String', 2, None, 4) and
    # ""Hello world!"" to be printed to stdout.
    f = io.StringIO()
    with warnings.catch_warnings(), redirect_stdout(f):
        # ignore warnings caused by importing deprecated modules
        warnings.filterwarnings(""ignore"", category=DeprecationWarning)
        try:
            module = __import__(module_name)
        except Exception as e:
            return e
    return module


def is_child(module, item):
    import inspect

    item_mod = inspect.getmodule(item)
    return item_mod is module


def dir_of_mod_or_error(module_name, keep_other=True):
    module = import_module(module_name)
    item_names = sorted(set(dir(module)))
    result = {}
    for item_name in item_names:
        item = getattr(module, item_name)
        # don't repeat items imported from other modules
        if keep_other or is_child(module, item) or inspect.getmodule(item) is None:
            result[item_name] = extra_info(item)
    return result


def gen_modules():
    # check name because modules listed have side effects on import,
    # e.g. printing something or opening a webpage
    modules = {}
    for mod_name in sorted(scan_modules(), key=name_sort_key):
        if mod_name in IGNORED_MODULES:
            continue
        # when generating CPython list, ignore items defined by other modules
        dir_result = dir_of_mod_or_error(mod_name, keep_other=False)
        if isinstance(dir_result, Exception):
            print(
                f""!!! {mod_name} skipped because {type(dir_result).__name__}: {str(dir_result)}"",
                file=sys.stderr,
            )
            continue
        modules[mod_name] = dir_result
    return modules


output = """"""\
# WARNING: THIS IS AN AUTOMATICALLY GENERATED FILE
# EDIT extra_tests/not_impl_gen.sh, NOT THIS FILE.
# RESULTS OF THIS TEST DEPEND ON THE CPYTHON
# VERSION AND PYTHON ENVIRONMENT USED
# TO RUN not_impl_mods_gen.py

""""""

output += gen_methods()
output += f""""""
cpymods = {gen_modules()!r}
libdir = {os.path.abspath(""Lib/"").encode('utf8')!r}

""""""

# Copy the source code of functions we will reuse in the generated script
REUSED = [
    attr_is_not_inherited,
    extra_info,
    dir_of_mod_or_error,
    import_module,
    is_child,
]
for fn in REUSED:
    output += """".join(inspect.getsourcelines(fn)[0]) + ""\n\n""

# Prevent missing variable linter errors from compare()
expected_methods = {}
cpymods = {}
libdir = """"
# This function holds the source code that will be run under RustPython
def compare():
    import inspect
    import io
    import os
    import re
    import sys
    import warnings
    from contextlib import redirect_stdout
    import json
    import platform

    def method_incompatibility_reason(typ, method_name, real_method_value):
        has_method = hasattr(typ, method_name)
        if not has_method:
            return """"

        is_inherited = not attr_is_not_inherited(typ, method_name)
        if is_inherited:
            return ""(inherited)""

        value = extra_info(getattr(typ, method_name))
        if value != real_method_value:
            return f""{value} != {real_method_value}""

        return None

    not_implementeds = {}
    for name, (typ, real_value, methods) in expected_methods.items():
        missing_methods = {}
        for method, real_method_value in methods:
            reason = method_incompatibility_reason(typ, method, real_method_value)
            if reason is not None:
                missing_methods[method] = reason
        if missing_methods:
            not_implementeds[name] = missing_methods

    if platform.python_implementation() == ""CPython"":
        if not_implementeds:
            sys.exit(""ERROR: CPython should have all the methods"")

    mod_names = [
        name.decode()
        for name, ext in map(os.path.splitext, os.listdir(libdir))
        if ext == b"".py"" or os.path.isdir(os.path.join(libdir, name))
    ]
    mod_names += list(sys.builtin_module_names)
    # Remove easter egg modules
    mod_names = sorted(set(mod_names) - {""this"", ""antigravity""})

    rustpymods = {mod: dir_of_mod_or_error(mod) for mod in mod_names}

    result = {
        ""cpython_modules"": {},
        ""implemented"": {},
        ""not_implemented"": {},
        ""failed_to_import"": {},
        ""missing_items"": {},
        ""mismatched_items"": {},
        ""mismatched_doc_items"": {},
    }
    for modname, cpymod in cpymods.items():
        rustpymod = rustpymods.get(modname)
        if rustpymod is None:
            result[""not_implemented""][modname] = None
        elif isinstance(rustpymod, Exception):
            result[""failed_to_import""][modname] = rustpymod.__class__.__name__ + str(rustpymod)
        else:
            implemented_items = sorted(set(cpymod) & set(rustpymod))
            mod_missing_items = set(cpymod) - set(rustpymod)
            mod_missing_items = sorted(
                f""{modname}.{item}"" for item in mod_missing_items
            )
            mod_mismatched_items = [
                (f""{modname}.{item}"", rustpymod[item][""sig""], cpymod[item][""sig""])
                for item in implemented_items
                if rustpymod[item][""sig""] != cpymod[item][""sig""]
                and not isinstance(cpymod[item][""sig""], Exception)
            ]
            mod_mismatched_doc_items = [
                (f""{modname}.{item}"", rustpymod[item][""doc""], cpymod[item][""doc""])
                for item in implemented_items
                if rustpymod[item][""doc""] != cpymod[item][""doc""]
            ]
            if mod_missing_items or mod_mismatched_items:
                if mod_missing_items:
                    result[""missing_items""][modname] = mod_missing_items
                if mod_mismatched_items:
                    result[""mismatched_items""][modname] = mod_mismatched_items
                if mod_mismatched_doc_items:
                    result[""mismatched_doc_items""][modname] = mod_mismatched_doc_items
            else:
                result[""implemented""][modname] = None

    result[""cpython_modules""] = cpymods
    result[""not_implementeds""] = not_implementeds

    print(json.dumps(result))


def remove_one_indent(s):
    indent = ""    ""
    return s[len(indent) :] if s.startswith(indent) else s


compare_src = inspect.getsourcelines(compare)[0][1:]
output += """".join(remove_one_indent(line) for line in compare_src)

with open(GENERATED_FILE, ""w"", encoding='utf-8') as f:
    f.write(output + ""\n"")


subprocess.run([""cargo"", ""build"", ""--release"", ""--features=ssl""], check=True)
result = subprocess.run(
    [""cargo"", ""run"", ""--release"", ""--features=ssl"", ""-q"", ""--"", GENERATED_FILE],
    env={**os.environ.copy(), ""RUSTPYTHONPATH"": ""Lib""},
    text=True,
    capture_output=True,
)
# The last line should be json output, the rest of the lines can contain noise
# because importing certain modules can print stuff to stdout/stderr
result = json.loads(result.stdout.splitlines()[-1])

if args.json:
    print(json.dumps(result))
    sys.exit()


# missing entire modules
print(""# modules"")
for modname in result[""not_implemented""]:
    print(modname, ""(entire module)"")
for modname, exception in result[""failed_to_import""].items():
    print(f""{modname} (exists but not importable: {exception})"")

# missing from builtins
print(""\n# builtin items"")
for module, missing_methods in result[""not_implementeds""].items():
    for method, reason in missing_methods.items():
        print(f""{module}.{method}"" + (f"" {reason}"" if reason else """"))

# missing from modules
print(""\n# stdlib items"")
for modname, missing in result[""missing_items""].items():
    for item in missing:
        print(item)

if args.signature:
    print(""\n# mismatching signatures (warnings)"")
    for modname, mismatched in result[""mismatched_items""].items():
        for i, (item, rustpy_value, cpython_value) in enumerate(mismatched):
            if cpython_value and cpython_value.startswith(""ValueError(""):
                continue  # these items will never match
            if rustpy_value is None or rustpy_value.startswith(""ValueError(""):
                rustpy_value = f"" {rustpy_value}""
            print(f""{item}{rustpy_value}"")
            if cpython_value is None:
                cpython_value = f"" {cpython_value}""
            print(f""{' ' * len(item)}{cpython_value}"")
            if i < len(mismatched) - 1:
                print()

if args.doc:
    print(""\n# mismatching `__doc__`s (warnings)"")
    for modname, mismatched in result[""mismatched_doc_items""].items():
        for (item, rustpy_doc, cpython_doc) in mismatched:
            print(f""{item} {repr(rustpy_doc)} != {repr(cpython_doc)}"")


print()
print(""# summary"")
for error_type, modules in result.items():
    print(""# "", error_type, len(modules))
",85,0,4,0.0,125,1.0,394,0
