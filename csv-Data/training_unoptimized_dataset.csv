file_name,full_code,complexity,bugs,code_smells,duplicated_lines_density,cognitive_complexity,security_rating,ncloc,vulnerabilities,optimized_code
8_puzzle.py,"from queue import PriorityQueue

class PuzzleState:
    def __init__(self, board, goal, moves=0, previous=None):
        self.board = board
        self.goal = goal
        self.moves = moves
        self.previous = previous

    def __lt__(self, other):
        return self.priority() < other.priority()

    def priority(self):
        return self.moves + self.manhattan()

    def manhattan(self):
        distance = 0
        for i in range(3):
            for j in range(3):
                if self.board[i][j] != 0:
                    x, y = divmod(self.board[i][j] - 1, 3)
                    distance += abs(x - i) + abs(y - j)
        return distance

    def is_goal(self):
        return self.board == self.goal

    def neighbors(self):
        neighbors = []
        x, y = next((i, j) for i in range(3) for j in range(3) if self.board[i][j] == 0)
        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 <= nx < 3 and 0 <= ny < 3:
                new_board = [row[:] for row in self.board]
                new_board[x][y], new_board[nx][ny] = new_board[nx][ny], new_board[x][y]
                neighbors.append(PuzzleState(new_board, self.goal, self.moves + 1, self))

        return neighbors

def solve_puzzle(initial_board, goal_board):
    initial_state = PuzzleState(initial_board, goal_board)
    frontier = PriorityQueue()
    frontier.put(initial_state)
    explored = set()

    while not frontier.empty():
        current_state = frontier.get()

        if current_state.is_goal():
            return current_state

        explored.add(tuple(map(tuple, current_state.board)))

        for neighbor in current_state.neighbors():
            if tuple(map(tuple, neighbor.board)) not in explored:
                frontier.put(neighbor)

    return None

def print_solution(solution):
    steps = []
    while solution:
        steps.append(solution.board)
        solution = solution.previous
    steps.reverse()

    for step in steps:
        for row in step:
            print(' '.join(map(str, row)))
        print()

# Example usage
initial_board = [
    [1, 2, 3],
    [4, 0, 5],
    [7, 8, 6]
]

goal_board = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 0]
]

solution = solve_puzzle(initial_board, goal_board)
if solution:
    print(""Solution found:"")
    print_solution(solution)
else:
    print(""No solution found."")
",23.0,0,0,0.0,24,1.0,72.0,0,
Anonymous_TextApp.py,"import tkinter as tk
from PIL import Image, ImageTk
from twilio.rest import Client

window = tk.Tk()
window.title(""Anonymous_Text_App"")
window.geometry(""800x750"")

# Define global variables
body = """"
to = """"

def message():
    global body, to
    account_sid = 'Your_account_sid' # Your account sid
    auth_token = 'Your_auth_token' # Your auth token
    client = Client(account_sid, auth_token)
    msg = client.messages.create(
        from_='Twilio_number',    # Twilio number
        body=body,
        to=to
    )
    print(msg.sid)
    confirmation_label.config(text=""Message Sent!"")  



try:
    # Load the background image
    bg_img = Image.open(r""D:\Downloads\img2.png"")
    
    #Canvas widget
    canvas = tk.Canvas(window, width=800, height=750)
    canvas.pack(fill=""both"", expand=True)
    
    #  background image to the Canvas
    bg_photo = ImageTk.PhotoImage(bg_img)
    bg_image_id = canvas.create_image(0, 0, image=bg_photo, anchor=""nw"")
    bg_image_id = canvas.create_image(550, 250, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(1100, 250, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(1250, 250, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(250, 750, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(850, 750, image=bg_photo, anchor=""center"")
    bg_image_id = canvas.create_image(1300, 750, image=bg_photo, anchor=""center"")
    
  
    
    # Foreground Image
    img = Image.open(r""D:\Downloads\output-onlinepngtools.png"")
    photo = ImageTk.PhotoImage(img)
    img_label = tk.Label(window, image=photo, anchor=""w"")
    img_label.image = photo  
    img_label.place(x=10, y=20)  
    
    # Text for number input
    canvas.create_text(1050, 300, text=""Enter the number starting with +[country code]"", font=(""Poppins"", 18, ""bold""), fill=""black"", anchor=""n"")
    text_field_number = tk.Entry(canvas, width=17, font=(""Poppins"", 25, ""bold""), bg=""#404040"", fg=""white"", show=""*"")
    canvas.create_window(1100, 350, window=text_field_number, anchor=""n"")
    
    # Text for message input
    canvas.create_text(1050, 450, text=""Enter the Message"", font=(""Poppins"", 18, ""bold""), fill=""black"", anchor=""n"")
    text_field_text = tk.Entry(canvas, width=17, font=(""Poppins"", 25, ""bold""), bg=""#404040"", fg=""white"")
    canvas.create_window(1100, 500, window=text_field_text, anchor=""n"")
    
    #  label for confirmation message
    confirmation_label = tk.Label(window, text="""", font=(""Poppins"", 16), fg=""green"")
    canvas.create_window(1100, 600, window=confirmation_label, anchor=""n"")
    
except Exception as e:
    print(f""Error loading image: {e}"")

# Function to save input and send message
def save_and_send():
    global body, to
    to = str(text_field_number.get())
    body = str(text_field_text.get())
    message()

# Button to save input and send message
save_button = tk.Button(window, text=""Save and Send"", command=save_and_send)
canvas.create_window(1200, 550, window=save_button, anchor='n')

window.mainloop()",2.0,0,0,0.0,1,5.0,55.0,1,
AreaOfTriangle.py,"# Python Program to find the area of triangle when all three side-lengths are known!

a = 5
b = 6
c = 7

# Uncomment below to take inputs from the user
# a = float(input('Enter first side: '))
# b = float(input('Enter second side: '))
# c = float(input('Enter third side: '))

# calculate the semi-perimeter
s = (a + b + c) / 2

# calculate the area
area = (s * (s - a) * (s - b) * (s - c)) ** 0.5
print(""The area of the triangle is: "" + area)
",0.0,0,0,0.0,0,1.0,6.0,0,
Base Converter Number system.py,"def base_check(xnumber, xbase):
    for char in xnumber[len(xnumber) - 1]:
        if int(char) >= int(xbase):
            return False
    return True


def convert_from_10(xnumber, xbase, arr, ybase):
    if int(xbase) == 2 or int(xbase) == 4 or int(xbase) == 6 or int(xbase) == 8:

        if xnumber == 0:
            return arr
        else:
            quotient = int(xnumber) // int(xbase)
            remainder = int(xnumber) % int(xbase)
            arr.append(remainder)
            dividend = quotient
            convert_from_10(dividend, xbase, arr, base)
    elif int(xbase) == 16:
        if int(xnumber) == 0:
            return arr
        else:
            quotient = int(xnumber) // int(xbase)
            remainder = int(xnumber) % int(xbase)
            if remainder > 9:
                if remainder == 10:
                    remainder = ""A""
                if remainder == 11:
                    remainder = ""B""
                if remainder == 12:
                    remainder = ""C""
                if remainder == 13:
                    remainder = ""D""
                if remainder == 14:
                    remainder = ""E""
                if remainder == 15:
                    remainder = ""F""
            arr.append(remainder)
            dividend = quotient
            convert_from_10(dividend, xbase, arr, ybase)


def convert_to_10(xnumber, xbase, arr, ybase):
    if int(xbase) == 10:
        for char in xnumber:
            arr.append(char)
        flipped = arr[::-1]
        ans = 0
        j = 0

        for i in flipped:
            ans = ans + (int(i) * (int(ybase) ** j))
            j = j + 1
        return ans


arrayfrom = []
arrayto = []
is_base_possible = False
number = input(""Enter the number you would like to convert: "")

while not is_base_possible:
    base = input(""What is the base of this number? "")
    is_base_possible = base_check(number, base)
    if not is_base_possible:
        print(f""The number {number} is not a base {base} number"")
        base = input
    else:
        break
dBase = input(""What is the base you would like to convert to? "")
if int(base) == 10:
    convert_from_10(number, dBase, arrayfrom, base)
    answer = arrayfrom[::-1]  # reverses the array
    print(f""In base {dBase} this number is: "")
    print(*answer, sep="""")
elif int(dBase) == 10:
    answer = convert_to_10(number, dBase, arrayto, base)
    print(f""In base {dBase} this number is: {answer} "")
else:
    number = convert_to_10(number, 10, arrayto, base)
    convert_from_10(number, dBase, arrayfrom, base)
    answer = arrayfrom[::-1]
    print(f""In base {dBase} this number is: "")
    print(*answer, sep="""")
",24.0,1,1,0.0,51,1.0,75.0,0,
Binary Coefficients.py,"def pascal_triangle(lineNumber):
    list1 = list()
    list1.append([1])
    i = 1
    while i <= lineNumber:
        j = 1
        l = []
        l.append(1)
        while j < i:
            l.append(list1[i - 1][j] + list1[i - 1][j - 1])
            j = j + 1
        l.append(1)
        list1.append(l)
        i = i + 1
    return list1


def binomial_coef(n, k):
    pascalTriangle = pascal_triangle(n)
    return pascalTriangle[n][k - 1]
",4.0,0,2,0.0,3,1.0,18.0,0,
BruteForce.py,"from itertools import product


def findPassword(chars, function, show=50, format_=""%s""):

    password = None
    attempts = 0
    size = 1
    stop = False

    while not stop:

        # Obtém todas as combinações possíveis com os dígitos do parâmetro ""chars"".
        for pw in product(chars, repeat=size):

            password = """".join(pw)

            # Imprime a senha que será tentada.
            if attempts % show == 0:
                print(format_ % password)

            # Verifica se a senha é a correta.
            if function(password):
                stop = True
                break
            else:
                attempts += 1
        size += 1

    return password, attempts


def getChars():
    """"""
    Método para obter uma lista contendo todas as
    letras do alfabeto e números.
    """"""
    chars = []

    # Acrescenta à lista todas as letras maiúsculas
    for id_ in range(ord(""A""), ord(""Z"") + 1):
        chars.append(chr(id_))

    # Acrescenta à lista todas as letras minúsculas
    for id_ in range(ord(""a""), ord(""z"") + 1):
        chars.append(chr(id_))

    # Acrescenta à lista todos os números
    for number in range(10):
        chars.append(str(number))

    return chars


# Se este módulo não for importado, o programa será testado.
# Para realizar o teste, o usuário deverá inserir uma senha para ser encontrada.

if __name__ == ""__main__"":

    import datetime
    import time

    # Pede ao usuário uma senha
    pw = input(""\n Type a password: "")
    print(""\n"")

    def testFunction(password):
        global pw
        if password == pw:
            return True
        else:
            return False

    # Obtém os dígitos que uma senha pode ter
    chars = getChars()

    t = time.process_time()

    # Obtém a senha encontrada e o múmero de tentativas
    password, attempts = findPassword(
        chars, testFunction, show=1000, format_="" Trying %s""
    )

    t = datetime.timedelta(seconds=int(time.process_time() - t))
    input(f""\n\n Password found: {password}\n Attempts: {attempts}\n Time: {t}\n"")
",12.0,0,3,0.0,16,1.0,45.0,0,
Caesar Cipher Encoder  & Decoder.py,"# PROJECT1
# CAESAR CIPHER ENCODER/DECODER

# Author: InTruder
# Cloned from: https://github.com/InTruder-Sec/caesar-cipher

# Improved by: OfficialAhmed (https://github.com/OfficialAhmed)

def get_int() -> int:
    """"""
    Get integer, otherwise redo
    """"""

    try:
        key = int(input(""Enter number of characters you want to shift: ""))
    except:
        print(""Enter an integer"")
        key = get_int()

    return key

def main():

    print(""[>] CAESAR CIPHER DECODER!!! \n"")
    print(""[1] Encrypt\n[2] Decrypt"")

    match input(""Choose one of the above(example for encode enter 1): ""):

        case ""1"":
            encode()

        case ""2"":
            decode()

        case _:
            print(""\n[>] Invalid input. Choose 1 or 2"")
            main()


def encode():

    encoded_cipher = """"
    text = input(""Enter text to encode: "")
    key = get_int()
        
    for char in text:
        
        ascii = ord(char) + key
        encoded_cipher += chr(ascii)

    print(f""Encoded text: {encoded_cipher}"")


def decode():

    decoded_cipher = """"
    cipher = input(""\n[>] Enter your cipher text: "")
    key = get_int()

    for character in cipher:
        ascii = ord(character) - key
        decoded_cipher += chr(ascii)

    print(decoded_cipher)


if __name__ == '__main__':
    main()
",7.0,0,3,0.0,4,1.0,36.0,0,
__main__.py,"""""""Allow cookiecutter to be executable from a checkout or zip file.""""""

import runpy

if __name__ == ""__main__"":
    runpy.run_module(""cookiecutter"", run_name=""__main__"")
",1.0,0,0,0.0,1,1.0,3.0,0,
biqukan.py,"# -*- coding:UTF-8 -*-
from urllib import request
from bs4 import BeautifulSoup
import collections
import re
import os
import time
import sys
import types

""""""
类说明:下载《笔趣看》网小说: url:https://www.biqukan.com/

Parameters:
	target - 《笔趣看》网指定的小说目录地址(string)

Returns:
	无

Modify:
	2017-05-06
""""""
class download(object):
	def __init__(self, target):
		self.__target_url = target
		self.__head = {'User-Agent':'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19',}

	""""""
	函数说明:获取下载链接

	Parameters:
		无

	Returns:
		novel_name + '.txt' - 保存的小说名(string)
		numbers - 章节数(int)
		download_dict - 保存章节名称和下载链接的字典(dict)

	Modify:
		2017-05-06
	""""""
	def get_download_url(self):
		charter = re.compile(u'[第弟](.+)章', re.IGNORECASE)
		target_req = request.Request(url = self.__target_url, headers = self.__head)
		target_response = request.urlopen(target_req)
		target_html = target_response.read().decode('gbk','ignore')
		listmain_soup = BeautifulSoup(target_html,'lxml')
		chapters = listmain_soup.find_all('div',class_ = 'listmain')
		download_soup = BeautifulSoup(str(chapters), 'lxml')
		novel_name = str(download_soup.dl.dt).split(""》"")[0][5:]
		flag_name = ""《"" + novel_name + ""》"" + ""正文卷""
		numbers = (len(download_soup.dl.contents) - 1) / 2 - 8
		download_dict = collections.OrderedDict()
		begin_flag = False
		numbers = 1
		for child in download_soup.dl.children:
			if child != '\n':
				if child.string == u""%s"" % flag_name:
					begin_flag = True
				if begin_flag == True and child.a != None:
					download_url = ""https://www.biqukan.com"" + child.a.get('href')
					download_name = child.string
					names = str(download_name).split('章')
					name = charter.findall(names[0] + '章')
					if name:
							download_dict['第' + str(numbers) + '章 ' + names[1]] = download_url
							numbers += 1
		return novel_name + '.txt', numbers, download_dict
	
	""""""
	函数说明:爬取文章内容

	Parameters:
		url - 下载连接(string)

	Returns:
		soup_text - 章节内容(string)

	Modify:
		2017-05-06
	""""""
	def Downloader(self, url):
		download_req = request.Request(url = url, headers = self.__head)
		download_response = request.urlopen(download_req)
		download_html = download_response.read().decode('gbk','ignore')
		soup_texts = BeautifulSoup(download_html, 'lxml')
		texts = soup_texts.find_all(id = 'content', class_ = 'showtxt')
		soup_text = BeautifulSoup(str(texts), 'lxml').div.text.replace('\xa0','')
		return soup_text

	""""""
	函数说明:将爬取的文章内容写入文件

	Parameters:
		name - 章节名称(string)
		path - 当前路径下,小说保存名称(string)
		text - 章节内容(string)

	Returns:
		无

	Modify:
		2017-05-06
	""""""
	def Writer(self, name, path, text):
		write_flag = True
		with open(path, 'a', encoding='utf-8') as f:
			f.write(name + '\n\n')
			for each in text:
				if each == 'h':
					write_flag = False
				if write_flag == True and each != ' ':
					f.write(each)
				if write_flag == True and each == '\r':
					f.write('\n')			
			f.write('\n\n')

if __name__ == ""__main__"":
	print(""\n\t\t欢迎使用《笔趣看》小说下载小工具\n\n\t\t作者:Jack-Cui\t时间:2017-05-06\n"")
	print(""*************************************************************************"")
	
	#小说地址
	target_url = str(input(""请输入小说目录下载地址:\n""))

	#实例化下载类
	d = download(target = target_url)
	name, numbers, url_dict = d.get_download_url()
	if name in os.listdir():
		os.remove(name)
	index = 1

	#下载中
	print(""《%s》下载中:"" % name[:-4])
	for key, value in url_dict.items():
		d.Writer(key, name, d.Downloader(value))
		sys.stdout.write(""已下载:%.3f%%"" %  float(index/numbers) + '\r')
		sys.stdout.flush()
		index += 1	

	print(""《%s》下载完成！"" % name[:-4])

	
",19.0,0,3,0.0,28,1.0,127.0,0,
cleaner.py,"import pandas as pd


dataset_path = ""github_code_dataset_with_all_sonar_metrics.csv""
df_final = pd.read_csv(dataset_path)


columns_to_drop = [""repo_name"", ""file_path"", ""sonar_component_key""]


df_final = df_final.drop(columns=[col for col in columns_to_drop if col in df_final.columns], errors=""ignore"")
df_final = df_final[df_final[""short_code_snippet""].notna() & (df_final[""short_code_snippet""].str.strip() != """")]

updated_file_path = ""sonarcloud_metrics.csv""
df_final.to_csv(updated_file_path, index=False)

df = pd.read_csv(""sonarcloud_metrics.csv"")

# Create blank rows
blank_row = pd.DataFrame([[""""] * len(df.columns)], columns=df.columns)

# Insert a blank row after every row
df_spaced = pd.concat([pd.concat([df.iloc[[i]], blank_row]) for i in range(len(df))], ignore_index=True)

# Save the updated dataset
df_spaced.to_csv(""sonar_metrics.csv"", index=False)

",1.0,0,0,0.0,0,1.0,12.0,0,
config.py,"# In this file, you can set the configurations of the app.

from src.utils.constants import DEBUG, ERROR, LLM_MODEL, OPENAI

#config related to logging must have prefix LOG_
LOG_LEVEL = 'ERROR'
LOG_SELENIUM_LEVEL = ERROR
LOG_TO_FILE = False
LOG_TO_CONSOLE = False

MINIMUM_WAIT_TIME_IN_SECONDS = 60

JOB_APPLICATIONS_DIR = ""job_applications""
JOB_SUITABILITY_SCORE = 7

JOB_MAX_APPLICATIONS = 5
JOB_MIN_APPLICATIONS = 1

LLM_MODEL_TYPE = 'openai'
LLM_MODEL = 'gpt-4o-mini'
# Only required for OLLAMA models
LLM_API_URL = ''
",0.0,0,0,0.0,0,1.0,13.0,0,
dangdang_top_500.py,"import requests
import re
import json


def request_dandan(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
    except requests.RequestException as e:
        print(e)
        return None


def parse_result(html):
    pattern = re.compile(
        '<li.*?list_num.*?(\d+)\.</div>.*?<img src=""(.*?)"".*?class=""name"".*?title=""(.*?)"">.*?class=""star"">.*?class=""tuijian"">(.*?)</span>.*?class=""publisher_info"">.*?target=""_blank"">(.*?)</a>.*?class=""biaosheng"">.*?<span>(.*?)</span></div>.*?<p><span class=""price_n"">(.*?)</span>.*?</li>', re.S)
    items = re.findall(pattern, html)

    for item in items:
        yield {
            'range': item[0],
            'image': item[1],
            'title': item[2],
            'recommend': item[3],
            'author': item[4],
            'times': item[5],
            'price': item[6]
        }


def write_item_to_file(item):
    print('开始写入数据 ====> ' + str(item))
    with open('book.txt', 'a', encoding='UTF-8') as f:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')


def main(page):
    url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page)
    html = request_dandan(url)
    items = parse_result(html)  # 解析过滤我们想要的信息
    for item in items:
        write_item_to_file(item)


if __name__ == ""__main__"":
    for i in range(1, 26):
        main(i)
",9.0,0,0,0.0,7,1.0,38.0,0,
demo_closures.py,"

def foo(x):
    def bar(z):
        return z + x
    return bar

f = foo(9)
g = foo(10)

print(f(2))
print(g(2))

",2.0,0,0,0.0,0,1.0,8.0,0,
demo_toolbox.py,"import argparse
import os
from pathlib import Path

from toolbox import Toolbox
from utils.argutils import print_args
from utils.default_models import ensure_default_models


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=""Runs the toolbox."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(""-d"", ""--datasets_root"", type=Path, help= \
        ""Path to the directory containing your datasets. See toolbox/__init__.py for a list of ""
        ""supported datasets."", default=None)
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"",
                        help=""Directory containing all saved models"")
    parser.add_argument(""--cpu"", action=""store_true"", help=\
        ""If True, all inference will be done on CPU"")
    parser.add_argument(""--seed"", type=int, default=None, help=\
        ""Optional random number seed value to make toolbox deterministic."")
    args = parser.parse_args()
    arg_dict = vars(args)
    print_args(args, parser)

    # Hide GPUs from Pytorch to force CPU processing
    if arg_dict.pop(""cpu""):
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

    # Remind the user to download pretrained models if needed
    ensure_default_models(args.models_dir)

    # Launch the toolbox
    Toolbox(**arg_dict)
",2.0,0,0,0.0,3,1.0,27.0,0,
douyin.py,"# -*- coding:utf-8 -*-
from bs4 import BeautifulSoup
from contextlib import closing
import requests, json, time, re, os, sys, time

class DouYin(object):
	def __init__(self):
		""""""
		抖音App视频下载
		""""""
		#SSL认证
		pass

	def get_video_urls(self, user_id):
		""""""
		获得视频播放地址
		Parameters:
			nickname：查询的用户名
		Returns:
			video_names: 视频名字列表
			video_urls: 视频链接列表
			aweme_count: 视频数量
		""""""
		video_names = []
		video_urls = []
		unique_id = ''
		while unique_id != user_id:
			search_url = 'https://api.amemv.com/aweme/v1/discover/search/?cursor=0&keyword=%s&count=10&type=1&retry_type=no_retry&iid=17900846586&device_id=34692364855&ac=wifi&channel=xiaomi&aid=1128&app_name=aweme&version_code=162&version_name=1.6.2&device_platform=android&ssmix=a&device_type=MI+5&device_brand=Xiaomi&os_api=24&os_version=7.0&uuid=861945034132187&openudid=dc451556fc0eeadb&manifest_version_code=162&resolution=1080*1920&dpi=480&update_version_code=1622' % user_id
			req = requests.get(url = search_url, verify = False)
			html = json.loads(req.text)
			aweme_count = html['user_list'][0]['user_info']['aweme_count']
			uid = html['user_list'][0]['user_info']['uid']
			nickname = html['user_list'][0]['user_info']['nickname']
			unique_id = html['user_list'][0]['user_info']['unique_id']
		user_url = 'https://www.douyin.com/aweme/v1/aweme/post/?user_id=%s&max_cursor=0&count=%s' % (uid, aweme_count)
		req = requests.get(url = user_url, verify = False)
		html = json.loads(req.text)
		i = 1
		for each in html['aweme_list']:
			share_desc = each['share_info']['share_desc']
			if '抖音-原创音乐短视频社区' == share_desc:
				video_names.append(str(i) + '.mp4')
				i += 1
			else:
				video_names.append(share_desc + '.mp4')
			video_urls.append(each['share_info']['share_url'])

		return video_names, video_urls, nickname

	def get_download_url(self, video_url):
		""""""
		获得视频播放地址
		Parameters:
			video_url：视频播放地址
		Returns:
			download_url: 视频下载地址
		""""""
		req = requests.get(url = video_url, verify = False)
		bf = BeautifulSoup(req.text, 'lxml')
		script = bf.find_all('script')[-1]
		video_url_js = re.findall('var data = \[(.+)\];', str(script))[0]
		video_html = json.loads(video_url_js)
		download_url = video_html['video']['play_addr']['url_list'][0]
		return download_url

	def video_downloader(self, video_url, video_name):
		""""""
		视频下载
		Parameters:
			None
		Returns:
			None
		""""""
		size = 0
		with closing(requests.get(video_url, stream=True, verify = False)) as response:
			chunk_size = 1024
			content_size = int(response.headers['content-length']) 
			if response.status_code == 200:
				sys.stdout.write('  [文件大小]:%0.2f MB\n' % (content_size / chunk_size / 1024))

				with open(video_name, ""wb"") as file:  
					for data in response.iter_content(chunk_size = chunk_size):
						file.write(data)
						size += len(data)
						file.flush()

					sys.stdout.write('    [下载进度]:%.2f%%' % float(size / content_size * 100))
					sys.stdout.flush()
		time.sleep(1)


	def run(self):
		""""""
		运行函数
		Parameters:
			None
		Returns:
			None
		""""""
		self.hello()
		# user_id = input('请输入ID(例如13978338):')
		user_id = 'sm666888'
		video_names, video_urls, nickname = self.get_video_urls(user_id)
		if nickname not in os.listdir():
			os.mkdir(nickname)
		sys.stdout.write('视频下载中:\n')
		for num in range(len(video_urls)):
			print('  %s\n' % video_urls[num])
			video_url = self.get_download_url(video_urls[num])
			if '\\' in video_names[num]:
				video_name = video_names[num].replace('\\', '')
			elif '/' in video_names[num]:
				video_name = video_names[num].replace('/', '')
			else:
				video_name = video_names[num]
			self.video_downloader(video_url, os.path.join(nickname, video_name))
			print('')

	def hello(self):
		""""""
		打印欢迎界面
		Parameters:
			None
		Returns:
			None
		""""""
		print('*' * 100)
		print('\t\t\t\t抖音App视频下载小助手')
		print('*' * 100)

		
if __name__ == '__main__':
	douyin = DouYin()
	douyin.run()",15.0,0,1,20.1,15,4.0,79.0,4,
download_checks.py,"{'https://s3.amazonaws.com/fast-ai-imageclas/caltech_101.tgz': (131740031,
                                                                'd673425306e98ee4619fcdeef8a0e876'),
 'https://s3.amazonaws.com/fast-ai-imagelocal/biwi_head_pose.tgz': (452316199,
                                                                    '00f4ccf66e8cba184bc292fdc08fb237'),
 'https://s3.amazonaws.com/fast-ai-imagelocal/camvid.tgz': (598913237,
                                                            '648371e4f3a833682afb39b08a3ce2aa'),
 'https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz': (11784419,
                                                          'b86f328f4dbd072486591cb7a5644dcd'),
 'https://s3.amazonaws.com/fast-ai-nlp/amazon_review_full_csv.tgz': (71606272,
                                                                     '4a1196cf0adaea22f4bc3f592cddde90'),
 'https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz': (688339454,
                                                                         '676f7e5208ec343c8274b4bb085bc938'),
 'https://s3.amazonaws.com/fast-ai-sample/adult_sample.tgz': (968212,
                                                              '64eb9d7e23732de0b138f7372d15492f'),
 'https://s3.amazonaws.com/fast-ai-sample/biwi_sample.tgz': (593774,
                                                             '9179f4c1435f4b291f0d5b072d60c2c9'),
 'https://s3.amazonaws.com/fast-ai-sample/camvid_tiny.tgz': (2314212,
                                                             '2cf6daf91b7a2083ecfa3e9968e9d915')}",0.0,1,0,0.0,0,1.0,18.0,0,
encoder_train.py,"from utils.argutils import print_args
from encoder.train import train
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Trains the speaker encoder. You must have run encoder_preprocess.py first."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(""run_id"", type=str, help= \
        ""Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state ""
        ""from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved ""
        ""states and restart from scratch."")
    parser.add_argument(""clean_data_root"", type=Path, help= \
        ""Path to the output directory of encoder_preprocess.py. If you left the default ""
        ""output directory when preprocessing, it should be <datasets_root>/SV2TTS/encoder/."")
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"", help=\
        ""Path to the root directory that contains all models. A directory <run_name> will be created under this root.""
        ""It will contain the saved model weights, as well as backups of those weights and plots generated during ""
        ""training."")
    parser.add_argument(""-v"", ""--vis_every"", type=int, default=10, help= \
        ""Number of steps between updates of the loss and the plots."")
    parser.add_argument(""-u"", ""--umap_every"", type=int, default=100, help= \
        ""Number of steps between updates of the umap projection. Set to 0 to never update the ""
        ""projections."")
    parser.add_argument(""-s"", ""--save_every"", type=int, default=500, help= \
        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""
        ""model."")
    parser.add_argument(""-b"", ""--backup_every"", type=int, default=7500, help= \
        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""
        ""model."")
    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \
        ""Do not load any saved model."")
    parser.add_argument(""--visdom_server"", type=str, default=""http://localhost"")
    parser.add_argument(""--no_visdom"", action=""store_true"", help= \
        ""Disable visdom."")
    args = parser.parse_args()

    # Run the training
    print_args(args, parser)
    train(**vars(args))
",1.0,0,0,0.0,1,1.0,39.0,0,
fuck_bilibili_captcha.py,"import time
import requests
from PIL import Image
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
from io import BytesIO

driver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver')
WAIT = WebDriverWait(driver, 10)
url = 'https://passport.bilibili.com/login'


def mergy_Image(image_file, location_list):
    """"""
    将原始图片进行合成
    :param image_file: 图片文件
    :param location_list: 图片位置
    :return: 合成新的图片
    """"""

    # 存放上下部分的各个小块
    upper_half_list = []
    down_half_list = []

    image = Image.open(image_file)

    # 通过 y 的位置来判断是上半部分还是下半部分,然后切割
    for location in location_list:
        if location['y'] == -58:
            # 间距为10，y：58-116
            im = image.crop((abs(location['x']), 58, abs(location['x'])+10, 116))
            upper_half_list.append(im)
        if location['y'] == 0:
            # 间距为10，y：0-58
            im = image.crop((abs(location['x']), 0, abs(location['x']) + 10, 58))
            down_half_list.append(im)

    # 创建一张大小一样的图片
    new_image = Image.new('RGB', (260, 116))

    # 粘贴好上半部分 y坐标是从上到下（0-116）
    offset = 0
    for im in upper_half_list:
        new_image.paste(im, (offset, 0))
        offset += 10

    # 粘贴好下半部分
    offset = 0
    for im in down_half_list:
        new_image.paste(im, (offset, 58))
        offset += 10

    return new_image


def get_distance(bg_Image, fullbg_Image):

    # 阈值
    threshold = 200

    print(bg_Image.size[0])
    print(bg_Image.size[1])


    for i in range(60, bg_Image.size[0]):
        for j in range(bg_Image.size[1]):
            bg_pix = bg_Image.getpixel((i, j))
            fullbg_pix = fullbg_Image.getpixel((i, j))
            r = abs(bg_pix[0] - fullbg_pix[0])
            g = abs(bg_pix[1] - fullbg_pix[1])
            b = abs(bg_pix[2] - fullbg_pix[2])

            if r + g + b > threshold:
               return i




def get_path(distance):
        result = []
        current = 0
        mid = distance * 4 / 5
        t = 0.2
        v = 0
        while current < (distance - 10):
            if current < mid:
                a = 2
            else:
                a = -3
            v0 = v
            v = v0 + a * t
            s = v0 * t + 0.5 * a * t * t
            current += s
            result.append(round(s))
        return result


def start_drag(driver, distance):

    # 被妖怪吃掉了
    # knob =  WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, ""#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show"")))
    # ActionChains(driver).click_and_hold(knob).perform()
    # ActionChains(driver).move_by_offset(xoffset=distance, yoffset=0.1).perform()
    # time.sleep(0.5)
    # ActionChains(driver).release(knob).perform()

    # 被妖怪吃掉了
    # ActionChains(driver).drag_and_drop_by_offset(knob, distance-10, 0).perform()

    knob = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, ""#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show"")))
    result = get_path(distance)
    ActionChains(driver).click_and_hold(knob).perform()

    for x in result:
        ActionChains(driver).move_by_offset(xoffset=x, yoffset=0).perform()

    time.sleep(0.5)
    ActionChains(driver).release(knob).perform()


def recognize_code(driver):
    """"""
    识别滑动验证码
    :param driver: selenium驱动
    :return:
    """"""

    bs = BeautifulSoup(driver.page_source,'lxml')
    # 找到背景图片和缺口图片的div
    bg_div = bs.find_all(class_='gt_cut_bg_slice')
    fullbg_div = bs.find_all(class_='gt_cut_fullbg_slice')

    # 获取缺口背景图片url
    bg_url = re.findall('background-image:\surl\(""(.*?)""\)',bg_div[0].get('style'))
    # 获取背景图片url
    fullbg_url = re.findall('background-image:\surl\(""(.*?)""\)',fullbg_div[0].get('style'))

    # 存放每个合成缺口背景图片的位置
    bg_location_list = []
    # 存放每个合成背景图片的位置
    fullbg_location_list = []

    for bg in bg_div:
        location = {}
        location['x'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', bg.get('style'))[0][0])
        location['y'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', bg.get('style'))[0][1])
        bg_location_list.append(location)

    for fullbg in fullbg_div:
        location = {}
        location['x'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', fullbg.get('style'))[0][0])
        location['y'] = int(re.findall('background-position:\s(.*?)px\s(.*?)px;', fullbg.get('style'))[0][1])
        fullbg_location_list.append(location)

    print(bg_location_list)
    print(fullbg_location_list)

    # 将图片格式存为 jpg 格式
    bg_url = bg_url[0].replace('webp', 'jpg')
    fullbg_url = fullbg_url[0].replace('webp', 'jpg')
    # print(bg_url)
    # print(fullbg_url)

    # 下载图片
    bg_image = requests.get(bg_url).content
    fullbg_image = requests.get(fullbg_url).content
    print('完成图片下载')

    # 写入图片
    bg_image_file = BytesIO(bg_image)
    fullbg_image_file = BytesIO(fullbg_image)

    # 合成图片
    bg_Image = mergy_Image(bg_image_file, bg_location_list)
    fullbg_Image = mergy_Image(fullbg_image_file, fullbg_location_list)
    # bg_Image.show()
    # fullbg_Image.show()

    # 计算缺口偏移距离
    distance = get_distance(bg_Image, fullbg_Image)
    print('得到距离：%s' % str(distance))

    start_drag(driver, distance)




if __name__ == '__main__':

    # 获取滑块按钮
    driver.get(url)
    slider = WAIT.until(EC.element_to_be_clickable(
        (By.CSS_SELECTOR, ""#gc-box > div > div.gt_slider > div.gt_slider_knob.gt_show"")))

    recognize_code(driver)


    # driver.close()

",19.0,0,10,0.0,21,1.0,110.0,0,
generate_pxi.py,"import argparse
import os

from Cython import Tempita


def process_tempita(pxifile, outfile) -> None:
    with open(pxifile, encoding=""utf-8"") as f:
        tmpl = f.read()
    pyxcontent = Tempita.sub(tmpl)

    with open(outfile, ""w"", encoding=""utf-8"") as f:
        f.write(pyxcontent)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(""infile"", type=str, help=""Path to the input file"")
    parser.add_argument(""-o"", ""--outdir"", type=str, help=""Path to the output directory"")
    args = parser.parse_args()

    if not args.infile.endswith("".in""):
        raise ValueError(f""Unexpected extension: {args.infile}"")

    outdir_abs = os.path.join(os.getcwd(), args.outdir)
    outfile = os.path.join(
        outdir_abs, os.path.splitext(os.path.split(args.infile)[1])[0]
    )

    process_tempita(args.infile, outfile)


main()
",3.0,0,0,0.0,1,1.0,22.0,0,
generate_version.py,"#!/usr/bin/env python3

# Note: This file has to live next to setup.py or versioneer will not work
import argparse
import os
import sys

import versioneer

sys.path.insert(0, """")


def write_version_info(path) -> None:
    version = None
    git_version = None

    try:
        import _version_meson

        version = _version_meson.__version__
        git_version = _version_meson.__git_version__
    except ImportError:
        version = versioneer.get_version()
        git_version = versioneer.get_versions()[""full-revisionid""]
    if os.environ.get(""MESON_DIST_ROOT""):
        path = os.path.join(os.environ.get(""MESON_DIST_ROOT""), path)
    with open(path, ""w"", encoding=""utf-8"") as file:
        file.write(f'__version__=""{version}""\n')
        file.write(f'__git_version__=""{git_version}""\n')


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""-o"",
        ""--outfile"",
        type=str,
        help=""Path to write version info to"",
        required=False,
    )
    parser.add_argument(
        ""--print"",
        default=False,
        action=""store_true"",
        help=""Whether to print out the version"",
        required=False,
    )
    args = parser.parse_args()

    if args.outfile:
        if not args.outfile.endswith("".py""):
            raise ValueError(
                f""Output file must be a Python file. ""
                f""Got: {args.outfile} as filename instead""
            )

        write_version_info(args.outfile)

    if args.print:
        try:
            import _version_meson

            version = _version_meson.__version__
        except ImportError:
            version = versioneer.get_version()
        print(version)


main()
",6.0,0,0,0.0,8,1.0,52.0,0,
github-scraper.py,"import requests
import os
from github import Github
import re
GITHUB_TOKEN = ""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""
GITHUB_API_URL = ""https://api.github.com/search/repositories""
g = Github(GITHUB_TOKEN)

MAX_FILE_SIZE = 20000
MAX_REPOS = 200
MAX_FILES = 20

os.makedirs(""smaller-git-dataset"", exist_ok=True)

HEADERS = {""Authorization"": f""token {GITHUB_TOKEN}""}

params = {
    ""q"": ""language: python stars:>10"",
    ""sort"": ""stars"",
    ""order"": ""desc"",
    ""per_page"": 200,
    ""page"": 1
}



local_import_pattern = re.compile(r""import (\w+)|from (\w+) import"")
def is_independent(code, repo_files):
    matches = local_import_pattern.findall(code)
    for match in matches:
        imported_module = match[0] or match[1]
        if f""{imported_module}.py"" in repo_files:
            return False
    return True

repositories = []
while len(repositories) < MAX_REPOS and params[""page""] <= 3:  # Limit to 200 repos (2 pages)
    response = requests.get(GITHUB_API_URL, headers=HEADERS, params=params)
    if response.status_code != 200:
        print(f""⚠️ API Error: {response.status_code}"")
        break

    data = response.json()
    repositories.extend(data[""items""])  # Add repos to list

    params[""page""] += 1  # Go to the next page

# 🔹 Limit to 200 repositories
repositories = repositories[:MAX_REPOS]
print(f""✅ Fetched {len(repositories)} Python repositories."")


# 🔹 Function to fetch Python files from a repository
def fetch_python_files(repo):
    repo_name = repo[""full_name""]
    files_url = repo[""url""] + ""/contents""

    try:
        response = requests.get(files_url, headers=HEADERS)
        if response.status_code != 200:
            print(f""⚠️ Error fetching files for {repo_name}"")
            return

        files = response.json()
        repo_files = {file[""name""] for file in files if file[""type""] == ""file""}
        count = 0  # Limit files per repo

        for file in files:
            if count >= MAX_FILES:
                break  # Stop after 10 files

            if file[""type""] == ""file"" and file[""name""].endswith("".py"") and file[""size""] < MAX_FILE_SIZE:
                file_url = file[""download_url""]

                # 🔹 Download and save the file
                response = requests.get(file_url)
                code = response.text
                if is_independent(code, repo_files):
                    with open(f""smaller-git-dataset/{file['name']}"", ""w"", encoding=""utf-8"") as f:
                        f.write(code)

                    print(f""✅ Saved: {file['name']} ({file['size']} bytes) from {repo_name}"")
                    count += 1

    except Exception as e:
        print(f""⚠️ Error processing {repo_name}: {e}"")


# 🔹 Process each repository
for i, repo in enumerate(repositories):
    print(f""🔍 Processing {i + 1}/{len(repositories)}: {repo['full_name']}"")
    fetch_python_files(repo)

print(""✅ Done processing repositories."")",0.0,0,0,0.0,0,1.0,0.0,0,
hatch_build.py,"import os

from hatchling.builders.hooks.plugin.interface import BuildHookInterface


class CustomBuildHook(BuildHookInterface):
    def initialize(self, version, build_data):
        from babel.messages.frontend import compile_catalog

        for theme in 'mkdocs', 'readthedocs':
            cmd = compile_catalog()
            cmd.directory = os.path.join('mkdocs', 'themes', theme, 'locales')
            cmd.finalize_options()
            cmd.run()
",2.0,0,0,0.0,1,1.0,10.0,0,
hubconf.py,"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""isort:skip_file""""""

import functools
import importlib


dependencies = [
    ""dataclasses"",
    ""hydra"",
    ""numpy"",
    ""omegaconf"",
    ""regex"",
    ""requests"",
    ""torch"",
]


# Check for required dependencies and raise a RuntimeError if any are missing.
missing_deps = []
for dep in dependencies:
    try:
        importlib.import_module(dep)
    except ImportError:
        # Hack: the hydra package is provided under the ""hydra-core"" name in
        # pypi. We don't want the user mistakenly calling `pip install hydra`
        # since that will install an unrelated package.
        if dep == ""hydra"":
            dep = ""hydra-core""
        missing_deps.append(dep)
if len(missing_deps) > 0:
    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))


# only do fairseq imports after checking for dependencies
from fairseq.hub_utils import (  # noqa; noqa
    BPEHubInterface as bpe,
    TokenizerHubInterface as tokenizer,
)
from fairseq.models import MODEL_REGISTRY  # noqa


# torch.hub doesn't build Cython components, so if they are not found then try
# to build them here
try:
    import fairseq.data.token_block_utils_fast  # noqa
except ImportError:
    try:
        import cython  # noqa
        import os
        from setuptools import sandbox

        sandbox.run_setup(
            os.path.join(os.path.dirname(__file__), ""setup.py""),
            [""build_ext"", ""--inplace""],
        )
    except ImportError:
        print(
            ""Unable to build Cython components. Please make sure Cython is ""
            ""installed if the torch.hub model you are loading depends on it.""
        )


# automatically expose models defined in FairseqModel::hub_models
for _model_type, _cls in MODEL_REGISTRY.items():
    for model_name in _cls.hub_models().keys():
        globals()[model_name] = functools.partial(
            _cls.from_pretrained,
            model_name,
        )
",5.0,0,1,0.0,13,1.0,47.0,0,
idc.py,"import pandas as pd
dataset_path = ""github_code_dataset_with_efficiency_scores.csv""
df = pd.read_csv(dataset_path)


columns_to_remove = [""efficiency_score""]
df = df.drop(columns=[col for col in columns_to_remove if col in df.columns], errors=""ignore"")


final_file_path = ""github_code_dataset_no_comments.csv""
df.to_csv(final_file_path, index=False)",1.0,0,0,0.0,0,1.0,7.0,0,
idk.py,"import requests
import pandas as pd
import os

SONARCLOUD_TOKEN = ""82a8fe6df3b0391119aa62fd413df6db3707e9b1""
ORGANIZATION_KEY = ""gamify""
PROJECT_KEY = ""dimp170_refined-sonar-analysis""
HEADERS = {""Authorization"": f""Bearer {SONARCLOUD_TOKEN}""}

full_results = []
snippet_results = []

def get_sonar_files():
    sonar_files = {}
    page = 1

    while True:
        url = f""https://sonarcloud.io/api/components/tree""
        params = {
            ""component"": PROJECT_KEY,
            ""qualifiers"": ""FIL"",
            ""organization"": ORGANIZATION_KEY,
            ""p"": page,
            ""ps"": 500
        }

        response = requests.get(url, headers=HEADERS, params=params)

        if response.status_code == 200:
            components = response.json().get(""components"", [])
            file_paths = [c[""path""] for c in components]
            print(f""SonarCloud Retrieved Files (First 5): {file_paths[:5]}"")
            return {c[""path""]: c[""key""] for c in components}
        else:
            print(f""API Error: {response.status_code} - {response.text}"")
            break



sonar_files = get_sonar_files()
print(f""Retrieved {len(sonar_files)} indexed files from SonarCloud."")


dataset_path = ""github_code_dataset_no_comments.csv""
try:
    df = pd.read_csv(dataset_path)
except FileNotFoundError:
    print(f""Error: Dataset file '{dataset_path}' not found."")
    exit()


df[""file_path""] = df[""file_path""].apply(lambda x: x.split(""/"")[-1])
df[""file_path""] = df[""file_path""].apply(lambda x: f""{PROJECT_KEY}:{x}"")
df[""file_path""] = df[""file_path""].apply(lambda x: x.replace(f""{PROJECT_KEY}:"", """"))


df[""file_path""] = df[""file_path""].apply(lambda x: f""{PROJECT_KEY}:{x}"" if not x.startswith(PROJECT_KEY) else x)
print(f""Updated Dataset File Paths (First 5): {df['file_path'].head().tolist()}"")

df[""sonar_component_key""] = df[""file_path""].map(sonar_files)


df_filtered = df.dropna(subset=[""sonar_component_key""])
print(f""Matched {len(df_filtered)} files to SonarCloud."")



def get_all_sonar_metrics(component_key):
    if not component_key or pd.isna(component_key):
        return {}

    url = ""https://sonarcloud.io/api/measures/component""
    params = {
        ""component"": component_key,
        ""organization"": ORGANIZATION_KEY,
        ""metricKeys"": ""code_smells,complexity,security_rating,cognitive_complexity,duplicated_lines_density,bugs,vulnerabilities,ncloc""

    }

    response = requests.get(url, headers=HEADERS, params=params)

    if response.status_code == 200:
        measures = response.json().get(""component"", {}).get(""measures"", [])
        metrics = {m[""metric""]: m[""value""] for m in measures}


        print(f""SonarCloud Metrics for {component_key}: {metrics}"")

        return metrics
    else:
        print(f""API Error ({component_key}): {response.status_code} - {response.text}"")
        return {}
def get_code(file_path):
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        full_code = f.read()
        snippet = ""\n"".join(full_code.split(""\n"")[:5]) + ""...""
        return full_code, snippet




sonar_results_all_metrics = {}
for file_path, component_key in sonar_files.items():
    clean_file_name = os.path.basename(file_path)
    full_code, snippet = get_code(file_path)
    metrics = get_all_sonar_metrics(component_key)

    full_row = {
        ""file_name"": clean_file_name,
        ""full_code"": full_code
    }
    full_row.update(metrics)
    full_results.append(full_row)

    snippet_row = {
        ""file_name"": clean_file_name,
        ""snippet"": snippet
    }
    snippet_row.update(metrics)
    snippet_results.append(snippet_row)

df_full = pd.DataFrame(full_results)
df_snip = pd.DataFrame(snippet_results)

df_full.to_csv(""refined-sonar-metrics-for-ai.csv"", index=False)
df_snip.to_csv(""refined-sonar-metrics.csv"", index=False)







",13.0,0,1,0.0,22,1.0,80.0,0,
ikun_basketball.py,"# coding=utf-8

# 最新版的selenium(4.x.x)已经不支持PhantomJS。如要用PhantomJS，可用旧版本selenium。如pip install selenium==3.8.0。
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import xlwt

# browser = webdriver.PhantomJS()
browser = webdriver.Chrome()
WAIT = WebDriverWait(browser, 10)
browser.set_window_size(1400, 900)

book = xlwt.Workbook(encoding='utf-8', style_compression=0)

sheet = book.add_sheet('蔡徐坤篮球', cell_overwrite_ok=True)
sheet.write(0, 0, '名称')
sheet.write(0, 1, '地址')
sheet.write(0, 2, '描述')
sheet.write(0, 3, '观看次数')
sheet.write(0, 4, '弹幕数')
sheet.write(0, 5, '发布时间')

n = 1


def search():
    try:
        print('开始访问b站....')
        browser.get(""https://www.bilibili.com/"")

        # 被那个破登录遮住了
        # index = WAIT.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ""#primary_menu > ul > li.home > a"")))
        # index.click()

        input = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR, ""#nav_searchform > input"")))
        submit = WAIT.until(EC.element_to_be_clickable(
            (By.XPATH, '/html/body/div[2]/div/div[1]/div[1]/div/div[2]/div/form/div/button')))

        input.send_keys('蔡徐坤 篮球')
        submit.click()

        # 跳转到新的窗口
        print('跳转到新窗口')
        all_h = browser.window_handles
        browser.switch_to.window(all_h[1])
        get_source()

        total = WAIT.until(EC.presence_of_element_located((By.CSS_SELECTOR,
                                                           ""#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.last > button"")))
        return int(total.text)
    except TimeoutException:
        return search()


def next_page(page_num):
    try:
        print('获取下一页数据')
        next_btn = WAIT.until(EC.element_to_be_clickable((By.CSS_SELECTOR,
                                                          '#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.next > button')))
        next_btn.click()
        WAIT.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,
                                                     '#all-list > div.flow-loader > div.page-wrap > div > ul > li.page-item.active > button'),
                                                    str(page_num)))
        get_source()
    except TimeoutException:
        browser.refresh()
        return next_page(page_num)


def save_to_excel(soup):
    list = soup.find(class_='video-list clearfix').find_all(class_='video-item matrix')

    for item in list:
        item_title = item.find('a').get('title')
        item_link = item.find('a').get('href')
        item_dec = item.find(class_='des hide').text
        item_view = item.find(class_='so-icon watch-num').text
        item_biubiu = item.find(class_='so-icon hide').text
        item_date = item.find(class_='so-icon time').text

        print('爬取：' + item_title)

        global n

        sheet.write(n, 0, item_title)
        sheet.write(n, 1, item_link)
        sheet.write(n, 2, item_dec)
        sheet.write(n, 3, item_view)
        sheet.write(n, 4, item_biubiu)
        sheet.write(n, 5, item_date)

        n = n + 1


def get_source():
    WAIT.until(EC.presence_of_element_located(
        (By.CSS_SELECTOR, '#all-list > div.flow-loader > div.filter-wrap')))

    html = browser.page_source
    soup = BeautifulSoup(html, 'lxml')
    print('到这')

    save_to_excel(soup)


def main():
    try:
        total = search()
        print(total)

        for i in range(2, int(total + 1)):
            next_page(i)

    finally:
        browser.close()


if __name__ == '__main__':
    main()
    book.save('蔡徐坤篮球.xlsx')
",8.0,0,4,0.0,5,1.0,86.0,0,
input_data.py,"""""""Functions for downloading and reading MNIST data.""""""
from __future__ import print_function
import gzip
import os
import urllib
import numpy
SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
def maybe_download(filename, work_directory):
  """"""Download the data from Yann's website, unless it's already here.""""""
  if not os.path.exists(work_directory):
    os.mkdir(work_directory)
  filepath = os.path.join(work_directory, filename)
  if not os.path.exists(filepath):
    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)
    statinfo = os.stat(filepath)
    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')
  return filepath
def _read32(bytestream):
  dt = numpy.dtype(numpy.uint32).newbyteorder('>')
  return numpy.frombuffer(bytestream.read(4), dtype=dt)
def extract_images(filename):
  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""
  print('Extracting', filename)
  with gzip.open(filename) as bytestream:
    magic = _read32(bytestream)
    if magic != 2051:
      raise ValueError(
          'Invalid magic number %d in MNIST image file: %s' %
          (magic, filename))
    num_images = _read32(bytestream)
    rows = _read32(bytestream)
    cols = _read32(bytestream)
    buf = bytestream.read(rows * cols * num_images)
    data = numpy.frombuffer(buf, dtype=numpy.uint8)
    data = data.reshape(num_images, rows, cols, 1)
    return data
def dense_to_one_hot(labels_dense, num_classes=10):
  """"""Convert class labels from scalars to one-hot vectors.""""""
  num_labels = labels_dense.shape[0]
  index_offset = numpy.arange(num_labels) * num_classes
  labels_one_hot = numpy.zeros((num_labels, num_classes))
  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
  return labels_one_hot
def extract_labels(filename, one_hot=False):
  """"""Extract the labels into a 1D uint8 numpy array [index].""""""
  print('Extracting', filename)
  with gzip.open(filename) as bytestream:
    magic = _read32(bytestream)
    if magic != 2049:
      raise ValueError(
          'Invalid magic number %d in MNIST label file: %s' %
          (magic, filename))
    num_items = _read32(bytestream)
    buf = bytestream.read(num_items)
    labels = numpy.frombuffer(buf, dtype=numpy.uint8)
    if one_hot:
      return dense_to_one_hot(labels)
    return labels
class DataSet(object):
  def __init__(self, images, labels, fake_data=False):
    if fake_data:
      self._num_examples = 10000
    else:
      assert images.shape[0] == labels.shape[0], (
          ""images.shape: %s labels.shape: %s"" % (images.shape,
                                                 labels.shape))
      self._num_examples = images.shape[0]
      # Convert shape from [num examples, rows, columns, depth]
      # to [num examples, rows*columns] (assuming depth == 1)
      assert images.shape[3] == 1
      images = images.reshape(images.shape[0],
                              images.shape[1] * images.shape[2])
      # Convert from [0, 255] -> [0.0, 1.0].
      images = images.astype(numpy.float32)
      images = numpy.multiply(images, 1.0 / 255.0)
    self._images = images
    self._labels = labels
    self._epochs_completed = 0
    self._index_in_epoch = 0
  @property
  def images(self):
    return self._images
  @property
  def labels(self):
    return self._labels
  @property
  def num_examples(self):
    return self._num_examples
  @property
  def epochs_completed(self):
    return self._epochs_completed
  def next_batch(self, batch_size, fake_data=False):
    """"""Return the next `batch_size`examples from this data set.""""""
    if fake_data:
      fake_image = [1.0 for _ in xrange(784)]
      fake_label = 0
      return [fake_image for _ in xrange(batch_size)], [
          fake_label for _ in xrange(batch_size)]
    start = self._index_in_epoch
    self._index_in_epoch += batch_size
    if self._index_in_epoch > self._num_examples:
      # Finished epoch
      self._epochs_completed += 1
      # Shuffle the data
      perm = numpy.arange(self._num_examples)
      numpy.random.shuffle(perm)
      self._images = self._images[perm]
      self._labels = self._labels[perm]
      # Start next epoch
      start = 0
      self._index_in_epoch = batch_size
      assert batch_size <= self._num_examples
    end = self._index_in_epoch
    return self._images[start:end], self._labels[start:end]
def read_data_sets(train_dir, fake_data=False, one_hot=False):
  class DataSets(object):
    pass
  data_sets = DataSets()
  if fake_data:
    data_sets.train = DataSet([], [], fake_data=True)
    data_sets.validation = DataSet([], [], fake_data=True)
    data_sets.test = DataSet([], [], fake_data=True)
    return data_sets
  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'
  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'
  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'
  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'
  VALIDATION_SIZE = 5000
  local_file = maybe_download(TRAIN_IMAGES, train_dir)
  train_images = extract_images(local_file)
  local_file = maybe_download(TRAIN_LABELS, train_dir)
  train_labels = extract_labels(local_file, one_hot=one_hot)
  local_file = maybe_download(TEST_IMAGES, train_dir)
  test_images = extract_images(local_file)
  local_file = maybe_download(TEST_LABELS, train_dir)
  test_labels = extract_labels(local_file, one_hot=one_hot)
  validation_images = train_images[:VALIDATION_SIZE]
  validation_labels = train_labels[:VALIDATION_SIZE]
  train_images = train_images[VALIDATION_SIZE:]
  train_labels = train_labels[VALIDATION_SIZE:]
  data_sets.train = DataSet(train_images, train_labels)
  data_sets.validation = DataSet(validation_images, validation_labels)
  data_sets.test = DataSet(test_images, test_labels)
  return data_sets
",21.0,0,1,0.0,10,1.0,132.0,0,
linter_plugin.py,"""""""
Certbot PyLint plugin.

The built-in ImportChecker of Pylint does a similar job to ForbidStandardOsModule to detect
deprecated modules. You can check its behavior as a reference to what is coded here.
See https://github.com/PyCQA/pylint/blob/b20a2984c94e2946669d727dbda78735882bf50a/pylint/checkers/imports.py#L287
See https://docs.pytest.org/en/latest/writing_plugins.html
""""""
import os.path
import re

from pylint.checkers import BaseChecker

# Modules whose file is matching one of these paths can import the os module.
WHITELIST_PATHS = [
    '/acme/acme/',
    '/certbot-ci/',
    '/certbot-compatibility-test/',
]


class ForbidStandardOsModule(BaseChecker):
    """"""
    This checker ensures that standard os module (and submodules) is not imported by certbot
    modules. Otherwise an 'os-module-forbidden' error will be registered for the faulty lines.
    """"""

    name = 'forbid-os-module'
    msgs = {
        'E5001': (
            'Forbidden use of os module, certbot.compat.os must be used instead',
            'os-module-forbidden',
            'Some methods from the standard os module cannot be used for security reasons on '
            'Windows: the safe wrapper certbot.compat.os must be used instead in Certbot.'
        )
    }
    priority = -1

    def visit_import(self, node):
        os_used = any(name for name in node.names if name[0] == 'os' or name[0].startswith('os.'))
        if os_used and not _check_disabled(node):
            self.add_message('os-module-forbidden', node=node)

    def visit_importfrom(self, node):
        if node.modname == 'os' or node.modname.startswith('os.') and not _check_disabled(node):
            self.add_message('os-module-forbidden', node=node)


def register(linter):
    """"""Pylint hook to auto-register this linter""""""
    linter.register_checker(ForbidStandardOsModule(linter))


def _check_disabled(node):
    module = node.root()
    return any(path for path in WHITELIST_PATHS
               if os.path.normpath(path) in os.path.normpath(module.file))
",12.0,0,0,0.0,6,1.0,32.0,0,
makeHosts.py,"#!/usr/bin/env python

# Script by gfyoung
# https://github.com/gfyoung
#
# This Python script will generate hosts files and update the readme file.

from __future__ import print_function

import argparse
import subprocess
import sys


def print_failure(msg):
    """"""
    Print a failure message.

    Parameters
    ----------
    msg : str
        The failure message to print.
    """"""

    print(""\033[91m"" + msg + ""\033[0m"")


def update_hosts_file(*flags):
    """"""
    Wrapper around running updateHostsFile.py

    Parameters
    ----------
    flags : varargs
        Commandline flags to pass into updateHostsFile.py. For more info, run
        the following command in the terminal or command prompt:

        ```
        python updateHostsFile.py -h
        ```
    """"""

    if subprocess.call([sys.executable, ""updateHostsFile.py""] + list(flags)):
        print_failure(""Failed to update hosts file"")


def update_readme_file():
    """"""
    Wrapper around running updateReadme.py
    """"""

    if subprocess.call([sys.executable, ""updateReadme.py""]):
        print_failure(""Failed to update readme file"")


def recursively_loop_extensions(extension, extensions, current_extensions):
    """"""
    Helper function that recursively calls itself to prevent manually creating
    all possible combinations of extensions.

    Will call update_hosts_file for all combinations of extensions
    """"""

    c_extensions = extensions.copy()
    c_current_extensions = current_extensions.copy()
    c_current_extensions.append(extension)

    name = ""-"".join(c_current_extensions)

    params = (""-a"", ""-n"", ""-o"", ""alternates/""+name, ""-e"") + tuple(c_current_extensions)
    update_hosts_file(*params)

    params = (""-a"", ""-n"", ""-s"", ""--nounifiedhosts"", ""-o"", ""alternates/""+name+""-only"", ""-e"") + tuple(c_current_extensions)
    update_hosts_file(*params)

    while len(c_extensions) > 0:
        recursively_loop_extensions(c_extensions.pop(0), c_extensions, c_current_extensions)


def main():
    parser = argparse.ArgumentParser(
        description=""Creates custom hosts ""
        ""file from hosts stored in ""
        ""data subfolders.""
    )
    parser.parse_args()

    # Update the unified hosts file
    update_hosts_file(""-a"")

    # List of extensions we want to generate, we will loop over them recursively to prevent manual definitions
    # Only add new extensions to the end of the array, to avoid relocating existing hosts-files
    extensions = [""fakenews"", ""gambling"", ""porn"", ""social""]

    while len(extensions) > 0:
        recursively_loop_extensions(extensions.pop(0), extensions, [])

    # Update the readme files.
    update_readme_file()


if __name__ == ""__main__"":
    main()
",10.0,0,0,0.0,5,1.0,37.0,0,
mixed_tabs_and_spaces.py,"def square(x):
    sum_so_far = 0
    for _ in range(x):
        sum_so_far += x
	return sum_so_far  # noqa: E999 # pylint: disable=mixed-indentation Python 3 will raise a TabError here

print(square(10))
",2.0,0,0,0.0,1,1.0,6.0,0,
mymodule.py,"def generate_full_name(firstname, lastname):
      space = ' '
      fullname = firstname + space + lastname
      return fullname

def sum_two_nums (num1, num2):
    return num1 + num2
gravity = 9.81
person = {
    ""firstname"": ""Asabeneh"",
    ""age"": 250,
    ""country"": ""Finland"",
    ""city"":'Helsinki'
}


",2.0,0,0,0.0,0,1.0,13.0,0,
pdm_build.py,"import os
from typing import Any, Dict

from pdm.backend.hooks import Context

TIANGOLO_BUILD_PACKAGE = os.getenv(""TIANGOLO_BUILD_PACKAGE"", ""fastapi"")


def pdm_build_initialize(context: Context) -> None:
    metadata = context.config.metadata
    # Get custom config for the current package, from the env var
    config: Dict[str, Any] = context.config.data[""tool""][""tiangolo""][
        ""_internal-slim-build""
    ][""packages""].get(TIANGOLO_BUILD_PACKAGE)
    if not config:
        return
    project_config: Dict[str, Any] = config[""project""]
    # Override main [project] configs with custom configs for this package
    for key, value in project_config.items():
        metadata[key] = value
",3.0,0,0,0.0,2,1.0,14.0,0,
release.py,"#!/usr/bin/env python
from subprocess import call
import os
import re


version = None


def get_new_setup_py_lines():
    global version
    with open('setup.py', 'r') as sf:
        current_setup = sf.readlines()
    for line in current_setup:
        if line.startswith('VERSION = '):
            major, minor = re.findall(r""VERSION = '(\d+)\.(\d+)'"", line)[0]
            version = ""{}.{}"".format(major, int(minor) + 1)
            yield ""VERSION = '{}'\n"".format(version)
        else:
            yield line


lines = list(get_new_setup_py_lines())
with open('setup.py', 'w') as sf:
    sf.writelines(lines)

call('git pull', shell=True)
call('git commit -am ""Bump to {}""'.format(version), shell=True)
call('git tag {}'.format(version), shell=True)
call('git push', shell=True)
call('git push --tags', shell=True)

env = os.environ
env['CONVERT_README'] = 'true'
call('rm -rf dist/*', shell=True, env=env)
call('python setup.py sdist bdist_wheel', shell=True, env=env)
call('twine upload dist/*', shell=True, env=env)
",3.0,0,0,0.0,4,1.0,28.0,0,
release_utils.py,"import argparse
from typing import Tuple


def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:
    current_ver = find_version(""fairseq/version.txt"")
    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]
    major, minor, patch = version_list[0], version_list[1], version_list[2]
    if release_type == ""patch"":
        patch += 1
    elif release_type == ""minor"":
        minor += 1
        patch = 0
    elif release_type == ""major"":
        major += 1
        minor = patch = 0
    else:
        raise ValueError(
            ""Incorrect release type specified. Acceptable types are major, minor and patch.""
        )

    new_version_tuple = (major, minor, patch)
    new_version_str = ""."".join([str(x) for x in new_version_tuple])
    new_tag_str = ""v"" + new_version_str
    return new_version_tuple, new_version_str, new_tag_str


def find_version(version_file_path) -> str:
    with open(version_file_path) as f:
        version = f.read().strip()
        return version


def update_version(new_version_str) -> None:
    """"""
    given the current version, update the version to the
    next version depending on the type of release.
    """"""

    with open(""fairseq/version.txt"", ""w"") as writer:
        writer.write(new_version_str)


def main(args):
    if args.release_type in [""major"", ""minor"", ""patch""]:
        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)
    else:
        raise ValueError(""Incorrect release type specified"")

    if args.update_version:
        update_version(new_version)

    print(new_version, new_tag)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Versioning utils"")
    parser.add_argument(
        ""--release-type"",
        type=str,
        required=True,
        help=""type of release = major/minor/patch"",
    )
    parser.add_argument(
        ""--update-version"",
        action=""store_true"",
        required=False,
        help=""updates the version in fairseq/version.txt"",
    )

    args = parser.parse_args()
    main(args)
",8.0,0,1,0.0,8,1.0,53.0,0,
run-venv.py,"#!./venv/bin/python
import re
import sys

from glances import main

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())
",1.0,0,0,0.0,1,1.0,6.0,0,
run.py,"#!/usr/bin/python
import re
import sys

from glances import main

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())
",1.0,0,0,0.0,1,1.0,6.0,0,
run_example.py,"# import tutorials.keras.text_NER as ft
# import tutorials.keras.brat_tag as ft
import tutorials.RecommenderSystems.rs_rating_demo as ft
# from middleware.utils import TimeStat, Chart
# import matplotlib.pyplot as plt
# import matplotlib.gridspec as gridspec
# from matplotlib.font_manager import FontProperties
# plt.rcParams['font.sans-serif'] = ['SimHei']
# plt.rcParams['axes.unicode_minus'] = False


def main():
    ft.main()
    # x=y=[1,2,3]
    # plt.plot(x, y, color='g', linestyle='-')  # 绘制
    # plt.grid(True, ls = '--')
    # plt.show()

if __name__ == ""__main__"":
    main()
",2.0,0,3,0.0,1,1.0,5.0,0,
setup.py,"#!/usr/bin/env python

from pathlib import Path

from setuptools import find_packages, setup

here = Path(__file__).resolve().parent
README = (here / ""README.rst"").read_text(encoding=""utf-8"")
VERSION = (here / ""VERSION"").read_text(encoding=""utf-8"").strip()

excluded_packages = [""docs"", ""tests"", ""tests.*""]


# this module can be zip-safe if the zipimporter implements iter_modules or if
# pkgutil.iter_importer_modules has registered a dispatch for the zipimporter.
try:
    import pkgutil
    import zipimport

    zip_safe = (
        hasattr(zipimport.zipimporter, ""iter_modules"")
        or zipimport.zipimporter in pkgutil.iter_importer_modules.registry.keys()
    )
except AttributeError:
    zip_safe = False

setup(
    name=""Faker"",
    version=VERSION,
    description=""Faker is a Python package that generates fake data for you."",
    long_description=README,
    entry_points={
        ""console_scripts"": [""faker=faker.cli:execute_from_command_line""],
        ""pytest11"": [""faker = faker.contrib.pytest.plugin""],
    },
    classifiers=[
        # See https://pypi.org/pypi?%3Aaction=list_classifiers
        ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Intended Audience :: Developers"",
        ""Programming Language :: Python :: 3.9"",
        ""Programming Language :: Python :: 3.10"",
        ""Programming Language :: Python :: 3.11"",
        ""Programming Language :: Python :: 3.12"",
        ""Programming Language :: Python :: 3.13"",
        ""Programming Language :: Python :: Implementation :: CPython"",
        ""Programming Language :: Python :: Implementation :: PyPy"",
        ""Topic :: Software Development :: Libraries :: Python Modules"",
        ""Topic :: Software Development :: Testing"",
        ""Topic :: Utilities"",
        ""License :: OSI Approved :: MIT License"",
    ],
    keywords=""faker fixtures data test mock generator"",
    author=""joke2k"",
    author_email=""joke2k@gmail.com"",
    url=""https://github.com/joke2k/faker"",
    project_urls={
        ""Bug Tracker"": ""https://github.com/joke2k/faker/issues"",
        ""Changes"": ""https://github.com/joke2k/faker/blob/master/CHANGELOG.md"",
        ""Documentation"": ""http://faker.rtfd.org/"",
        ""Source Code"": ""https://github.com/joke2k/faker"",
    },
    license=""MIT License"",
    packages=find_packages(exclude=excluded_packages),
    package_data={
        ""faker"": [""py.typed"", ""proxy.pyi""],
    },
    platforms=[""any""],
    zip_safe=zip_safe,
    install_requires=[""tzdata""],
    python_requires="">=3.9"",
)
",1.0,0,0,0.0,2,1.0,60.0,0,
shuaia.py,"# -*- coding:UTF-8 -*-
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import requests
import os
import time

if __name__ == '__main__':
	list_url = []
	for num in range(1,3):
		if num == 1:
			url = 'http://www.shuaia.net/index.html'
		else:
			url = 'http://www.shuaia.net/index_%d.html' % num
		headers = {
				""User-Agent"":""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36""
		}
		req = requests.get(url = url,headers = headers)
		req.encoding = 'utf-8'
		html = req.text
		bf = BeautifulSoup(html, 'lxml')
		targets_url = bf.find_all(class_='item-img')
		
		for each in targets_url:
			list_url.append(each.img.get('alt') + '=' + each.get('href'))

	print('连接采集完成')

	for each_img in list_url:
		img_info = each_img.split('=')
		target_url = img_info[1]
		filename = img_info[0] + '.jpg'
		print('下载：' + filename)
		headers = {
			""User-Agent"":""Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36""
		}
		img_req = requests.get(url = target_url,headers = headers)
		img_req.encoding = 'utf-8'
		img_html = img_req.text
		img_bf_1 = BeautifulSoup(img_html, 'lxml')
		img_url = img_bf_1.find_all('div', class_='wr-single-content-list')
		img_bf_2 = BeautifulSoup(str(img_url), 'lxml')
		img_url = 'http://www.shuaia.net' + img_bf_2.div.img.get('src')
		if 'images' not in os.listdir():
			os.makedirs('images')
		urlretrieve(url = img_url,filename = 'images/' + filename)
		time.sleep(1)

	print('下载完成！')",6.0,0,0,0.0,15,1.0,43.0,0,
synthesizer_preprocess_audio.py,"from synthesizer.preprocess import preprocess_dataset
from synthesizer.hparams import hparams
from utils.argutils import print_args
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Preprocesses audio files from datasets, encodes them as mel spectrograms ""
                    ""and writes them to  the disk. Audio files are also saved, to be used by the ""
                    ""vocoder for training."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(""datasets_root"", type=Path, help=\
        ""Path to the directory containing your LibriSpeech/TTS datasets."")
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\
        ""Path to the output directory that will contain the mel spectrograms, the audios and the ""
        ""embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/"")
    parser.add_argument(""-n"", ""--n_processes"", type=int, default=4, help=\
        ""Number of processes in parallel."")
    parser.add_argument(""-s"", ""--skip_existing"", action=""store_true"", help=\
        ""Whether to overwrite existing files with the same name. Useful if the preprocessing was ""
        ""interrupted."")
    parser.add_argument(""--hparams"", type=str, default="""", help=\
        ""Hyperparameter overrides as a comma-separated list of name-value pairs"")
    parser.add_argument(""--no_alignments"", action=""store_true"", help=\
        ""Use this option when dataset does not include alignments\
        (these are used to split long audio files into sub-utterances.)"")
    parser.add_argument(""--datasets_name"", type=str, default=""LibriSpeech"", help=\
        ""Name of the dataset directory to process."")
    parser.add_argument(""--subfolders"", type=str, default=""train-clean-100,train-clean-360"", help=\
        ""Comma-separated list of subfolders to process inside your dataset directory"")
    args = parser.parse_args()

    # Process the arguments
    if not hasattr(args, ""out_dir""):
        args.out_dir = args.datasets_root.joinpath(""SV2TTS"", ""synthesizer"")

    # Create directories
    assert args.datasets_root.exists()
    args.out_dir.mkdir(exist_ok=True, parents=True)

    # Preprocess the dataset
    print_args(args, parser)
    args.hparams = hparams.parse(args.hparams)
    preprocess_dataset(**vars(args))
",2.0,0,0,0.0,3,1.0,39.0,0,
synthesizer_preprocess_embeds.py,"from synthesizer.preprocess import create_embeddings
from utils.argutils import print_args
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Creates embeddings for the synthesizer from the LibriSpeech utterances."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(""synthesizer_root"", type=Path, help=\
        ""Path to the synthesizer training data that contains the audios and the train.txt file. ""
        ""If you let everything as default, it should be <datasets_root>/SV2TTS/synthesizer/."")
    parser.add_argument(""-e"", ""--encoder_model_fpath"", type=Path,
                        default=""saved_models/default/encoder.pt"", help=\
        ""Path your trained encoder model."")
    parser.add_argument(""-n"", ""--n_processes"", type=int, default=4, help= \
        ""Number of parallel processes. An encoder is created for each, so you may need to lower ""
        ""this value on GPUs with low memory. Set it to 1 if CUDA is unhappy."")
    args = parser.parse_args()

    # Preprocess the dataset
    print_args(args, parser)
    create_embeddings(**vars(args))
",1.0,0,0,0.0,1,1.0,21.0,0,
synthesizer_train.py,"from pathlib import Path

from synthesizer.hparams import hparams
from synthesizer.train import train
from utils.argutils import print_args
import argparse


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""run_id"", type=str, help= \
        ""Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state ""
        ""from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved ""
        ""states and restart from scratch."")
    parser.add_argument(""syn_dir"", type=Path, help= \
        ""Path to the synthesizer directory that contains the ground truth mel spectrograms, ""
        ""the wavs and the embeds."")
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"", help=\
        ""Path to the output directory that will contain the saved model weights and the logs."")
    parser.add_argument(""-s"", ""--save_every"", type=int, default=1000, help= \
        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""
        ""model."")
    parser.add_argument(""-b"", ""--backup_every"", type=int, default=25000, help= \
        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""
        ""model."")
    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \
        ""Do not load any saved model and restart from scratch."")
    parser.add_argument(""--hparams"", default="""", help=\
        ""Hyperparameter overrides as a comma-separated list of name=value pairs"")
    args = parser.parse_args()
    print_args(args, parser)

    args.hparams = hparams.parse(args.hparams)

    # Run the training
    train(**vars(args))
",1.0,0,0,0.0,1,1.0,30.0,0,
update.py,"import os
import sys
import json
import re
import shutil


def update():
    from Config import config
    config.parse(silent=True)

    if getattr(sys, 'source_update_dir', False):
        if not os.path.isdir(sys.source_update_dir):
            os.makedirs(sys.source_update_dir)
        source_path = sys.source_update_dir.rstrip(""/"")
    else:
        source_path = os.getcwd().rstrip(""/"")

    if config.dist_type.startswith(""bundle_linux""):
        runtime_path = os.path.normpath(os.path.dirname(sys.executable) + ""/../.."")
    else:
        runtime_path = os.path.dirname(sys.executable)

    updatesite_path = config.data_dir + ""/"" + config.updatesite

    sites_json = json.load(open(config.data_dir + ""/sites.json""))
    updatesite_bad_files = sites_json.get(config.updatesite, {}).get(""cache"", {}).get(""bad_files"", {})
    print(
        ""Update site path: %s, bad_files: %s, source path: %s, runtime path: %s, dist type: %s"" %
        (updatesite_path, len(updatesite_bad_files), source_path, runtime_path, config.dist_type)
    )

    updatesite_content_json = json.load(open(updatesite_path + ""/content.json""))
    inner_paths = list(updatesite_content_json.get(""files"", {}).keys())
    inner_paths += list(updatesite_content_json.get(""files_optional"", {}).keys())

    # Keep file only in ZeroNet directory
    inner_paths = [inner_path for inner_path in inner_paths if re.match(""^(core|bundle)"", inner_path)]

    # Checking plugins
    plugins_enabled = []
    plugins_disabled = []
    if os.path.isdir(""%s/plugins"" % source_path):
        for dir in os.listdir(""%s/plugins"" % source_path):
            if dir.startswith(""disabled-""):
                plugins_disabled.append(dir.replace(""disabled-"", """"))
            else:
                plugins_enabled.append(dir)
        print(""Plugins enabled:"", plugins_enabled, ""disabled:"", plugins_disabled)

    update_paths = {}

    for inner_path in inner_paths:
        if "".."" in inner_path:
            continue
        inner_path = inner_path.replace(""\\"", ""/"").strip(""/"")  # Make sure we have unix path
        print(""."", end="" "")
        if inner_path.startswith(""core""):
            dest_path = source_path + ""/"" + re.sub(""^core/"", """", inner_path)
        elif inner_path.startswith(config.dist_type):
            dest_path = runtime_path + ""/"" + re.sub(""^bundle[^/]+/"", """", inner_path)
        else:
            continue

        if not dest_path:
            continue

        # Keep plugin disabled/enabled status
        match = re.match(re.escape(source_path) + ""/plugins/([^/]+)"", dest_path)
        if match:
            plugin_name = match.group(1).replace(""disabled-"", """")
            if plugin_name in plugins_enabled:  # Plugin was enabled
                dest_path = dest_path.replace(""plugins/disabled-"" + plugin_name, ""plugins/"" + plugin_name)
            elif plugin_name in plugins_disabled:  # Plugin was disabled
                dest_path = dest_path.replace(""plugins/"" + plugin_name, ""plugins/disabled-"" + plugin_name)
            print(""P"", end="" "")

        dest_dir = os.path.dirname(dest_path)
        if dest_dir and not os.path.isdir(dest_dir):
            os.makedirs(dest_dir)

        if dest_dir != dest_path.strip(""/""):
            update_paths[updatesite_path + ""/"" + inner_path] = dest_path

    num_ok = 0
    num_rename = 0
    num_error = 0
    for path_from, path_to in update_paths.items():
        print(""-"", path_from, ""->"", path_to)
        if not os.path.isfile(path_from):
            print(""Missing file"")
            continue

        data = open(path_from, ""rb"").read()

        try:
            open(path_to, 'wb').write(data)
            num_ok += 1
        except Exception as err:
            try:
                print(""Error writing: %s. Renaming old file as workaround..."" % err)
                path_to_tmp = path_to + ""-old""
                if os.path.isfile(path_to_tmp):
                    os.unlink(path_to_tmp)
                os.rename(path_to, path_to_tmp)
                num_rename += 1
                open(path_to, 'wb').write(data)
                shutil.copymode(path_to_tmp, path_to)  # Copy permissions
                print(""Write done after rename!"")
                num_ok += 1
            except Exception as err:
                print(""Write error after rename: %s"" % err)
                num_error += 1
    print(""* Updated files: %s, renamed: %s, error: %s"" % (num_ok, num_rename, num_error))


if __name__ == ""__main__"":
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), ""src""))  # Imports relative to src

    update()
",21.0,0,1,0.0,45,1.0,96.0,0,
updateReadme.py,"#!/usr/bin/env python3

# Script by Steven Black
# https://github.com/StevenBlack
#
# This Python script will update the readme files in this repo.

import json
import os
import time
from string import Template

# Project Settings
BASEDIR_PATH = os.path.dirname(os.path.realpath(__file__))
README_TEMPLATE = os.path.join(BASEDIR_PATH, ""readme_template.md"")
README_FILENAME = ""readme.md""
README_DATA_FILENAME = ""readmeData.json""


def main():
    s = Template(
        ""${description} | [Readme](https://github.com/StevenBlack/""
        ""hosts/blob/master/${location}readme.md) | ""
        ""[link](https://raw.githubusercontent.com/StevenBlack/""
        ""hosts/master/${location}hosts) | ""
        ""${fmtentries} | ""
        ""[link](http://sbc.io/hosts/${location}hosts)""
    )
    with open(README_DATA_FILENAME, ""r"", encoding=""utf-8"", newline=""\n"") as f:
        data = json.load(f)

    keys = list(data.keys())
    # Sort by the number of en-dashes in the key
    # and then by the key string itself.
    keys.sort(key=lambda item: (item.replace(""-only"", """").count(""-""), item.replace(""-only"", """")))

    toc_rows = """"
    for key in keys:
        data[key][""fmtentries""] = ""{:,}"".format(data[key][""entries""])
        if key == ""base"":
            data[key][""description""] = ""Unified hosts = **(adware + malware)**""
        else:
            if data[key][""no_unified_hosts""]:
                data[key][""description""] = (
                    ""**"" + key.replace(""-only"", """").replace(""-"", "" + "") + ""**""
                )
            else:
                data[key][""description""] = (
                    ""Unified hosts **+ "" + key.replace(""-"", "" + "") + ""**""
                )

        if ""\\"" in data[key][""location""]:
            data[key][""location""] = data[key][""location""].replace(""\\"", ""/"")

        toc_rows += s.substitute(data[key]) + ""\n""

    row_defaults = {
        ""name"": """",
        ""homeurl"": """",
        ""url"": """",
        ""license"": """",
        ""issues"": """",
        ""description"": """",
    }

    t = Template(
        ""${name} |[link](${homeurl})""
        "" | [raw](${url}) | ${license} | [issues](${issues})| ${description}""
    )
    size_history_graph = ""![Size history](https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts_file_size_history.png)""
    for key in keys:
        extensions = key.replace(""-only"", """").replace(""-"", "", "")
        extensions_str = ""* Extensions: **"" + extensions + ""**.""
        if data[key][""no_unified_hosts""]:
            extensions_header = ""Limited to the extensions: "" + extensions
        else:
            extensions_header = ""Unified hosts file with "" + extensions + "" extensions""

        source_rows = """"
        source_list = data[key][""sourcesdata""]

        for source in source_list:
            this_row = {}
            this_row.update(row_defaults)
            this_row.update(source)
            source_rows += t.substitute(this_row) + ""\n""

        with open(
            os.path.join(data[key][""location""], README_FILENAME),
            ""wt"",
            encoding=""utf-8"",
            newline=""\n"",
        ) as out:
            for line in open(README_TEMPLATE, encoding=""utf-8"", newline=""\n""):
                line = line.replace(
                    ""@GEN_DATE@"", time.strftime(""%B %d %Y"", time.gmtime())
                )
                line = line.replace(""@EXTENSIONS@"", extensions_str)
                line = line.replace(""@EXTENSIONS_HEADER@"", extensions_header)
                line = line.replace(
                    ""@NUM_ENTRIES@"", ""{:,}"".format(data[key][""entries""])
                )
                line = line.replace(
                    ""@SUBFOLDER@"", os.path.join(data[key][""location""], """")
                )
                line = line.replace(""@TOCROWS@"", toc_rows)
                line = line.replace(""@SOURCEROWS@"", source_rows)
                # insert the size graph on the home readme only, for now.
                if key == ""base"":
                    line = line.replace(
                        ""@SIZEHISTORY@"", size_history_graph
                    )
                else:
                    line = line.replace(
                        ""@SIZEHISTORY@"", ""![Size history](stats.png)"")

                out.write(line)


if __name__ == ""__main__"":
    main()
",11.0,0,1,0.0,23,1.0,95.0,0,
update_version.py,"import os
import subprocess


def get_version():
    command = [""git"", ""describe"", ""--tags""]
    try:
        version = subprocess.check_output(command).decode().strip()
        version_parts = version.split(""-"")
        if len(version_parts) > 1 and version_parts[0].startswith(""magic_pdf""):
            return version_parts[1]
        else:
            raise ValueError(f""Invalid version tag {version}. Expected format is magic_pdf-<version>-released."")
    except Exception as e:
        print(e)
        return ""0.0.0""


def write_version_to_commons(version):
    commons_path = os.path.join(os.path.dirname(__file__), 'magic_pdf', 'libs', 'version.py')
    with open(commons_path, 'w') as f:
        f.write(f'__version__ = ""{version}""\n')


if __name__ == '__main__':
    version_name = get_version()
    write_version_to_commons(version_name)
",5.0,0,0,0.0,5,1.0,21.0,0,
utils.py,"# Numpy and pandas by default assume a narrow screen - this fixes that
from fastai.vision.all import *
from nbdev.showdoc import *
from ipywidgets import widgets
from pandas.api.types import CategoricalDtype

import matplotlib as mpl
import json

# mpl.rcParams['figure.dpi']= 200
mpl.rcParams['savefig.dpi']= 200
mpl.rcParams['font.size']=12

set_seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
pd.set_option('display.max_columns',999)
np.set_printoptions(linewidth=200)
torch.set_printoptions(linewidth=200)

import graphviz
def gv(s): return graphviz.Source('digraph G{ rankdir=""LR""' + s + '; }')

def get_image_files_sorted(path, recurse=True, folders=None): return get_image_files(path, recurse, folders).sorted()


# +
# pip install azure-cognitiveservices-search-imagesearch

from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api
from msrest.authentication import CognitiveServicesCredentials as auth

def search_images_bing(key, term, min_sz=128, max_images=150):    
     params = {'q':term, 'count':max_images, 'minHeight':min_sz, 'minWidth':min_sz, 'imageType':'photo'}
     headers = {""Ocp-Apim-Subscription-Key"":key}
     search_url = ""https://api.bing.microsoft.com/v7.0/images/search""
     response = requests.get(search_url, headers=headers, params=params)
     response.raise_for_status()
     search_results = response.json()
     return L(search_results['value'])


# -

def search_images_ddg(key,max_n=200):
     """"""Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images
        (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api)
     """"""
     url        = 'https://duckduckgo.com/'
     params     = {'q':key}
     res        = requests.post(url,data=params)
     searchObj  = re.search(r'vqd=([\d-]+)\&',res.text)
     if not searchObj: print('Token Parsing Failed !'); return
     requestUrl = url + 'i.js'
     headers    = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'}
     params     = (('l','us-en'),('o','json'),('q',key),('vqd',searchObj.group(1)),('f',',,,'),('p','1'),('v7exp','a'))
     urls       = []
     while True:
         try:
             res  = requests.get(requestUrl,headers=headers,params=params)
             data = json.loads(res.text)
             for obj in data['results']:
                 urls.append(obj['image'])
                 max_n = max_n - 1
                 if max_n < 1: return L(set(urls))     # dedupe
             if 'next' not in data: return L(set(urls))
             requestUrl = url + data['next']
         except:
             pass


def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):
    x = torch.linspace(min,max)
    fig,ax = plt.subplots(figsize=figsize)
    ax.plot(x,f(x))
    if tx is not None: ax.set_xlabel(tx)
    if ty is not None: ax.set_ylabel(ty)
    if title is not None: ax.set_title(title)

# +
from sklearn.tree import export_graphviz

def draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs):
    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,
                      special_characters=True, rotate=False, precision=precision, **kwargs)
    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))


# +
from scipy.cluster import hierarchy as hc

def cluster_columns(df, figsize=(10,6), font_size=12):
    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)
    corr_condensed = hc.distance.squareform(1-corr)
    z = hc.linkage(corr_condensed, method='average')
    fig = plt.figure(figsize=figsize)
    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)
    plt.show()
",15.0,0,8,0.0,14,1.0,69.0,0,
vocoder_train.py,"import argparse
from pathlib import Path

from utils.argutils import print_args
from vocoder.train import train


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Trains the vocoder from the synthesizer audios and the GTA synthesized mels, ""
                    ""or ground truth mels."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(""run_id"", type=str, help= \
        ""Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state ""
        ""from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved ""
        ""states and restart from scratch."")
    parser.add_argument(""datasets_root"", type=Path, help= \
        ""Path to the directory containing your SV2TTS directory. Specifying --syn_dir or --voc_dir ""
        ""will take priority over this argument."")
    parser.add_argument(""--syn_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the synthesizer directory that contains the ground truth mel spectrograms, ""
        ""the wavs and the embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/."")
    parser.add_argument(""--voc_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the vocoder directory that contains the GTA synthesized mel spectrograms. ""
        ""Defaults to <datasets_root>/SV2TTS/vocoder/. Unused if --ground_truth is passed."")
    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""saved_models"", help=\
        ""Path to the directory that will contain the saved model weights, as well as backups ""
        ""of those weights and wavs generated during training."")
    parser.add_argument(""-g"", ""--ground_truth"", action=""store_true"", help= \
        ""Train on ground truth spectrograms (<datasets_root>/SV2TTS/synthesizer/mels)."")
    parser.add_argument(""-s"", ""--save_every"", type=int, default=1000, help= \
        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""
        ""model."")
    parser.add_argument(""-b"", ""--backup_every"", type=int, default=25000, help= \
        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""
        ""model."")
    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \
        ""Do not load any saved model and restart from scratch."")
    args = parser.parse_args()

    # Process the arguments
    if not hasattr(args, ""syn_dir""):
        args.syn_dir = args.datasets_root / ""SV2TTS"" / ""synthesizer""
    if not hasattr(args, ""voc_dir""):
        args.voc_dir = args.datasets_root / ""SV2TTS"" / ""vocoder""
    del args.datasets_root
    args.models_dir.mkdir(exist_ok=True)

    # Run the training
    print_args(args, parser)
    train(**vars(args))
",3.0,0,0,0.0,5,1.0,45.0,0,
wechat_public_account.py,"#-*- coding:UTF-8 -*-
import json
import time
import pdfkit

import requests

base_url = 'https://mp.weixin.qq.com/mp/profile_ext'


# 这些信息不能抄我的，要用你自己的才有效
headers = {
    'Connection': 'keep - alive',
    'Accept': '* / *',
    'User-Agent': '写你自己的',
    'Referer': '写你自己的',
    'Accept-Encoding': 'br, gzip, deflate'
}

cookies = {
    'devicetype': 'iOS12.2',
    'lang': 'zh_CN',
    'pass_ticket': '写你自己的',
    'version': '1700042b',
    'wap_sid2': '写你自己的',
    'wxuin': '3340537333'
}



def get_params(offset):
    params = {
        'action': 'getmsg',
        '__biz': '写你自己的',
        'f': 'json',
        'offset': '{}'.format(offset),
        'count': '10',
        'is_ok': '1',
        'scene': '126',
        'uin': '777',
        'key': '777',
        'pass_ticket': '写你自己的',
        'appmsg_token': '写你自己的',
        'x5': '0',
        'f': 'json',
    }

    return params


def get_list_data(offset):
    res = requests.get(base_url, headers=headers, params=get_params(offset), cookies=cookies)
    data = json.loads(res.text)
    can_msg_continue = data['can_msg_continue']
    next_offset = data['next_offset']

    general_msg_list = data['general_msg_list']
    list_data = json.loads(general_msg_list)['list']

    for data in list_data:
        try:
            if data['app_msg_ext_info']['copyright_stat'] == 11:
                msg_info = data['app_msg_ext_info']
                title = msg_info['title']
                content_url = msg_info['content_url']
                # 自己定义存储路径
                pdfkit.from_url(content_url, '/home/wistbean/wechat_article/'+title+'.pdf')
                print('获取到原创文章：%s ： %s' % (title, content_url))
        except:
            print('不是图文')

    if can_msg_continue == 1:
        time.sleep(1)
        get_list_data(next_offset)


if __name__ == '__main__':
    get_list_data(0)",6.0,0,3,0.0,7,1.0,59.0,0,
