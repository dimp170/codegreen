file_name,full_code,complexity,bugs,code_smells,duplicated_lines_density,cognitive_complexity,security_rating,ncloc,vulnerabilities,optimized_code
main.py,"from dotenv import load_dotenv
import logging
from pathlib import Path

# Create logs directory if it doesn't exist
logs_dir = Path(""logs"")
logs_dir.mkdir(exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        # File handler for general application logs
        logging.FileHandler('logs/app.log'),
        # Stream handler for console output
        logging.StreamHandler()
    ]
)

# Suppress verbose fontTools logging
logging.getLogger('fontTools').setLevel(logging.WARNING)
logging.getLogger('fontTools.subset').setLevel(logging.WARNING)
logging.getLogger('fontTools.ttLib').setLevel(logging.WARNING)

# Create logger instance
logger = logging.getLogger(__name__)

load_dotenv()

from backend.server.server import app

if __name__ == ""__main__"":
    import uvicorn
    
    logger.info(""Starting server..."")
    uvicorn.run(app, host=""0.0.0.0"", port=8000)",1.0,0,0,0.0,1,1.0,23.0,0,
douban_top_250_books_mul_process.py,"import requests
from bs4 import BeautifulSoup
import xlwt
import multiprocessing
import time
import sys

def request_douban(url):
    try:
        response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None

def main(url):
    sys.setrecursionlimit(1000000)
    data = []
    html = request_douban(url)
    # soup = BeautifulSoup(html, 'lxml')
    soup = BeautifulSoup(html, 'html.parser')
    list = soup.find(class_='grid_view').find_all('li')
    for item in list:
        item_name = item.find(class_='title').string
        item_img = item.find('a').find('img').get('src')
        item_index = item.find(class_='').string
        item_score = item.find(class_='rating_num').string
        item_author = item.find('p').text
        item_intr = ''
        if (item.find(class_='inq') != None):
            item_intr = item.find(class_='inq').string
        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)
        item = {
            'item_index': item_index,
            'item_name': item_name,
            'item_score': item_score,
            'item_intr': item_intr,
            'item_img': item_img,
            'item_author': item_author
        }
        data.append(item)
    return data
    
if __name__ == '__main__':
    startTime = time.time()
    data = []
    urls = []
    pool = multiprocessing.Pool(multiprocessing.cpu_count()-1)
    for i in range(0, 10):
        url = 'https://movie.douban.com/top250?start=' + str(i * 25) + '&filter='
        urls.append(url)
    pool.map(main, urls)
    for pageItem in pool.map(main, urls):
        data.extend(pageItem)
    book = xlwt.Workbook(encoding='utf-8', style_compression=0)
    sheet = book.add_sheet('豆瓣电影Top250-test', cell_overwrite_ok=True)
    sheet.write(0, 0, '名称')
    sheet.write(0, 1, '图片')
    sheet.write(0, 2, '排名')
    sheet.write(0, 3, '评分')
    sheet.write(0, 4, '作者')
    sheet.write(0, 5, '简介')
    for index,item in enumerate(data):
        sheet.write(index+1, 0, item['item_name'])
        sheet.write(index+1, 1, item['item_img'])
        sheet.write(index+1, 2, item['item_index'])
        sheet.write(index+1, 3, item['item_score'])
        sheet.write(index+1, 4, item['item_author'])
        sheet.write(index+1, 5, item['item_intr'])
    book.save(u'豆瓣最受欢迎的250部电影-mul.xlsx')

    endTime = time.time()
    dtime = endTime - startTime
    print(""程序运行时间：%s s"" % dtime)  # 4.036666631698608 s",9.0,0,2,0.0,12,1.0,69.0,0,
vocoder_preprocess.py,"import argparse
import os
from pathlib import Path

from synthesizer.hparams import hparams
from synthesizer.synthesize import run_synthesis
from utils.argutils import print_args



if __name__ == ""__main__"":
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
        pass

    parser = argparse.ArgumentParser(
        description=""Creates ground-truth aligned (GTA) spectrograms from the vocoder."",
        formatter_class=MyFormatter
    )
    parser.add_argument(""datasets_root"", type=Path, help=\
        ""Path to the directory containing your SV2TTS directory. If you specify both --in_dir and ""
        ""--out_dir, this argument won't be used."")
    parser.add_argument(""-s"", ""--syn_model_fpath"", type=Path,
                        default=""saved_models/default/synthesizer.pt"",
                        help=""Path to a saved synthesizer"")
    parser.add_argument(""-i"", ""--in_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the synthesizer directory that contains the mel spectrograms, the wavs and the ""
        ""embeds. Defaults to  <datasets_root>/SV2TTS/synthesizer/."")
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help= \
        ""Path to the output vocoder directory that will contain the ground truth aligned mel ""
        ""spectrograms. Defaults to <datasets_root>/SV2TTS/vocoder/."")
    parser.add_argument(""--hparams"", default="""", help=\
        ""Hyperparameter overrides as a comma-separated list of name=value pairs"")
    parser.add_argument(""--cpu"", action=""store_true"", help=\
        ""If True, processing is done on CPU, even when a GPU is available."")
    args = parser.parse_args()
    print_args(args, parser)
    modified_hp = hparams.parse(args.hparams)

    if not hasattr(args, ""in_dir""):
        args.in_dir = args.datasets_root / ""SV2TTS"" / ""synthesizer""
    if not hasattr(args, ""out_dir""):
        args.out_dir = args.datasets_root / ""SV2TTS"" / ""vocoder""

    if args.cpu:
        # Hide GPUs from Pytorch to force CPU processing
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

    run_synthesis(args.in_dir, args.out_dir, args.syn_model_fpath, modified_hp)
",4.0,0,0,0.0,7,1.0,39.0,0,
Binary_search.py,"# It returns location of x in given array arr
# if present, else returns -1
def binary_search(arr, l, r, x):
    # Base case: if left index is greater than right index, element is not present
    if l > r:
        return -1

    # Calculate the mid index
    mid = (l + r) // 2

    # If element is present at the middle itself
    if arr[mid] == x:
        return mid

    # If element is smaller than mid, then it can only be present in left subarray
    elif arr[mid] > x:
        return binary_search(arr, l, mid - 1, x)

    # Else the element can only be present in right subarray
    else:
        return binary_search(arr, mid + 1, r, x)


# Main Function
if __name__ == ""__main__"":
    # User input array
    arr = [int(x) for x in input(""Enter the array with elements separated by commas: "").split("","")]

    # User input element to search for
    x = int(input(""Enter the element you want to search for: ""))

    # Function call
    result = binary_search(arr, 0, len(arr) - 1, x)

    # printing the output
    if result != -1:
        print(""Element is present at index {}"".format(result))
    else:
        print(""Element is not present in array"")
",5.0,0,0,0.0,8,1.0,18.0,0,
wechat_moment.py,"import time

from appium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class Wechat_Moment():
    def __init__(self):
        desired_caps = {}
        desired_caps['platformName'] = 'Android'
        desired_caps['platformVersion'] = '5.1'
        desired_caps['deviceName'] = '88CKBM622PAM'
        desired_caps['appPackage'] = 'com.tencent.mm'
        desired_caps['appActivity'] = '.ui.LauncherUI'

        # 定义在朋友圈的时候滑动位置
        self.start_x = 300
        self.start_y = 800
        self.end_x = 300
        self.end_y = 300

        # 启动微信
        self.driver = webdriver.Remote('http://localhost:4723/wd/hub', desired_caps)
        # 设置等待
        self.wait = WebDriverWait(self.driver, 300)
        print('微信启动...')


    def login(self):
        # 获取到登录按钮后点击
        login_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/e4g"")))
        login_btn.click()
        # 获取使用微信号登录按钮
        change_login_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/cou"")))
        change_login_btn.click()
        # 获取输入账号元素并输入
        account = self.wait.until(EC.presence_of_element_located((By.XPATH, '//*[@resource-id=""com.tencent.mm:id/cos""]/android.widget.EditText')))
        account.send_keys(""xxxxxxxx"")
        # 获取密码元素并输入
        password = self.wait.until(EC.presence_of_element_located((By.XPATH,  '//*[@resource-id=""com.tencent.mm:id/cot""]/android.widget.EditText')))
        password.send_keys(""xxxxxx"")
        # 登录
        login = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/cov"")))
        login.click()
        # 点击去掉通讯录提示框
        no_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/az9"")))
        no_btn.click()
        print('登录成功...')


    def find_xiaoshuaib(self):
        # 获取到搜索按钮后点击
        search_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/iq"")))
        # 等搜索建立索引再点击
        time.sleep(10)
        search_btn.click()
        # 获取搜索框并输入
        search_input = self.wait.until(EC.presence_of_element_located((By.ID, ""com.tencent.mm:id/kh"")))
        search_input.send_keys(""wistbean"")
        print('搜索小帅b...')
        # 点击头像进入
        xiaoshuaib_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/py"")))
        xiaoshuaib_btn.click()
        # 点击右上角...进入
        menu_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/jy"")))
        menu_btn.click()
        # 再点击头像
        icon_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/e0c"")))
        icon_btn.click()
        # 点击朋友圈
        moment_btn = self.wait.until(EC.element_to_be_clickable((By.ID, ""com.tencent.mm:id/d86"")))
        moment_btn.click()
        print('进入朋友圈...')

    def get_data(self):
        while True:
            # 获取 ListView
            items = self.wait.until(EC.presence_of_all_elements_located((By.ID, 'com.tencent.mm:id/eew')))
            # 滑动
            self.driver.swipe(self.start_x, self.start_y, self.end_x, self.end_y, 2000)
            #遍历获取每个List数据
            for item in items:
                moment_text = item.find_element_by_id('com.tencent.mm:id/kt').text
                day_text = item.find_element_by_id('com.tencent.mm:id/eke').text
                month_text = item.find_element_by_id('com.tencent.mm:id/ekf').text
                print('抓取到小帅b朋友圈数据： %s' % moment_text)
                print('抓取到小帅b发布时间： %s月%s' % (month_text, day_text))

if __name__ == '__main__':
    wc_moment = Wechat_Moment()
    wc_moment.login()
    wc_moment.find_xiaoshuaib()
    wc_moment.get_data()











",7.0,0,1,0.0,4,1.0,65.0,0,
sort.py,"#!/usr/bin/env python
# coding: utf-8

""""""
    The approach taken is explained below. I decided to do it simply.
    Initially I was considering parsing the data into some sort of
    structure and then generating an appropriate README. I am still
    considering doing it - but for now this should work. The only issue
    I see is that it only sorts the entries at the lowest level, and that
    the order of the top-level contents do not match the order of the actual
    entries.

    This could be extended by having nested blocks, sorting them recursively
    and flattening the end structure into a list of lines. Revision 2 maybe ^.^.
""""""

def sort_blocks():
    # First, we load the current README into memory
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.read()

    # Separating the 'table of contents' from the contents (blocks)
    table_of_contents = ''.join(read_me.split('- - -')[0])
    blocks = ''.join(read_me.split('- - -')[1]).split('\n# ')
    for i in range(len(blocks)):
        if i == 0:
            blocks[i] = blocks[i] + '\n'
        else:
            blocks[i] = '# ' + blocks[i] + '\n'

    # Sorting the libraries
    inner_blocks = sorted(blocks[0].split('##'))
    for i in range(1, len(inner_blocks)):
        if inner_blocks[i][0] != '#':
            inner_blocks[i] = '##' + inner_blocks[i]
    inner_blocks = ''.join(inner_blocks)

    # Replacing the non-sorted libraries by the sorted ones and gathering all at the final_README file
    blocks[0] = inner_blocks
    final_README = table_of_contents + '- - -' + ''.join(blocks)

    with open('README.md', 'w+') as sorted_file:
        sorted_file.write(final_README)

def main():
    # First, we load the current README into memory as an array of lines
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.readlines()

    # Then we cluster the lines together as blocks
    # Each block represents a collection of lines that should be sorted
    # This was done by assuming only links ([...](...)) are meant to be sorted
    # Clustering is done by indentation
    blocks = []
    last_indent = None
    for line in read_me:
        s_line = line.lstrip()
        indent = len(line) - len(s_line)

        if any([s_line.startswith(s) for s in ['* [', '- [']]):
            if indent == last_indent:
                blocks[-1].append(line)
            else:
                blocks.append([line])
            last_indent = indent
        else:
            blocks.append([line])
            last_indent = None

    with open('README.md', 'w+') as sorted_file:
        # Then all of the blocks are sorted individually
        blocks = [
            ''.join(sorted(block, key=str.lower)) for block in blocks
        ]
        # And the result is written back to README.md
        sorted_file.write(''.join(blocks))

    # Then we call the sorting method
    sort_blocks()


if __name__ == ""__main__"":
    main()
",10.0,0,2,0.0,16,1.0,44.0,0,
manage.py,"#!/usr/bin/env python3
import os
import sys

if __name__ == ""__main__"":
    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""saleor.settings"")

    from django.core.management import execute_from_command_line

    execute_from_command_line(sys.argv)
",1.0,0,0,0.0,1,1.0,6.0,0,
douyin_pro.py,"# -*- coding:utf-8 -*-
from splinter.driver.webdriver.chrome import Options, Chrome
from splinter.browser import Browser
from contextlib import closing
import requests, json, time, re, os, sys, time
from bs4 import BeautifulSoup

class DouYin(object):
	def __init__(self, width = 500, height = 300):
		""""""
		抖音App视频下载
		""""""
		# 无头浏览器
		chrome_options = Options()
		chrome_options.add_argument('user-agent=""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36""')
		self.driver = Browser(driver_name='chrome', executable_path='D:/chromedriver', options=chrome_options, headless=True)

	def get_video_urls(self, user_id):
		""""""
		获得视频播放地址
		Parameters:
			user_id：查询的用户ID
		Returns:
			video_names: 视频名字列表
			video_urls: 视频链接列表
			nickname: 用户昵称
		""""""
		video_names = []
		video_urls = []
		unique_id = ''
		while unique_id != user_id:
			search_url = 'https://api.amemv.com/aweme/v1/discover/search/?cursor=0&keyword=%s&count=10&type=1&retry_type=no_retry&iid=17900846586&device_id=34692364855&ac=wifi&channel=xiaomi&aid=1128&app_name=aweme&version_code=162&version_name=1.6.2&device_platform=android&ssmix=a&device_type=MI+5&device_brand=Xiaomi&os_api=24&os_version=7.0&uuid=861945034132187&openudid=dc451556fc0eeadb&manifest_version_code=162&resolution=1080*1920&dpi=480&update_version_code=1622' % user_id
			req = requests.get(url = search_url, verify = False)
			html = json.loads(req.text)
			aweme_count = html['user_list'][0]['user_info']['aweme_count']
			uid = html['user_list'][0]['user_info']['uid']
			nickname = html['user_list'][0]['user_info']['nickname']
			unique_id = html['user_list'][0]['user_info']['unique_id']
		user_url = 'https://www.douyin.com/aweme/v1/aweme/post/?user_id=%s&max_cursor=0&count=%s' % (uid, aweme_count)
		req = requests.get(url = user_url, verify = False)
		html = json.loads(req.text)
		i = 1
		for each in html['aweme_list']:
			share_desc = each['share_info']['share_desc']
			if '抖音-原创音乐短视频社区' == share_desc:
				video_names.append(str(i) + '.mp4')
				i += 1
			else:
				video_names.append(share_desc + '.mp4')
			video_urls.append(each['share_info']['share_url'])

		return video_names, video_urls, nickname

	def get_download_url(self, video_url):
		""""""
		获得带水印的视频播放地址
		Parameters:
			video_url：带水印的视频播放地址
		Returns:
			download_url: 带水印的视频下载地址
		""""""
		req = requests.get(url = video_url, verify = False)
		bf = BeautifulSoup(req.text, 'lxml')
		script = bf.find_all('script')[-1]
		video_url_js = re.findall('var data = \[(.+)\];', str(script))[0]
		video_html = json.loads(video_url_js)
		download_url = video_html['video']['play_addr']['url_list'][0]
		return download_url

	def video_downloader(self, video_url, video_name, watermark_flag=True):
		""""""
		视频下载
		Parameters:
			video_url: 带水印的视频地址
			video_name: 视频名
			watermark_flag: 是否下载不带水印的视频
		Returns:
			无
		""""""
		size = 0
		if watermark_flag == True:
			video_url = self.remove_watermark(video_url)
		else:
			video_url = self.get_download_url(video_url)
		with closing(requests.get(video_url, stream=True, verify = False)) as response:
			chunk_size = 1024
			content_size = int(response.headers['content-length']) 
			if response.status_code == 200:
				sys.stdout.write('  [文件大小]:%0.2f MB\n' % (content_size / chunk_size / 1024))

				with open(video_name, ""wb"") as file:  
					for data in response.iter_content(chunk_size = chunk_size):
						file.write(data)
						size += len(data)
						file.flush()

						sys.stdout.write('  [下载进度]:%.2f%%' % float(size / content_size * 100) + '\r')
						sys.stdout.flush()


	def remove_watermark(self, video_url):
		""""""
		获得无水印的视频播放地址
		Parameters:
			video_url: 带水印的视频地址
		Returns:
			无水印的视频下载地址
		""""""
		self.driver.visit('http://douyin.iiilab.com/')
		self.driver.find_by_tag('input').fill(video_url)
		self.driver.find_by_xpath('//button[@class=""btn btn-default""]').click()
		html = self.driver.find_by_xpath('//div[@class=""thumbnail""]/div/p')[0].html
		bf = BeautifulSoup(html, 'lxml')
		return bf.find('a').get('href')

	def run(self):
		""""""
		运行函数
		Parameters:
			None
		Returns:
			None
		""""""
		self.hello()
		user_id = input('请输入ID(例如40103580):')
		video_names, video_urls, nickname = self.get_video_urls(user_id)
		if nickname not in os.listdir():
			os.mkdir(nickname)
		print('视频下载中:共有%d个作品!\n' % len(video_urls))
		for num in range(len(video_urls)):
			print('  解析第%d个视频链接 [%s] 中，请稍后!\n' % (num+1, video_urls[num]))
			if '\\' in video_names[num]:
				video_name = video_names[num].replace('\\', '')
			elif '/' in video_names[num]:
				video_name = video_names[num].replace('/', '')
			else:
				video_name = video_names[num]
			self.video_downloader(video_urls[num], os.path.join(nickname, video_name))
			print('\n')

		print('下载完成!')

	def hello(self):
		""""""
		打印欢迎界面
		Parameters:
			None
		Returns:
			None
		""""""
		print('*' * 100)
		print('\t\t\t\t抖音App视频下载小助手')
		print('\t\t作者:Jack Cui')
		print('*' * 100)


if __name__ == '__main__':
	douyin = DouYin()
	douyin.run()
",17.0,0,0,16.9,17,4.0,94.0,4,
Armstrong_number.py,"""""""

In number theory, a narcissistic number (also known as a pluperfect digital invariant (PPDI), an Armstrong number (after Michael F. Armstrong) or a plus perfect number), 

in a given number base b, is a number that is the total of its own digits each raised to the power of the number of digits.

Source: https://en.wikipedia.org/wiki/Narcissistic_number

NOTE:

this scripts only works for number in base 10

""""""



def is_armstrong_number(number:str):

    total:int = 0

    exp:int = len(number) #get the number of digits, this will determinate the exponent



    digits:list[int] = []

    for digit in number: digits.append(int(digit)) #get the single digits

    for x in digits: total += x ** exp #get the power of each digit and sum it to the total

    

    # display the result

    if int(number) == total:

       print(number,""is an Armstrong number"")

    else:

       print(number,""is not an Armstrong number"")



number = input(""Enter the number : "")

is_armstrong_number(number)

",4.0,0,0,0.0,4,1.0,12.0,0,
12306.py,"# -*- coding: utf-8 -*-
""""""
@author: liuyw
""""""
from splinter.browser import Browser
from time import sleep
import traceback
import time, sys

class huoche(object):
	driver_name = ''
	executable_path = ''
	#用户名，密码
	username = u""xxx""
	passwd = u""xxx""
	# cookies值得自己去找, 下面两个分别是沈阳, 哈尔滨
	starts = u""%u6C88%u9633%2CSYT""
	ends = u""%u54C8%u5C14%u6EE8%2CHBB""
	
	# 时间格式2018-01-19
	dtime = u""2018-01-19""
	# 车次，选择第几趟，0则从上之下依次点击
	order = 0
	###乘客名
	users = [u""xxx"",u""xxx""]
	##席位
	xb = u""二等座""
	pz = u""成人票""

	""""""网址""""""
	ticket_url = ""https://kyfw.12306.cn/otn/leftTicket/init""
	login_url = ""https://kyfw.12306.cn/otn/login/init""
	initmy_url = ""https://kyfw.12306.cn/otn/index/initMy12306""
	buy = ""https://kyfw.12306.cn/otn/confirmPassenger/initDc""
	
	def __init__(self):
		self.driver_name = 'chrome'
		self.executable_path = 'D:/chromedriver'

	def login(self):
		self.driver.visit(self.login_url)
		self.driver.fill(""loginUserDTO.user_name"", self.username)
		# sleep(1)
		self.driver.fill(""userDTO.password"", self.passwd)
		print(u""等待验证码，自行输入..."")
		while True:
			if self.driver.url != self.initmy_url:
				sleep(1)
			else:
				break

	def start(self):
		self.driver = Browser(driver_name=self.driver_name,executable_path=self.executable_path)
		self.driver.driver.set_window_size(1400, 1000)
		self.login()
		# sleep(1)
		self.driver.visit(self.ticket_url)
		try:
			print(u""购票页面开始..."")
			# sleep(1)
			# 加载查询信息
			self.driver.cookies.add({""_jc_save_fromStation"": self.starts})
			self.driver.cookies.add({""_jc_save_toStation"": self.ends})
			self.driver.cookies.add({""_jc_save_fromDate"": self.dtime})

			self.driver.reload()

			count = 0
			if self.order != 0:
				while self.driver.url == self.ticket_url:
					self.driver.find_by_text(u""查询"").click()
					count += 1
					print(u""循环点击查询... 第 %s 次"" % count)
					# sleep(1)
					try:
						self.driver.find_by_text(u""预订"")[self.order - 1].click()
					except Exception as e:
						print(e)
						print(u""还没开始预订"")
						continue
			else:
				while self.driver.url == self.ticket_url:
					self.driver.find_by_text(u""查询"").click()
					count += 1
					print(u""循环点击查询... 第 %s 次"" % count)
					# sleep(0.8)
					try:
						for i in self.driver.find_by_text(u""预订""):
							i.click()
							sleep(1)
					except Exception as e:
						print(e)
						print(u""还没开始预订 %s"" % count)
						continue
			print(u""开始预订..."")
			# sleep(3)
			# self.driver.reload()
			sleep(1)
			print(u'开始选择用户...')
			for user in self.users:
				self.driver.find_by_text(user).last.click()

			print(u""提交订单..."")
			sleep(1)
			self.driver.find_by_text(self.pz).click()
			self.driver.find_by_id('').select(self.pz)
			# sleep(1)
			self.driver.find_by_text(self.xb).click()
			sleep(1)
			self.driver.find_by_id('submitOrder_id').click()
			print(u""开始选座..."")
			self.driver.find_by_id('1D').last.click()
			self.driver.find_by_id('1F').last.click()

			sleep(1.5)
			print(u""确认选座..."")
			self.driver.find_by_id('qr_submit_id').click()

		except Exception as e:
			print(e)

if __name__ == '__main__':
	huoche = huoche()
	huoche.start()",11.0,0,3,0.0,22,1.0,93.0,0,
conanfile.py,"#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""Conan recipe package for Google FlatBuffers
""""""
import os
import shutil
from conans import ConanFile, CMake, tools


class FlatbuffersConan(ConanFile):
    name = ""flatbuffers""
    license = ""Apache-2.0""
    url = ""https://github.com/google/flatbuffers""
    homepage = ""http://google.github.io/flatbuffers/""
    author = ""Wouter van Oortmerssen""
    topics = (""conan"", ""flatbuffers"", ""serialization"", ""rpc"", ""json-parser"")
    description = ""Memory Efficient Serialization Library""
    settings = ""os"", ""compiler"", ""build_type"", ""arch""
    options = {""shared"": [True, False], ""fPIC"": [True, False]}
    default_options = {""shared"": False, ""fPIC"": True}
    generators = ""cmake""
    exports = ""LICENSE""
    exports_sources = [""CMake/*"", ""include/*"", ""src/*"", ""grpc/*"", ""CMakeLists.txt"", ""conan/CMakeLists.txt""]

    def source(self):
        """"""Wrap the original CMake file to call conan_basic_setup
        """"""
        shutil.move(""CMakeLists.txt"", ""CMakeListsOriginal.txt"")
        shutil.move(os.path.join(""conan"", ""CMakeLists.txt""), ""CMakeLists.txt"")

    def config_options(self):
        """"""Remove fPIC option on Windows platform
        """"""
        if self.settings.os == ""Windows"":
            self.options.remove(""fPIC"")

    def configure_cmake(self):
        """"""Create CMake instance and execute configure step
        """"""
        cmake = CMake(self)
        cmake.definitions[""FLATBUFFERS_BUILD_TESTS""] = False
        cmake.definitions[""FLATBUFFERS_BUILD_SHAREDLIB""] = self.options.shared
        cmake.definitions[""FLATBUFFERS_BUILD_FLATLIB""] = not self.options.shared
        cmake.configure()
        return cmake

    def build(self):
        """"""Configure, build and install FlatBuffers using CMake.
        """"""
        cmake = self.configure_cmake()
        cmake.build()

    def package(self):
        """"""Copy Flatbuffers' artifacts to package folder
        """"""
        cmake = self.configure_cmake()
        cmake.install()
        self.copy(pattern=""LICENSE"", dst=""licenses"")
        self.copy(pattern=""FindFlatBuffers.cmake"", dst=os.path.join(""lib"", ""cmake"", ""flatbuffers""), src=""CMake"")
        self.copy(pattern=""flathash*"", dst=""bin"", src=""bin"")
        self.copy(pattern=""flatc*"", dst=""bin"", src=""bin"")
        if self.settings.os == ""Windows"" and self.options.shared:
            if self.settings.compiler == ""Visual Studio"":
                shutil.move(os.path.join(self.package_folder, ""lib"", ""%s.dll"" % self.name),
                            os.path.join(self.package_folder, ""bin"", ""%s.dll"" % self.name))
            elif self.settings.compiler == ""gcc"":
                shutil.move(os.path.join(self.package_folder, ""lib"", ""lib%s.dll"" % self.name),
                            os.path.join(self.package_folder, ""bin"", ""lib%s.dll"" % self.name))

    def package_info(self):
        """"""Collect built libraries names and solve flatc path.
        """"""
        self.cpp_info.libs = tools.collect_libs(self)
        self.user_info.flatc = os.path.join(self.package_folder, ""bin"", ""flatc"")
",10.0,0,1,0.0,6,1.0,50.0,0,
render_readme.py,"#!/usr/bin/env python

import re
from pathlib import Path

README_TEMPLATE_FILEPATH = ""readme_template.md""
GETTING_STARTED_TEMPLATE_FILEPATH = ""guides/01_getting-started/01_quickstart.md""

readme_template = Path(README_TEMPLATE_FILEPATH).read_text()
getting_started_template = Path(GETTING_STARTED_TEMPLATE_FILEPATH).read_text()
getting_started_template = getting_started_template.replace(""# Quickstart"", """")
getting_started_template = getting_started_template.replace(""Tip:"", ""> [!TIP]\n >"")

# Extract all the code and demo tags from the getting started template
code_tags = re.findall(r""\$code_([^\s]+)"", getting_started_template)
demo_tags = re.findall(r""\$demo_([^\s]+)"", getting_started_template)
codes = {}
demos = {}

for src in code_tags:
    context = Path(f""demo/{src}/run.py"").read_text()
    # Replace the condition to run the demo directly with actual launch code
    context = re.sub(r""if __name__(.*[\n$]*)*"", ""demo.launch()"", context)
    codes[src] = f""```python\n{context}\n```\n""  # Convert to Markdown code block

for src in demo_tags:
    demos[src] = f""![`{src}` demo](demo/{src}/screenshot.gif)""

# Replace the headers in the getting started template with a smaller header (e.g. H3 instead of H2) to
# make the README more readable and less cluttered.
getting_started_template = re.sub(r""^(#+)"", r""#\1"", getting_started_template, flags=re.MULTILINE)
readme_template = readme_template.replace(""$getting_started"", getting_started_template)

# Now put the codes and the screenshots in the README template
readme_template = re.sub(r""\$code_([^\s]+)"", lambda x: codes[x.group(1)], readme_template)
readme_template = re.sub(r""\$demo_([^\s]+)"", lambda x: demos[x.group(1)], readme_template)

# Save the README template to the actual README.md file (with a note about the editing)
EDITING_NOTE = (""<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR ""
                ""`guides/1)getting_started/1)quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->"")
Path(""README.md"").write_text(f""{EDITING_NOTE}\n\n{readme_template}"")
",2.0,1,0,0.0,2,1.0,25.0,0,
financical.py,"#-*- coding:UTF-8 -*-
import sys
import pymysql
import requests
import json
import re
from bs4 import BeautifulSoup

""""""
类说明:获取财务数据

Author:
	Jack Cui
Blog:
	http://blog.csdn.net/c406495762
Zhihu:
	https://www.zhihu.com/people/Jack--Cui/
Modify:
	2017-08-31
""""""
class FinancialData():

	def __init__(self):
		#服务器域名
		self.server = 'http://quotes.money.163.com/'
		self.cwnb = 'http://quotes.money.163.com/hkstock/cwsj_'
		#主要财务指标
		self.cwzb_dict = {'EPS':'基本每股收益','EPS_DILUTED':'摊薄每股收益','GROSS_MARGIN':'毛利率',
		'CAPITAL_ADEQUACY':'资本充足率','LOANS_DEPOSITS':'贷款回报率','ROTA':'总资产收益率',
		'ROEQUITY':'净资产收益率','CURRENT_RATIO':'流动比率','QUICK_RATIO':'速动比率',
		'ROLOANS':'存贷比','INVENTORY_TURNOVER':'存货周转率','GENERAL_ADMIN_RATIO':'管理费用比率',
		'TOTAL_ASSET2TURNOVER':'资产周转率','FINCOSTS_GROSSPROFIT':'财务费用比率','TURNOVER_CASH':'销售现金比率','YEAREND_DATE':'报表日期'}
		#利润表
		self.lrb_dict = {'TURNOVER':'总营收','OPER_PROFIT':'经营利润','PBT':'除税前利润',
		'NET_PROF':'净利润','EPS':'每股基本盈利','DPS':'每股派息',
		'INCOME_INTEREST':'利息收益','INCOME_NETTRADING':'交易收益','INCOME_NETFEE':'费用收益','YEAREND_DATE':'报表日期'}
		#资产负债表
		self.fzb_dict = {
			'FIX_ASS':'固定资产','CURR_ASS':'流动资产','CURR_LIAB':'流动负债',
			'INVENTORY':'存款','CASH':'现金及银行存结','OTHER_ASS':'其他资产',
			'TOTAL_ASS':'总资产','TOTAL_LIAB':'总负债','EQUITY':'股东权益',
			'CASH_SHORTTERMFUND':'库存现金及短期资金','DEPOSITS_FROM_CUSTOMER':'客户存款',
			'FINANCIALASSET_SALE':'可供出售之证券','LOAN_TO_BANK':'银行同业存款及贷款',
			'DERIVATIVES_LIABILITIES':'金融负债','DERIVATIVES_ASSET':'金融资产','YEAREND_DATE':'报表日期'}
		#现金流表
		self.llb_dict = {
			'CF_NCF_OPERACT':'经营活动产生的现金流','CF_INT_REC':'已收利息','CF_INT_PAID':'已付利息',
			'CF_INT_REC':'已收股息','CF_DIV_PAID':'已派股息','CF_INV':'投资活动产生现金流',
			'CF_FIN_ACT':'融资活动产生现金流','CF_BEG':'期初现金及现金等价物','CF_CHANGE_CSH':'现金及现金等价物净增加额',
			'CF_END':'期末现金及现金等价物','CF_EXCH':'汇率变动影响','YEAREND_DATE':'报表日期'}
		#总表
		self.table_dict = {'cwzb':self.cwzb_dict,'lrb':self.lrb_dict,'fzb':self.fzb_dict,'llb':self.llb_dict}
		#请求头
		self.headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
			'Accept-Encoding': 'gzip, deflate',
			'Accept-Language': 'zh-CN,zh;q=0.8',
			'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'}
	
	""""""
	函数说明:获取股票页面信息

	Author:
		Jack Cui
	Parameters:
	    url - 股票财务数据界面地址
	Returns:
	    name - 股票名
	    table_name_list - 财务报表名称
	    table_date_list - 财务报表年限
	    url_list - 财务报表查询连接
	Blog:
		http://blog.csdn.net/c406495762
	Zhihu:
		https://www.zhihu.com/people/Jack--Cui/
	Modify:
		2017-08-31
	""""""
	def get_informations(self, url):
		req = requests.get(url = url, headers = self.headers)
		req.encoding = 'utf-8'
		html = req.text
		page_bf = BeautifulSoup(html, 'lxml')
		#股票名称，股票代码
		name = page_bf.find_all('span', class_ = 'name')[0].string
		# code = page_bf.find_all('span', class_ = 'code')[0].string
		# code = re.findall('\d+',code)[0]

		#存储各个表名的列表
		table_name_list = []
		table_date_list = []
		each_date_list = []
		url_list = []
		#表名和表时间
		table_name = page_bf.find_all('div', class_ = 'titlebar3')
		for each_table_name in table_name:
			#表名
			table_name_list.append(each_table_name.span.string)
			#表时间
			for each_table_date in each_table_name.div.find_all('select', id = re.compile('.+1$')):
				url_list.append(re.findall('(\w+)1',each_table_date.get('id'))[0])
				for each_date in each_table_date.find_all('option'):
					each_date_list.append(each_date.string)
				table_date_list.append(each_date_list)
				each_date_list = []
		return name,table_name_list,table_date_list,url_list

	""""""
	函数说明:财务报表入库

	Author:
		Jack Cui
	Parameters:
	    name - 股票名
	    table_name_list - 财务报表名称
	    table_date_list - 财务报表年限
	    url_list - 财务报表查询连接
	Returns:
		无
	Blog:
		http://blog.csdn.net/c406495762
	Zhihu:
		https://www.zhihu.com/people/Jack--Cui/
	Modify:
		2017-08-31
	""""""
	def insert_tables(self, name, table_name_list,table_date_list, url_list):
		#打开数据库连接:host-连接主机地址,port-端口号,user-用户名,passwd-用户密码,db-数据库名,charset-编码
		conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='yourpasswd',db='financialdata',charset='utf8')
		#使用cursor()方法获取操作游标
		cursor = conn.cursor()  
		#插入信息
		for i in range(len(table_name_list)):
			sys.stdout.write('    [正在下载       ]    %s' % table_name_list[i] + '\r')
			#获取数据地址
			url = self.server + 'hk/service/cwsj_service.php?symbol={}&start={}&end={}&type={}&unit=yuan'.format(code,table_date_list[i][-1],table_date_list[i][0],url_list[i])
			req_table = requests.get(url = url, headers = self.headers)
			table = req_table.json()
			nums = len(table)
			value_dict = {}
			for num in range(nums):
				sys.stdout.write('    [正在下载 %.2f%%]   ' % (((num+1) / nums)*100) + '\r')
				sys.stdout.flush()
				value_dict['股票名'] = name
				value_dict['股票代码'] = code
				for key, value in table[i].items():
					if key in self.table_dict[url_list[i]]:
						value_dict[self.table_dict[url_list[i]][key]] = value

				sql1 = """"""
				INSERT INTO %s (`股票名`,`股票代码`,`报表日期`) VALUES ('%s','%s','%s')"""""" % (url_list[i],value_dict['股票名'],value_dict['股票代码'],value_dict['报表日期'])
				try:
					cursor.execute(sql1)
					# 执行sql语句
					conn.commit()
				except:
					# 发生错误时回滚
					conn.rollback()

				for key, value in value_dict.items():
					if key not in ['股票名','股票代码','报表日期']:
						sql2 = """"""
						UPDATE %s SET %s='%s' WHERE `股票名`='%s' AND `报表日期`='%s'"""""" % (url_list[i],key,value,value_dict['股票名'],value_dict['报表日期'])
						try:
							cursor.execute(sql2)
							# 执行sql语句
							conn.commit()
						except:
							# 发生错误时回滚
							conn.rollback()
				value_dict = {}
			print('    [下载完成 ')

		# 关闭数据库连接
		cursor.close()  
		conn.close()

if __name__ == '__main__':
	print('*' * 100)
	print('\t\t\t\t\t财务数据下载助手\n')
	print('作者:Jack-Cui\n')
	print('About Me:\n')
	print('  知乎:https://www.zhihu.com/people/Jack--Cui')
	print('  Blog:http://blog.csdn.net/c406495762')
	print('  Gihub:https://github.com/Jack-Cherish\n')
	print('*' * 100)
	fd = FinancialData()
	#上市股票地址
	code = input('请输入股票代码:')

	name,table_name_list,table_date_list,url_list = fd.get_informations(fd.cwnb + code + '.html')
	print('\n  %s:(%s)财务数据下载中！\n' % (name,code))
	fd.insert_tables(name,table_name_list,table_date_list,url_list)
	print('\n  %s:(%s)财务数据下载完成！' % (name,code))",13.0,0,5,0.0,32,1.0,158.0,0,
Calendar (GUI).py,"from tkinter import *
import calendar

root = Tk()
# root.geometry(""400x300"")
root.title(""Calendar"")

# Function

def text():
    month_int = int(month.get())
    year_int = int(year.get())
    cal = calendar.month(year_int, month_int)
    textfield.delete(0.0, END)
    textfield.insert(INSERT, cal)


# Creating Labels
label1 = Label(root, text=""Month:"")
label1.grid(row=0, column=0)

label2 = Label(root, text=""Year:"")
label2.grid(row=0, column=1)

# Creating spinbox
month = Spinbox(root, from_=1, to=12, width=8)
month.grid(row=1, column=0, padx=5)

year = Spinbox(root, from_=2000, to=2100, width=10)
year.grid(row=1, column=1, padx=10)

# Creating Button
button = Button(root, text=""Go"", command=text)
button.grid(row=1, column=2, padx=10)

# Creating Textfield
textfield = Text(root, width=25, height=10, fg=""red"")
textfield.grid(row=2, columnspan=2)


root.mainloop()
",1.0,0,1,0.0,0,1.0,23.0,0,
downloader.py,"#-*- coding: UTF-8 -*-
import requests  
from contextlib import closing

class ProgressBar(object):  
    def __init__(self, title, count=0.0, run_status=None, fin_status=None, total=100.0, unit='', sep='/', chunk_size=1.0):  
        super(ProgressBar, self).__init__()  
        self.info = ""[%s] %s %.2f %s %s %.2f %s""  
        self.title = title  
        self.total = total  
        self.count = count  
        self.chunk_size = chunk_size  
        self.status = run_status or """"  
        self.fin_status = fin_status or "" "" * len(self.status)  
        self.unit = unit  
        self.seq = sep  
  
    def __get_info(self):  
        #[名称] 状态 进度 单位 分割线 总数 单位  
        _info = self.info % (self.title, self.status, self.count/self.chunk_size, self.unit, self.seq, self.total/self.chunk_size, self.unit)  
        return _info  
  
    def refresh(self, count = 1, status = None):  
        self.count += count  
        self.status = status or self.status  
        end_str = ""\r""  
        if self.count >= self.total:  
            end_str = '\n'  
            self.status = status or self.fin_status  
        print(self.__get_info(), end=end_str, )  


if __name__ == '__main__':
	#url = 'http://www.demongan.com/source/game/二十四点.zip'
	#filename = '二十四点.zip'
	print('*' * 100)
	print('\t\t\t\t欢迎使用文件下载小助手')
	print('作者:Jack-Cui\n博客:http://blog.csdn.net/c406495762')
	print('*' * 100)
	url  = input('请输入需要下载的文件链接:\n')
	filename = url.split('/')[-1]
	with closing(requests.get(url, stream=True)) as response:  
		chunk_size = 1024  
		content_size = int(response.headers['content-length'])  
		if response.status_code == 200:
			print('文件大小:%0.2f KB' % (content_size / chunk_size))
			progress = ProgressBar(""%s下载进度"" % filename
			            , total = content_size  
			            , unit = ""KB""  
			            , chunk_size = chunk_size  
			            , run_status = ""正在下载""  
			            , fin_status = ""下载完成"")  

			with open(filename, ""wb"") as file:  
			        for data in response.iter_content(chunk_size=chunk_size):  
			            file.write(data)  
			            progress.refresh(count=len(data))  
		else:
			print('链接异常')",11.0,0,1,0.0,12,1.0,49.0,0,
A solution to project euler problem 3.py,"""""""
Problem:
The prime factors of 13195 are 5,7,13 and 29. What is the largest prime factor
of a given number N?

e.g. for 10, largest prime factor = 5. For 17, largest prime factor = 17.
""""""


# def solution(n: int) -> int:
def solution(n: int = 600851475143) -> int:
    """"""Returns the largest prime factor of a given number n.
    >>> solution(13195)
    29
    >>> solution(10)
    5
    >>> solution(17)
    17
    >>> solution(3.4)
    3
    >>> solution(0)
    Traceback (most recent call last):
        ...
    ValueError: Parameter n must be greater or equal to one.
    >>> solution(-17)
    Traceback (most recent call last):
        ...
    ValueError: Parameter n must be greater or equal to one.
    >>> solution([])
    Traceback (most recent call last):
        ...
    TypeError: Parameter n must be int or passive of cast to int.
    >>> solution(""asd"")
    Traceback (most recent call last):
        ...
    TypeError: Parameter n must be int or passive of cast to int.
    """"""
    try:
        n = int(n)
    except (TypeError, ValueError):
        raise TypeError(""Parameter n must be int or passive of cast to int."")
    if n <= 0:
        raise ValueError(""Parameter n must be greater or equal to one."")

    i = 2
    ans = 0

    if n == 2:
        return 2

    while n > 2:
        while n % i != 0:
            i += 1

        ans = i

        while n % i == 0:
            n = n / i

        i += 1

    return int(ans)


if __name__ == ""__main__"":
    # print(solution(int(input().strip())))
    import doctest

    doctest.testmod()
    print(solution(int(input().strip())))
",7.0,0,0,0.0,9,1.0,23.0,0,
build.py,"#!/usr/bin/env python3

import sys
from pathlib import Path

path = Path(__file__).resolve().parent / "".github"" / ""workflows"" / ""scripts""
sys.path.insert(0, str(path))

import ti_build.entry

sys.exit(ti_build.entry.main())
",0.0,0,0,0.0,0,1.0,6.0,0,
baiduwenku.py,"# -*- coding:UTF-8 -*-
from selenium import webdriver
from bs4 import BeautifulSoup
import re
import time

if __name__ == '__main__':

	options = webdriver.ChromeOptions()
	options.add_argument('user-agent=""Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19""')
	driver = webdriver.Chrome('J:\迅雷下载\chromedriver.exe', chrome_options=options)
	driver.get('https://wenku.baidu.com/view/aa31a84bcf84b9d528ea7a2c.html')

	html = driver.page_source
	bf1 = BeautifulSoup(html, 'lxml')
	result = bf1.find_all(class_='rtcspage')
	bf2 = BeautifulSoup(str(result[0]), 'lxml')
	title = bf2.div.div.h1.string
	pagenum = bf2.find_all(class_='size')
	pagenum = BeautifulSoup(str(pagenum), 'lxml').span.string
	pagepattern = re.compile('页数：(\d+)页')
	num = int(pagepattern.findall(pagenum)[0])
	print('文章标题：%s' % title)
	print('文章页数：%d' % num)


	while True:
		num = num / 5.0
		html = driver.page_source
		bf1 = BeautifulSoup(html, 'lxml')
		result = bf1.find_all(class_='rtcspage')
		for each_result in result:
			bf2 = BeautifulSoup(str(each_result), 'lxml')
			texts = bf2.find_all('p')
			for each_text in texts:
				main_body = BeautifulSoup(str(each_text), 'lxml')
				for each in main_body.find_all(True):
					if each.name == 'span':
						print(each.string.replace('\xa0',''),end='')
					elif each.name == 'br':
						print('')
			print('\n')
		if num > 1:
			page = driver.find_elements_by_xpath(""//div[@class='page']"")
			driver.execute_script('arguments[0].scrollIntoView();', page[-1]) #拖动到可见的元素去
			nextpage = driver.find_element_by_xpath(""//a[@data-fun='next']"")
			nextpage.click()
			time.sleep(3)
		else:
			break",7.0,0,0,0.0,26,1.0,44.0,0,
test.py,"#!./kitty/launcher/kitty +launch
# License: GPL v3 Copyright: 2016, Kovid Goyal <kovid at kovidgoyal.net>

import importlib


def main() -> None:
    m = importlib.import_module('kitty_tests.main')
    getattr(m, 'main')()


if __name__ == '__main__':
    main()
",2.0,0,0,0.0,1,1.0,6.0,0,
pip_deploy.py,"import subprocess



subprocess.call('python setup.py sdist')

subprocess.call('python setup.py bdist_wheel --universal')

subprocess.call('twine upload dist/*')

",0.0,0,0,0.0,0,1.0,4.0,0,
generate_changelog.py,"#!/usr/bin/env python3

import os
import subprocess
import sys

github_api_token = (
    os.getenv(""CHANGELOG_GITHUB_TOKEN"") if os.getenv(""CHANGELOG_GITHUB_TOKEN"") else input(""Enter Github API token: "")
)

if len(sys.argv) < 2:
    raise Exception(""Provide a version number as parameter (--future-release argument)"")

version = sys.argv[1]

cmd = [
    ""github_changelog_generator"",
    ""-t"",
    github_api_token,
    ""-u"",
    ""locustio"",
    ""-p"",
    ""locust"",
    ""--exclude-labels"",
    ""duplicate,question,invalid,wontfix,cantfix,stale,no-changelog"",
    ""--header-label"",
    ""# Detailed changelog\nThe most important changes can also be found in [the documentation](https://docs.locust.io/en/latest/changelog.html)."",
    ""--since-tag"",
    ""2.27.0"",
    # ""--since-commit"", # these cause issues
    # ""2020-07-01 00:00:00"",
    ""--future-release"",
    version,
]

print(f""Running command: {' '.join(cmd)}\n"")
subprocess.run(cmd)
",2.0,0,2,0.0,2,1.0,28.0,0,
pavement.py,"r""""""
This paver file is intended to help with the release process as much as
possible. It relies on virtualenv to generate 'bootstrap' environments as
independent from the user system as possible (e.g. to make sure the sphinx doc
is built against the built numpy, not an installed one).

Building changelog + notes
==========================

Assumes you have git and the binaries/tarballs in installers/::

    paver write_release
    paver write_note

This automatically put the checksum into README.rst, and writes the Changelog.

TODO
====
    - the script is messy, lots of global variables
    - make it more easily customizable (through command line args)
    - missing targets: install & test, sdist test, debian packaging
    - fix bdist_mpkg: we build the same source twice -> how to make sure we use
      the same underlying python for egg install in venv and for bdist_mpkg
""""""
import os
import hashlib
import textwrap

# The paver package needs to be installed to run tasks
import paver
from paver.easy import Bunch, options, task, sh


#-----------------------------------
# Things to be changed for a release
#-----------------------------------

# Path to the release notes
RELEASE_NOTES = 'doc/source/release/2.3.0-notes.rst'


#-------------------------------------------------------
# Hardcoded build/install dirs, virtualenv options, etc.
#-------------------------------------------------------

# Where to put the release installers
options(installers=Bunch(releasedir=""release"",
                         installersdir=os.path.join(""release"", ""installers"")),)


#-------------
# README stuff
#-------------

def _compute_hash(idirs, hashfunc):
    """"""Hash files using given hashfunc.

    Parameters
    ----------
    idirs : directory path
        Directory containing files to be hashed.
    hashfunc : hash function
        Function to be used to hash the files.

    """"""
    released = paver.path.path(idirs).listdir()
    checksums = []
    for fpath in sorted(released):
        with open(fpath, 'rb') as fin:
            fhash = hashfunc(fin.read())
            checksums.append(
                '%s  %s' % (fhash.hexdigest(), os.path.basename(fpath)))
    return checksums


def compute_md5(idirs):
    """"""Compute md5 hash of files in idirs.

    Parameters
    ----------
    idirs : directory path
        Directory containing files to be hashed.

    """"""
    return _compute_hash(idirs, hashlib.md5)


def compute_sha256(idirs):
    """"""Compute sha256 hash of files in idirs.

    Parameters
    ----------
    idirs : directory path
        Directory containing files to be hashed.

    """"""
    # better checksum so gpg signed README.rst containing the sums can be used
    # to verify the binaries instead of signing all binaries
    return _compute_hash(idirs, hashlib.sha256)


def write_release_task(options, filename='README'):
    """"""Append hashes of release files to release notes.

    This appends file hashes to the release notes and creates
    four README files of the result in various formats:

    - README.rst
    - README.rst.gpg
    - README.md
    - README.md.gpg

    The md file are created using `pandoc` so that the links are
    properly updated. The gpg files are kept separate, so that
    the unsigned files may be edited before signing if needed.

    Parameters
    ----------
    options :
        Set by ``task`` decorator.
    filename : str
        Filename of the modified notes. The file is written
        in the release directory.

    """"""
    idirs = options.installers.installersdir
    notes = paver.path.path(RELEASE_NOTES)
    rst_readme = paver.path.path(filename + '.rst')
    md_readme = paver.path.path(filename + '.md')

    # append hashes
    with open(rst_readme, 'w') as freadme:
        with open(notes) as fnotes:
            freadme.write(fnotes.read())

        freadme.writelines(textwrap.dedent(
            """"""
            Checksums
            =========

            MD5
            ---
            ::

            """"""))
        freadme.writelines([f'    {c}\n' for c in compute_md5(idirs)])

        freadme.writelines(textwrap.dedent(
            """"""
            SHA256
            ------
            ::

            """"""))
        freadme.writelines([f'    {c}\n' for c in compute_sha256(idirs)])

    # generate md file using pandoc before signing
    sh(f""pandoc -s -o {md_readme} {rst_readme}"")

    # Sign files
    if hasattr(options, 'gpg_key'):
        cmd = f'gpg --clearsign --armor --default_key {options.gpg_key}'
    else:
        cmd = 'gpg --clearsign --armor'

    sh(cmd + f' --output {rst_readme}.gpg {rst_readme}')
    sh(cmd + f' --output {md_readme}.gpg {md_readme}')


@task
def write_release(options):
    """"""Write the README files.

    Two README files are generated from the release notes, one in ``rst``
    markup for the general release, the other in ``md`` markup for the github
    release notes.

    Parameters
    ----------
    options :
        Set by ``task`` decorator.

    """"""
    rdir = options.installers.releasedir
    write_release_task(options, os.path.join(rdir, 'README'))
",7.0,0,0,0.0,3,1.0,59.0,0,
noxfile.py,"""""""
The file for Nox
""""""
from typing import Any

import nox
from nox.sessions import Session

locations = ""ciphey/"", ""tests/"", ""docs/""
nox.options.sessions = [""tests""]
package = ""ciphey""


def install_with_constraints(session: Session, *args: str, **kwargs: Any) -> None:
    """"""Install packages constrained by Poetry's lock file.
    This function is a wrapper for nox.sessions.Session.install. It
    invokes pip to install packages inside of the session's virtualenv.
    Additionally, pip is passed a constraints file generated from
    Poetry's lock file, to ensure that the packages are pinned to the
    versions specified in poetry.lock. This allows you to manage the
    packages as Poetry development dependencies.
    Arguments:
        session: The Session object.
        args: Command-line arguments for pip.
        kwargs: Additional keyword arguments for Session.install.
    """"""
    session.run(
        ""poetry"",
        ""export"",
        ""--dev"",
        ""--format=requirements.txt"",
        ""--output=requirements.txt"",
        external=True,
    )
    session.install(""--constraint=requirements.txt"", *args, **kwargs)


# noxfile.py
@nox.session
def black(session):
    args = session.posargs or locations
    session.install(""black"")
    session.run(""black"", *args)


@nox.session(python=""3.8"")
def coverage(session: Session) -> None:
    """"""Upload coverage data.""""""
    install_with_constraints(session, ""coverage[toml]"", ""codecov"")
    session.run(""pip3"", ""install"", ""cipheydists"")
    session.run(""coverage"", ""xml"", ""--fail-under=0"")
    session.run(""codecov"", *session.posargs)


# noxfile.py
@nox.session
def docs(session: Session) -> None:
    """"""Build the documentation.""""""
    install_with_constraints(session, ""sphinx"")
    session.run(""sphinx-build"", ""docs"", ""docs/_build"")


@nox.session
def tests(session):
    session.run(""pip3"", ""install"", ""cipheydists"")
    session.run(""poetry"", ""install"", external=True)
    session.run(""poetry"", ""run"", ""pytest"", ""--cov=ciphey"")
",6.0,0,0,0.0,1,1.0,36.0,0,
Battery_notifier.py,"from plyer import notification  # pip install plyer
import psutil  # pip install psutil

# psutil.sensors_battery() will return the information related to battery
battery = psutil.sensors_battery()

# battery percent will return the current battery prcentage
percent = battery.percent
charging = (
    battery.power_plugged
)

# Notification(title, description, duration)--to send
# notification to desktop
# help(Notification)
if charging:
    if percent == 100:
        charging_message = ""Unplug your Charger""
    else:
        charging_message = ""Charging""
else:
    charging_message = ""Not Charging""
message = str(percent) + ""% Charged\n"" + charging_message

notification.notify(""Battery Information"", message, timeout=10)
",2.0,0,0,0.0,5,1.0,16.0,0,
douban_top_250_books.py,"import requests
from bs4 import BeautifulSoup
import xlwt


def request_douban(url):
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/88.0.4324.146 Safari/537.36',
    }

    try:
        response = requests.get(url=url, headers=headers)
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None


book = xlwt.Workbook(encoding='utf-8', style_compression=0)

sheet = book.add_sheet('豆瓣电影Top250', cell_overwrite_ok=True)
sheet.write(0, 0, '名称')
sheet.write(0, 1, '图片')
sheet.write(0, 2, '排名')
sheet.write(0, 3, '评分')
sheet.write(0, 4, '作者')
sheet.write(0, 5, '简介')

n = 1


def save_to_excel(soup):
    list = soup.find(class_='grid_view').find_all('li')

    for item in list:
        item_name = item.find(class_='title').string
        item_img = item.find('a').find('img').get('src')
        item_index = item.find(class_='').string
        item_score = item.find(class_='rating_num').string
        item_author = item.find('p').text
        if item.find(class_='inq') is not None:
            item_intr = item.find(class_='inq').string
        else:
            item_intr = 'NOT AVAILABLE'        

        # print('爬取电影：' + item_index + ' | ' + item_name +' | ' + item_img +' | ' + item_score +' | ' + item_author +' | ' + item_intr )
        print('爬取电影：' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)

        global n

        sheet.write(n, 0, item_name)
        sheet.write(n, 1, item_img)
        sheet.write(n, 2, item_index)
        sheet.write(n, 3, item_score)
        sheet.write(n, 4, item_author)
        sheet.write(n, 5, item_intr)

        n = n + 1


def main(page):
    url = 'https://movie.douban.com/top250?start=' + str(page * 25) + '&filter='
    html = request_douban(url)
    soup = BeautifulSoup(html, 'lxml')
    save_to_excel(soup)


if __name__ == '__main__':

    for i in range(0, 10):
        main(i)

book.save(u'豆瓣最受欢迎的250部电影.xlsx')
",,0,0,0.0,0,1.0,,0,
conftest.py,"import os
import shutil
import warnings

import django


def pytest_addoption(parser):
    parser.addoption(
        ""--deprecation"",
        choices=[""all"", ""pending"", ""imminent"", ""none""],
        default=""pending"",
    )
    parser.addoption(""--postgres"", action=""store_true"")
    parser.addoption(""--elasticsearch"", action=""store_true"")


def pytest_configure(config):
    deprecation = config.getoption(""deprecation"")

    only_wagtail = r""^wagtail(\.|$)""
    if deprecation == ""all"":
        # Show all deprecation warnings from all packages
        warnings.simplefilter(""default"", DeprecationWarning)
        warnings.simplefilter(""default"", PendingDeprecationWarning)
    elif deprecation == ""pending"":
        # Show all deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
        warnings.filterwarnings(
            ""default"", category=PendingDeprecationWarning, module=only_wagtail
        )
    elif deprecation == ""imminent"":
        # Show only imminent deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
    elif deprecation == ""none"":
        # Deprecation warnings are ignored by default
        pass

    if config.getoption(""postgres""):
        os.environ[""DATABASE_ENGINE""] = ""django.db.backends.postgresql""

    # Setup django after processing the pytest arguments so that the env
    # variables are available in the settings
    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""wagtail.test.settings"")
    django.setup()

    # Activate a language: This affects HTTP header HTTP_ACCEPT_LANGUAGE sent by
    # the Django test client.
    from django.utils import translation

    translation.activate(""en"")

    from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT

    shutil.rmtree(STATIC_ROOT, ignore_errors=True)
    shutil.rmtree(MEDIA_ROOT, ignore_errors=True)


def pytest_unconfigure(config):
    from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT

    shutil.rmtree(STATIC_ROOT, ignore_errors=True)
    shutil.rmtree(MEDIA_ROOT, ignore_errors=True)
",5.0,0,0,0.0,5,1.0,44.0,0,
ARKA.py,"def sumOfSeries(n):
    x = n * (n + 1) / 2
    return (int)(x * x)


# Driver Function
n = 5
print(sumOfSeries(n))
",1.0,0,1,0.0,0,1.0,5.0,0,
setting.py,"# -*- coding: utf-8 -*-
""""""
-------------------------------------------------
   File Name：     setting.py
   Description :   配置文件
   Author :        JHao
   date：          2019/2/15
-------------------------------------------------
   Change Activity:
                   2019/2/15:
-------------------------------------------------
""""""

BANNER = r""""""
****************************************************************
*** ______  ********************* ______ *********** _  ********
*** | ___ \_ ******************** | ___ \ ********* | | ********
*** | |_/ / \__ __   __  _ __   _ | |_/ /___ * ___  | | ********
*** |  __/|  _// _ \ \ \/ /| | | ||  __// _ \ / _ \ | | ********
*** | |   | | | (_) | >  < \ |_| || |  | (_) | (_) || |___  ****
*** \_|   |_|  \___/ /_/\_\ \__  |\_|   \___/ \___/ \_____/ ****
****                       __ / /                          *****
************************* /___ / *******************************
*************************       ********************************
****************************************************************
""""""

VERSION = ""2.4.0""

# ############### server config ###############
HOST = ""0.0.0.0""

PORT = 5010

# ############### database config ###################
# db connection uri
# example:
#      Redis: redis://:password@ip:port/db
#      Ssdb:  ssdb://:password@ip:port
DB_CONN = 'redis://:pwd@127.0.0.1:6379/0'

# proxy table name
TABLE_NAME = 'use_proxy'


# ###### config the proxy fetch function ######
PROXY_FETCHER = [
    ""freeProxy01"",
    ""freeProxy02"",
    ""freeProxy03"",
    ""freeProxy04"",
    ""freeProxy05"",
    ""freeProxy06"",
    ""freeProxy07"",
    ""freeProxy08"",
    ""freeProxy09"",
    ""freeProxy10"",
    ""freeProxy11""
]

# ############# proxy validator #################
# 代理验证目标网站
HTTP_URL = ""http://httpbin.org""

HTTPS_URL = ""https://www.qq.com""

# 代理验证时超时时间
VERIFY_TIMEOUT = 10

# 近PROXY_CHECK_COUNT次校验中允许的最大失败次数,超过则剔除代理
MAX_FAIL_COUNT = 0

# 近PROXY_CHECK_COUNT次校验中允许的最大失败率,超过则剔除代理
# MAX_FAIL_RATE = 0.1

# proxyCheck时代理数量少于POOL_SIZE_MIN触发抓取
POOL_SIZE_MIN = 20

# ############# proxy attributes #################
# 是否启用代理地域属性
PROXY_REGION = True

# ############# scheduler config #################

# Set the timezone for the scheduler forcely (optional)
# If it is running on a VM, and
#   ""ValueError: Timezone offset does not match system offset""
#   was raised during scheduling.
# Please uncomment the following line and set a timezone for the scheduler.
# Otherwise it will detect the timezone from the system automatically.

TIMEZONE = ""Asia/Shanghai""
",0.0,0,1,0.0,0,1.0,38.0,0,
ASCIIvaluecharacter.py,"# Program to find the ASCII value of the given character

c = ""p""
print(""The ASCII value of '"" + c + ""' is"", ord(c))
",0.0,0,0,0.0,0,1.0,2.0,0,
meizitu.py,"# encoding = utf-8
import concurrent
import os
from concurrent.futures import ThreadPoolExecutor
import requests
from bs4 import BeautifulSoup


def header(referer):

    headers = {
        'Host': 'i.meizitu.net',
        'Pragma': 'no-cache',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        'Referer': '{}'.format(referer),
    }

    return headers


def request_page(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None


def get_page_urls():

    for i in range(1, 2):
        baseurl = 'https://www.mzitu.com/page/{}'.format(i)
        html = request_page(baseurl)
        soup = BeautifulSoup(html, 'lxml')
        elements = soup.find(class_='postlist').find_all('li')
        urls = []
        for item in elements:
            url = item.find('span').find('a').get('href')
            print('页面链接：%s' % url)
            urls.append(url)

    return urls


def download_Pic(title, image_list):
    # 新建文件夹
    os.mkdir(title)
    j = 1
    # 下载图片
    for item in image_list:
        filename = '%s/%s.jpg' % (title, str(j))
        print('downloading....%s : NO.%s' % (title, str(j)))
        with open(filename, 'wb') as f:
            img = requests.get(item, headers=header(item)).content
            f.write(img)
        j += 1

def download(url):
    html = request_page(url)
    soup = BeautifulSoup(html, 'lxml')
    total = soup.find(class_='pagenavi').find_all('a')[-2].find('span').string
    title = soup.find('h2').string
    image_list = []

    for i in range(int(total)):
        html = request_page(url + '/%s' % (i + 1))
        soup = BeautifulSoup(html, 'lxml')
        img_url = soup.find('img').get('src')
        image_list.append(img_url)

    download_Pic(title, image_list)


def download_all_images(list_page_urls):
    # 获取每一个详情妹纸
    # works = len(list_page_urls)
    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as exector:
        for url in list_page_urls:
            exector.submit(download, url)


if __name__ == '__main__':
    # 获取每一页的链接和名称
    list_page_urls = get_page_urls()
    download_all_images(list_page_urls)",13.0,0,3,0.0,9,1.0,66.0,0,
version.py,"import os

ZULIP_VERSION = ""10.0-dev+git""

# Add information on number of commits and commit hash to version, if available
zulip_git_version_file = os.path.join(
    os.path.dirname(os.path.abspath(__file__)), ""zulip-git-version""
)
lines = [ZULIP_VERSION, """"]
if os.path.exists(zulip_git_version_file):
    with open(zulip_git_version_file) as f:
        lines = [*f, """", """"]
ZULIP_VERSION = lines.pop(0).strip()
ZULIP_MERGE_BASE = lines.pop(0).strip()

LATEST_MAJOR_VERSION = ""9.0""
LATEST_RELEASE_VERSION = ""9.4""
LATEST_RELEASE_ANNOUNCEMENT = ""https://blog.zulip.com/2023/12/15/zulip-8-0-released/""

# Versions of the desktop app below DESKTOP_MINIMUM_VERSION will be
# prevented from connecting to the Zulip server.  Versions above
# DESKTOP_MINIMUM_VERSION but below DESKTOP_WARNING_VERSION will have
# a banner at the top of the page asking the user to upgrade.
DESKTOP_MINIMUM_VERSION = ""5.4.3""
DESKTOP_WARNING_VERSION = ""5.9.3""

# Bump the API_FEATURE_LEVEL whenever an API change is made
# that clients might want to condition on.  If we forget at
# the time we make the change, then bump it later as soon
# as we notice; clients using API_FEATURE_LEVEL will just not
# use the new feature/API until the bump.
#
# Changes should be accompanied by documentation explaining what the
# new level means in api_docs/changelog.md, as well as ""**Changes**""
# entries in the endpoint's documentation in `zulip.yaml`.

API_FEATURE_LEVEL = 353  # Last bumped for Zoom server to server video chat option.

# Bump the minor PROVISION_VERSION to indicate that folks should provision
# only when going from an old version of the code to a newer version. Bump
# the major version to indicate that folks should provision in both
# directions.

# Typically,
# * adding a dependency only requires a minor version bump;
# * removing a dependency requires a major version bump;
# * upgrading a dependency requires a major version bump, unless the
#   upgraded dependency is backwards compatible with all of our
#   historical commits sharing the same major version, in which case a
#   minor version bump suffices.

PROVISION_VERSION = (312, 0)  # bumped 2024-02-14 to upgrade asgiref
",1.0,0,0,0.0,1,1.0,18.0,0,
cli.py,"""""""
Provides a command line interface for the GPTResearcher class.

Usage:

```shell
python cli.py ""<query>"" --report_type <report_type>
```

""""""
import asyncio
import argparse
from argparse import RawTextHelpFormatter
from uuid import uuid4
import os

from dotenv import load_dotenv

from gpt_researcher import GPTResearcher
from gpt_researcher.utils.enum import ReportType, Tone
from backend.report_type import DetailedReport

# =============================================================================
# CLI
# =============================================================================

cli = argparse.ArgumentParser(
    description=""Generate a research report."",
    # Enables the use of newlines in the help message
    formatter_class=RawTextHelpFormatter)

# =====================================
# Arg: Query
# =====================================

cli.add_argument(
    # Position 0 argument
    ""query"",
    type=str,
    help=""The query to conduct research on."")

# =====================================
# Arg: Report Type
# =====================================

choices = [report_type.value for report_type in ReportType]

report_type_descriptions = {
    ReportType.ResearchReport.value: ""Summary - Short and fast (~2 min)"",
    ReportType.DetailedReport.value: ""Detailed - In depth and longer (~5 min)"",
    ReportType.ResourceReport.value: """",
    ReportType.OutlineReport.value: """",
    ReportType.CustomReport.value: """",
    ReportType.SubtopicReport.value: """"
}

cli.add_argument(
    ""--report_type"",
    type=str,
    help=""The type of report to generate. Options:\n"" + ""\n"".join(
        f""  {choice}: {report_type_descriptions[choice]}"" for choice in choices
    ),
    # Deserialize ReportType as a List of strings:
    choices=choices,
    required=True)

# First, let's see what values are actually in the Tone enum
print([t.value for t in Tone])

cli.add_argument(
    ""--tone"",
    type=str,
    help=""The tone of the report (optional)."",
    choices=[""objective"", ""formal"", ""analytical"", ""persuasive"", ""informative"", 
            ""explanatory"", ""descriptive"", ""critical"", ""comparative"", ""speculative"", 
            ""reflective"", ""narrative"", ""humorous"", ""optimistic"", ""pessimistic""],
    default=""objective""
)

# =============================================================================
# Main
# =============================================================================


async def main(args):
    """""" 
    Conduct research on the given query, generate the report, and write
    it as a markdown file to the output directory.
    """"""
    if args.report_type == 'detailed_report':
        detailed_report = DetailedReport(
            query=args.query,
            report_type=""research_report"",
            report_source=""web_search"",
        )

        report = await detailed_report.run()
    else:
        # Convert the simple keyword to the full Tone enum value
        tone_map = {
            ""objective"": Tone.Objective,
            ""formal"": Tone.Formal,
            ""analytical"": Tone.Analytical,
            ""persuasive"": Tone.Persuasive,
            ""informative"": Tone.Informative,
            ""explanatory"": Tone.Explanatory,
            ""descriptive"": Tone.Descriptive,
            ""critical"": Tone.Critical,
            ""comparative"": Tone.Comparative,
            ""speculative"": Tone.Speculative,
            ""reflective"": Tone.Reflective,
            ""narrative"": Tone.Narrative,
            ""humorous"": Tone.Humorous,
            ""optimistic"": Tone.Optimistic,
            ""pessimistic"": Tone.Pessimistic
        }

        researcher = GPTResearcher(
            query=args.query,
            report_type=args.report_type,
            tone=tone_map[args.tone]
        )

        await researcher.conduct_research()

        report = await researcher.write_report()

    # Write the report to a file
    artifact_filepath = f""outputs/{uuid4()}.md""
    os.makedirs(""outputs"", exist_ok=True)
    with open(artifact_filepath, ""w"") as f:
        f.write(report)

    print(f""Report written to '{artifact_filepath}'"")

if __name__ == ""__main__"":
    load_dotenv()
    args = cli.parse_args()
    asyncio.run(main(args))
",3.0,0,0,0.0,3,1.0,85.0,0,
Chrome Dino Automater.py,"import pyautogui  # pip install pyautogui
from PIL import Image, ImageGrab  # pip install pillow

# from numpy import asarray
import time


def hit(key):
    pyautogui.press(key)
    return


def isCollide(data):

    # for cactus
    for i in range(329, 425):
        for j in range(550, 650):
            if data[i, j] < 100:
                hit(""up"")
                return

    # Draw the rectangle for birds
    # for i in range(310, 425):
    #     for j in range(390, 550):
    #         if data[i, j] < 100:
    #             hit(""down"")
    #             return

    # return


if __name__ == ""__main__"":
    print(""Hey.. Dino game about to start in 3 seconds"")
    time.sleep(2)
    # hit('up')

    while True:
        image = ImageGrab.grab().convert(""L"")
        data = image.load()
        isCollide(data)

        # print(aarray(image))

        # Draw the rectangle for cactus
        # for i in range(315, 425):
        #     for j in range(550, 650):
        #         data[i, j] = 0

        # # # # # Draw the rectangle for birds
        # for i in range(310, 425):
        #     for j in range(390, 550):
        #         data[i, j] = 171

        # image.show()
        # break
",7.0,0,3,0.0,9,1.0,19.0,0,
train.py,"#!/usr/bin/env python3 -u
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""
Legacy entry point. Use fairseq_cli/train.py or fairseq-train instead.
""""""

from fairseq_cli.train import cli_main


if __name__ == ""__main__"":
    cli_main()
",1.0,0,0,0.0,1,1.0,3.0,0,
Add_two_Linked_List.py,"class Node:

    def __init__(self, data):

        self.data = data

        self.next = None





class LinkedList:

    def __init__(self):

        self.head = None



    def insert_at_beginning(self, new_data):

        new_node = Node(new_data)

        if self.head is None:

            self.head = new_node

            return

        new_node.next = self.head

        self.head = new_node



    def add_two_no(self, first, second):

        prev = None

        temp = None

        carry = 0

        while first is not None or second is not None:

            first_data = 0 if first is None else first.data

            second_data = 0 if second is None else second.data

            Sum = carry + first_data + second_data

            carry = 1 if Sum >= 10 else 0

            Sum = Sum if Sum < 10 else Sum % 10

            temp = Node(Sum)

            if self.head is None:

                self.head = temp

            else:

                prev.next = temp

            prev = temp

            if first is not None:

                first = first.next

            if second is not None:

                second = second.next

        if carry > 0:

            temp.next = Node(carry)



    def __str__(self):

        temp = self.head

        while temp:

            print(temp.data, ""->"", end="" "")

            temp = temp.next

        return ""None""





if __name__ == ""__main__"":

    first = LinkedList()

    second = LinkedList()

    first.insert_at_beginning(6)

    first.insert_at_beginning(4)

    first.insert_at_beginning(9)



    second.insert_at_beginning(2)

    second.insert_at_beginning(2)



    print(""First Linked List: "")

    print(first)

    print(""Second Linked List: "")

    print(second)



    result = LinkedList()

    result.add_two_no(first.head, second.head)

    print(""Final Result: "")

    print(result)

",18.0,1,2,0.0,21,1.0,58.0,0,
encoder_preprocess.py,"from encoder.preprocess import preprocess_librispeech, preprocess_voxceleb1, preprocess_voxceleb2
from utils.argutils import print_args
from pathlib import Path
import argparse


if __name__ == ""__main__"":
    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
        pass

    parser = argparse.ArgumentParser(
        description=""Preprocesses audio files from datasets, encodes them as mel spectrograms and ""
                    ""writes them to the disk. This will allow you to train the encoder. The ""
                    ""datasets required are at least one of VoxCeleb1, VoxCeleb2 and LibriSpeech. ""
                    ""Ideally, you should have all three. You should extract them as they are ""
                    ""after having downloaded them and put them in a same directory, e.g.:\n""
                    ""-[datasets_root]\n""
                    ""  -LibriSpeech\n""
                    ""    -train-other-500\n""
                    ""  -VoxCeleb1\n""
                    ""    -wav\n""
                    ""    -vox1_meta.csv\n""
                    ""  -VoxCeleb2\n""
                    ""    -dev"",
        formatter_class=MyFormatter
    )
    parser.add_argument(""datasets_root"", type=Path, help=\
        ""Path to the directory containing your LibriSpeech/TTS and VoxCeleb datasets."")
    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\
        ""Path to the output directory that will contain the mel spectrograms. If left out, ""
        ""defaults to <datasets_root>/SV2TTS/encoder/"")
    parser.add_argument(""-d"", ""--datasets"", type=str,
                        default=""librispeech_other,voxceleb1,voxceleb2"", help=\
        ""Comma-separated list of the name of the datasets you want to preprocess. Only the train ""
        ""set of these datasets will be used. Possible names: librispeech_other, voxceleb1, ""
        ""voxceleb2."")
    parser.add_argument(""-s"", ""--skip_existing"", action=""store_true"", help=\
        ""Whether to skip existing output files with the same name. Useful if this script was ""
        ""interrupted."")
    parser.add_argument(""--no_trim"", action=""store_true"", help=\
        ""Preprocess audio without trimming silences (not recommended)."")
    args = parser.parse_args()

    # Verify webrtcvad is available
    if not args.no_trim:
        try:
            import webrtcvad
        except:
            raise ModuleNotFoundError(""Package 'webrtcvad' not found. This package enables ""
                ""noise removal and is recommended. Please install and try again. If installation fails, ""
                ""use --no_trim to disable this error message."")
    del args.no_trim

    # Process the arguments
    args.datasets = args.datasets.split("","")
    if not hasattr(args, ""out_dir""):
        args.out_dir = args.datasets_root.joinpath(""SV2TTS"", ""encoder"")
    assert args.datasets_root.exists()
    args.out_dir.mkdir(exist_ok=True, parents=True)

    # Preprocess the datasets
    print_args(args, parser)
    preprocess_func = {
        ""librispeech_other"": preprocess_librispeech,
        ""voxceleb1"": preprocess_voxceleb1,
        ""voxceleb2"": preprocess_voxceleb2,
    }
    args = vars(args)
    for dataset in args.pop(""datasets""):
        print(""Preprocessing %s"" % dataset)
        preprocess_func[dataset](**args)
",4.0,0,1,0.0,10,1.0,62.0,0,
optimized-code.py,"from transformers import AutoModelForCasualLM, AutoTokenizer
import torch
import pandas as pd

model_name = ""bigcode/starcoder""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCasualLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"")

df = pd.read_csv(""sonarcloud_metrics.csv"")

if ""optimized_code"" not in df.columns:
    df[""optimized_code""] = """"

def op_code(code_snippet):
    prompt = f""Optimize the following Python code for better readability and efficiency:\n\n{code_snippet}""
    inputs = tokenizer(prompt, return_tensors=""pt"").to(""cuda"")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length)",0.0,0,0,0.0,0,1.0,0.0,0,
daili.py,"# -*- coding:UTF-8 -*-
from bs4 import BeautifulSoup
from selenium import webdriver
import subprocess as sp
from lxml import etree
import requests
import random
import re

""""""
函数说明:获取IP代理
Parameters:
	page - 高匿代理页数,默认获取第一页
Returns:
	proxys_list - 代理列表
Modify:
	2017-05-27
""""""
def get_proxys(page = 1):
	#requests的Session可以自动保持cookie,不需要自己维护cookie内容
	S = requests.Session()
	#西祠代理高匿IP地址
	target_url = 'http://www.xicidaili.com/nn/%d' % page
	#完善的headers
	target_headers = {'Upgrade-Insecure-Requests':'1',
		'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
		'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
		'Referer':'http://www.xicidaili.com/nn/',
		'Accept-Encoding':'gzip, deflate, sdch',
		'Accept-Language':'zh-CN,zh;q=0.8',
	}
	#get请求
	target_response = S.get(url = target_url, headers = target_headers)
	#utf-8编码
	target_response.encoding = 'utf-8'
	#获取网页信息
	target_html = target_response.text
	#获取id为ip_list的table
	bf1_ip_list = BeautifulSoup(target_html, 'lxml')
	bf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id = 'ip_list')), 'lxml')
	ip_list_info = bf2_ip_list.table.contents
	#存储代理的列表
	proxys_list = []
	#爬取每个代理信息
	for index in range(len(ip_list_info)):
		if index % 2 == 1 and index != 1:
			dom = etree.HTML(str(ip_list_info[index]))
			ip = dom.xpath('//td[2]')
			port = dom.xpath('//td[3]')
			protocol = dom.xpath('//td[6]')
			proxys_list.append(protocol[0].text.lower() + '#' + ip[0].text + '#' + port[0].text)
	#返回代理列表
	return proxys_list

""""""
函数说明:检查代理IP的连通性
Parameters:
	ip - 代理的ip地址
	lose_time - 匹配丢包数
	waste_time - 匹配平均时间
Returns:
	average_time - 代理ip平均耗时
Modify:
	2017-05-27
""""""
def check_ip(ip, lose_time, waste_time):
	#命令 -n 要发送的回显请求数 -w 等待每次回复的超时时间(毫秒)
	cmd = ""ping -n 3 -w 3 %s""
	#执行命令
	p = sp.Popen(cmd % ip, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE, shell=True) 
	#获得返回结果并解码
	out = p.stdout.read().decode(""gbk"")
	#丢包数
	lose_time = lose_time.findall(out)
	#当匹配到丢失包信息失败,默认为三次请求全部丢包,丢包数lose赋值为3
	if len(lose_time) == 0:
		lose = 3
	else:
		lose = int(lose_time[0])
	#如果丢包数目大于2个,则认为连接超时,返回平均耗时1000ms
	if lose > 2:
		#返回False
		return 1000
	#如果丢包数目小于等于2个,获取平均耗时的时间
	else:
		#平均时间
		average = waste_time.findall(out)
		#当匹配耗时时间信息失败,默认三次请求严重超时,返回平均好使1000ms
		if len(average) == 0:
			return 1000
		else:
			#
			average_time = int(average[0])
			#返回平均耗时
			return average_time

""""""
函数说明:初始化正则表达式
Parameters:
	无
Returns:
	lose_time - 匹配丢包数
	waste_time - 匹配平均时间
Modify:
	2017-05-27
""""""
def initpattern():
	#匹配丢包数
	lose_time = re.compile(u""丢失 = (\d+)"", re.IGNORECASE)
	#匹配平均时间
	waste_time = re.compile(u""平均 = (\d+)ms"", re.IGNORECASE)
	return lose_time, waste_time

if __name__ == '__main__':
	#初始化正则表达式
	lose_time, waste_time = initpattern()
	#获取IP代理
	proxys_list = get_proxys(1)

	#如果平均时间超过200ms重新选取ip
	while True:
		#从100个IP中随机选取一个IP作为代理进行访问
		proxy = random.choice(proxys_list)
		split_proxy = proxy.split('#')
		#获取IP
		ip = split_proxy[1]
		#检查ip
		average_time = check_ip(ip, lose_time, waste_time)
		if average_time > 200:
			#去掉不能使用的IP
			proxys_list.remove(proxy)
			print(""ip连接超时, 重新获取中!"")
		if average_time < 200:
			break

	#去掉已经使用的IP
	proxys_list.remove(proxy)
	proxy_dict = {split_proxy[0]:split_proxy[1] + ':' + split_proxy[2]}
	print(""使用代理:"", proxy_dict)
",13.0,0,0,0.0,20,1.0,100.0,0,
benchmark.py,"#!./kitty/launcher/kitty +launch
# License: GPL v3 Copyright: 2016, Kovid Goyal <kovid at kovidgoyal.net>

import fcntl
import io
import os
import select
import signal
import struct
import sys
import termios
import time
from pty import CHILD, fork

from kitty.constants import kitten_exe
from kitty.fast_data_types import Screen, safe_pipe
from kitty.utils import read_screen_size


def run_parsing_benchmark(cell_width: int = 10, cell_height: int = 20, scrollback: int = 20000) -> None:
    isatty = sys.stdout.isatty()
    if isatty:
        sz = read_screen_size()
        columns, rows = sz.cols, sz.rows
    else:
        columns, rows = 80, 25
    child_pid, master_fd = fork()
    is_child = child_pid == CHILD
    argv = [kitten_exe(), '__benchmark__', '--with-scrollback']
    if is_child:
        while read_screen_size().width != columns * cell_width:
            time.sleep(0.01)
        signal.pthread_sigmask(signal.SIG_SETMASK, ())
        os.execvp(argv[0], argv)
    # os.set_blocking(master_fd, False)
    x_pixels = columns * cell_width
    y_pixels = rows * cell_height
    s = struct.pack('HHHH', rows, columns, x_pixels, y_pixels)
    fcntl.ioctl(master_fd, termios.TIOCSWINSZ, s)

    write_buf = b''
    r_pipe, w_pipe = safe_pipe(True)
    class ToChild:
        def write(self, x: bytes | str) -> None:
            nonlocal write_buf
            if isinstance(x, str):
                x = x.encode()
            write_buf += x
            os.write(w_pipe, b'1')

    screen = Screen(None, rows, columns, scrollback, cell_width, cell_height, 0, ToChild())

    def parse_bytes(data: bytes) -> None:
        data = memoryview(data)
        while data:
            dest = screen.test_create_write_buffer()
            s = screen.test_commit_write_buffer(data, dest)
            data = data[s:]
            screen.test_parse_written_data()


    while True:
        rd, wd, _ = select.select([master_fd, r_pipe], [master_fd] if write_buf else [], [])
        if r_pipe in rd:
            os.read(r_pipe, 256)
        if master_fd in rd:
            try:
                data = os.read(master_fd, io.DEFAULT_BUFFER_SIZE)
            except OSError:
                data = b''
            if not data:
                break
            parse_bytes(data)
        if master_fd in wd:
            n = os.write(master_fd, write_buf)
            write_buf = write_buf[n:]
    if isatty:
        lines: list[str] = []
        screen.linebuf.as_ansi(lines.append)
        sys.stdout.write(''.join(lines))
    else:
        sys.stdout.write(str(screen.linebuf))


def main() -> None:
    run_parsing_benchmark()


if __name__ == '__main__':
    main()
",17.0,0,1,0.0,26,1.0,74.0,0,
pip_build.py,"""""""Script to create (and optionally install) a `.whl` archive for Keras 3.

Usage:

1. Create a `.whl` file in `dist/`:

```
python3 pip_build.py
```

2. Also install the new package immediately after:

```
python3 pip_build.py --install
```
""""""

import argparse
import datetime
import glob
import os
import pathlib
import re
import shutil

# Needed because importing torch after TF causes the runtime to crash
import torch  # noqa: F401

package = ""keras""
build_directory = ""tmp_build_dir""
dist_directory = ""dist""
to_copy = [""pyproject.toml"", ""README.md""]


def export_version_string(version, is_nightly=False, rc_index=None):
    """"""Export Version and Package Name.""""""
    if is_nightly:
        date = datetime.datetime.now()
        version += f"".dev{date:%Y%m%d%H}""
        # Update `name = ""keras""` with ""keras-nightly""
        pyproj_pth = pathlib.Path(""pyproject.toml"")
        pyproj_str = pyproj_pth.read_text().replace(
            'name = ""keras""', 'name = ""keras-nightly""'
        )
        pyproj_pth.write_text(pyproj_str)
    elif rc_index is not None:
        version += ""rc"" + str(rc_index)

    # Make sure to export the __version__ string
    with open(os.path.join(package, ""src"", ""version.py"")) as f:
        init_contents = f.read()
    with open(os.path.join(package, ""src"", ""version.py""), ""w"") as f:
        init_contents = re.sub(
            ""\n__version__ = .*\n"",
            f'\n__version__ = ""{version}""\n',
            init_contents,
        )
        f.write(init_contents)


def ignore_files(_, filenames):
    return [f for f in filenames if f.endswith(""_test.py"")]


def copy_source_to_build_directory(root_path):
    # Copy sources (`keras/` directory and setup files) to build
    # directory
    os.chdir(root_path)
    os.mkdir(build_directory)
    shutil.copytree(
        package, os.path.join(build_directory, package), ignore=ignore_files
    )
    for fname in to_copy:
        shutil.copy(fname, os.path.join(f""{build_directory}"", fname))
    os.chdir(build_directory)


def build(root_path, is_nightly=False, rc_index=None):
    if os.path.exists(build_directory):
        raise ValueError(f""Directory already exists: {build_directory}"")

    try:
        copy_source_to_build_directory(root_path)
        move_tf_keras_directory()

        from keras.src.version import __version__  # noqa: E402

        export_version_string(__version__, is_nightly, rc_index)
        return build_and_save_output(root_path, __version__)
    finally:
        # Clean up: remove the build directory (no longer needed)
        shutil.rmtree(build_directory)


def move_tf_keras_directory():
    """"""Move `keras/api/_tf_keras` to `keras/_tf_keras`, update references.""""""
    shutil.move(os.path.join(package, ""api"", ""_tf_keras""), ""keras"")
    with open(os.path.join(package, ""api"", ""__init__.py"")) as f:
        contents = f.read()
        contents = contents.replace(""from keras.api import _tf_keras"", """")
    with open(os.path.join(package, ""api"", ""__init__.py""), ""w"") as f:
        f.write(contents)
    # Replace `keras.api._tf_keras` with `keras._tf_keras`.
    for root, _, fnames in os.walk(os.path.join(package, ""_tf_keras"")):
        for fname in fnames:
            if fname.endswith("".py""):
                tf_keras_fpath = os.path.join(root, fname)
                with open(tf_keras_fpath) as f:
                    contents = f.read()
                    contents = contents.replace(
                        ""keras.api._tf_keras"", ""keras._tf_keras""
                    )
                with open(tf_keras_fpath, ""w"") as f:
                    f.write(contents)


def build_and_save_output(root_path, __version__):
    # Build the package
    os.system(""python3 -m build"")

    # Save the dist files generated by the build process
    os.chdir(root_path)
    if not os.path.exists(dist_directory):
        os.mkdir(dist_directory)
    for fpath in glob.glob(
        os.path.join(build_directory, dist_directory, ""*.*"")
    ):
        shutil.copy(fpath, dist_directory)

    # Find the .whl file path
    whl_path = None
    for fname in os.listdir(dist_directory):
        if __version__ in fname and fname.endswith("".whl""):
            whl_path = os.path.abspath(os.path.join(dist_directory, fname))
    if whl_path:
        print(f""Build successful. Wheel file available at {whl_path}"")
    else:
        print(""Build failed."")
    return whl_path


def install_whl(whl_fpath):
    print(f""Installing wheel file: {whl_fpath}"")
    os.system(f""pip3 install {whl_fpath} --force-reinstall --no-dependencies"")


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--install"",
        action=""store_true"",
        help=""Whether to install the generated wheel file."",
    )
    parser.add_argument(
        ""--nightly"",
        action=""store_true"",
        help=""Whether to generate nightly wheel file."",
    )
    parser.add_argument(
        ""--rc"",
        type=int,
        help=""Specify `[0-9] when generating RC wheels."",
    )
    args = parser.parse_args()
    root_path = pathlib.Path(__file__).parent.resolve()
    whl_path = build(root_path, args.nightly, args.rc)
    if whl_path and args.install:
        install_whl(whl_path)
",23.0,0,0,0.0,22,1.0,115.0,0,
crawl_sourcecode.py,""""""" This script can be used to test the equivalence in parsing between
rustpython and cpython.

Usage example:

$ python crawl_sourcecode.py crawl_sourcecode.py > cpython.txt
$ cargo run crawl_sourcecode.py crawl_sourcecode.py > rustpython.txt
$ diff cpython.txt rustpython.txt
""""""


import ast
import sys
import symtable
import dis

filename = sys.argv[1]
print('Crawling file:', filename)


with open(filename, 'r') as f:
    source = f.read()

t = ast.parse(source)
print(t)

shift = 3
def print_node(node, indent=0):
    indents = ' ' * indent
    if isinstance(node, ast.AST):
        lineno = 'row={}'.format(node.lineno) if hasattr(node, 'lineno') else ''
        print(indents, ""NODE"", node.__class__.__name__, lineno)
        for field in node._fields:
            print(indents,'-', field)
            f = getattr(node, field)
            if isinstance(f, list):
                for f2 in f:
                    print_node(f2, indent=indent+shift)
            else:
                print_node(f, indent=indent+shift)
    else:
        print(indents, 'OBJ', node)

print_node(t)

# print(ast.dump(t))
flag_names = [
    'is_referenced',
    'is_assigned',
    'is_global',
    'is_local',
    'is_parameter',
    'is_free',
]

def print_table(table, indent=0):
    indents = ' ' * indent
    print(indents, 'table:', table.get_name())
    print(indents, ' ', 'name:', table.get_name())
    print(indents, ' ', 'type:', table.get_type())
    print(indents, ' ', 'line:', table.get_lineno())
    print(indents, ' ', 'identifiers:', table.get_identifiers())
    print(indents, ' ', 'Syms:')
    for sym in table.get_symbols():
        flags = []
        for flag_name in flag_names:
            func = getattr(sym, flag_name)
            if func():
                flags.append(flag_name)
        print(indents, '   sym:', sym.get_name(), 'flags:', ' '.join(flags))
    if table.has_children():
        print(indents, ' ', 'Child tables:')
        for child in table.get_children():
            print_table(child, indent=indent+shift)

table = symtable.symtable(source, 'a', 'exec')
print_table(table)

print()
print('======== dis.dis ========')
print()
co = compile(source, filename, 'exec')
dis.dis(co)
",12.0,0,0,0.0,23,1.0,61.0,0,
AREA OF TRIANGLE.py,"# Python Program to find the area of triangle
# calculates area of traingle in efficient way!!
a = 5
b = 6
c = 7

# Uncomment below to take inputs from the user
# a = float(input('Enter first side: '))
# b = float(input('Enter second side: '))
# c = float(input('Enter third side: '))

# calculate the semi-perimeter
s = (a + b + c) / 2

# calculate the area
area = (s * (s - a) * (s - b) * (s - c)) ** 0.5
print(""The area of the triangle is %0.2f"" % area)
",0.0,0,0,0.0,0,1.0,6.0,0,
hero.py,"#-*- coding: UTF-8 -*-
from urllib.request import urlretrieve
import requests
import os

""""""
函数说明:下载《英雄联盟盒子》中的英雄图片

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_imgs_download(url, header):
    req = requests.get(url = url, headers = header).json()
    hero_num = len(req['list'])
    print('一共有%d个英雄' % hero_num)
    hero_images_path = 'hero_images'
    for each_hero in req['list']:
        hero_photo_url = each_hero['cover']
        hero_name = each_hero['name'] + '.jpg'
        filename = hero_images_path + '/' + hero_name
        if hero_images_path not in os.listdir():
            os.makedirs(hero_images_path)
        urlretrieve(url = hero_photo_url, filename = filename)

""""""
函数说明:打印所有英雄的名字和ID

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
Returns:
    无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_list(url, header):
	print('*' * 100)
	print('\t\t\t\t欢迎使用《王者荣耀》出装下助手！')
	print('*' * 100)
	req = requests.get(url = url, headers = header).json()
	flag = 0
	for each_hero in req['list']:
		flag += 1
		print('%s的ID为:%-7s' % (each_hero['name'], each_hero['hero_id']), end = '\t\t')
		if flag == 3:
			print('\n', end = '')
			flag = 0

""""""
函数说明:根据equip_id查询武器名字和价格

Parameters:
    equip_id - 武器的ID
    weapon_info - 存储所有武器的字典
Returns:
    weapon_name - 武器的名字
    weapon_price - 武器的价格
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def seek_weapon(equip_id, weapon_info):
	for each_weapon in weapon_info:
		if each_weapon['equip_id'] == str(equip_id):
			weapon_name = each_weapon['name']
			weapon_price = each_weapon['price']
			return weapon_name, weapon_price


""""""
函数说明:获取并打印出装信息

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
    weapon_info - 存储所有武器的字典
Returns:
	无
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_info(url, header, weapon_info):
	req = requests.get(url = url, headers = header).json()
	print('\n历史上的%s:\n    %s' % (req['info']['name'], req['info']['history_intro']))
	for each_equip_choice in req['info']['equip_choice']:
		print('\n%s:\n   %s' % (each_equip_choice['title'], each_equip_choice['description']))
		total_price = 0
		flag = 0
		for each_weapon in each_equip_choice['list']:
			flag += 1
			weapon_name, weapon_price = seek_weapon(each_weapon['equip_id'], weapon_info)
			print('%s:%s' % (weapon_name, weapon_price), end = '\t')
			if flag == 3:
				print('\n', end = '')
				flag = 0
			total_price += int(weapon_price)
		print('神装套件价格共计:%d' % total_price)


""""""
函数说明:获取武器信息

Parameters:
    url - GET请求地址，通过Fiddler抓包获取
    header - headers信息
Returns:
    weapon_info_dict - 武器信息
Author:
    Jack Cui
Blog:
    http://blog.csdn.net/c406495762
Modify:
    2017-08-07
""""""
def hero_weapon(url, header):
    req = requests.get(url = url, headers = header).json()
    weapon_info_dict = req['list']
    return weapon_info_dict


if __name__ == '__main__':
    headers = {'Accept-Charset': 'UTF-8',
            'Accept-Encoding': 'gzip,deflate',
            'User-Agent': 'Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI 5 MIUI/V8.1.6.0.MAACNDI)',
            'X-Requested-With': 'XMLHttpRequest',
            'Content-type': 'application/x-www-form-urlencoded',
            'Connection': 'Keep-Alive',
            'Host': 'gamehelper.gm825.com'}
    weapon_url = ""http://gamehelper.gm825.com/wzry/equip/list?channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8""
    heros_url = ""http://gamehelper.gm825.com/wzry/hero/list?channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8""
    hero_list(heros_url, headers)
    hero_id = input(""请输入要查询的英雄ID:"")
    hero_url = ""http://gamehelper.gm825.com/wzry/hero/detail?hero_id={}&channel_id=90009a&app_id=h9044j&game_id=7622&game_name=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&vcode=12.0.3&version_code=1203&cuid=2654CC14D2D3894DBF5808264AE2DAD7&ovr=6.0.1&device=Xiaomi_MI+5&net_type=1&client_id=1Yfyt44QSqu7PcVdDduBYQ%3D%3D&info_ms=fBzJ%2BCu4ZDAtl4CyHuZ%2FJQ%3D%3D&info_ma=XshbgIgi0V1HxXTqixI%2BKbgXtNtOP0%2Fn1WZtMWRWj5o%3D&mno=0&info_la=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&info_ci=9AChHTMC3uW%2BfY8%2BCFhcFw%3D%3D&mcc=0&clientversion=&bssid=VY%2BeiuZRJ%2FwaXmoLLVUrMODX1ZTf%2F2dzsWn2AOEM0I4%3D&os_level=23&os_id=dc451556fc0eeadb&resolution=1080_1920&dpi=480&client_ip=192.168.0.198&pdunid=a83d20d8"".format(hero_id)
    weapon_info_dict = hero_weapon(weapon_url, headers)
    hero_info(hero_url, headers, weapon_info_dict)",15.0,0,0,0.0,16,1.0,145.0,0,
changelog.py,"from git_changelog.cli import build_and_render

# 运行这段脚本自动生成CHANGELOG.md文件

build_and_render(
    repository=""."",
    output=""CHANGELOG.md"",
    convention=""angular"",
    provider=""github"",
    template=""keepachangelog"",
    parse_trailers=True,
    parse_refs=False,
    sections=[""build"", ""deps"", ""feat"", ""fix"", ""refactor""],
    versioning=""pep440"",
    bump=""1.1.2"",  # 指定bump版本
    in_place=True,
)
",0.0,0,0,0.0,0,1.0,14.0,0,
Binary_to_Decimal.py,"# Program to convert binary to decimal


def binaryToDecimal(binary):
    """"""
    >>> binaryToDecimal(111110000)
    496
    >>> binaryToDecimal(10100)
    20
    >>> binaryToDecimal(101011)
    43
    """"""
    decimal, i, n = 0, 0, 0
    while binary != 0:
        dec = binary % 10
        decimal = decimal + dec * pow(2, i)
        binary = binary // 10
        i += 1
    print(decimal)


binaryToDecimal(100)
",2.0,0,2,0.0,1,1.0,9.0,0,
runtests.py,"#!/usr/bin/env python

import argparse
import os
import shutil
import sys
import warnings

from django.core.management import execute_from_command_line

os.environ[""DJANGO_SETTINGS_MODULE""] = ""wagtail.test.settings""


def make_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--deprecation"",
        choices=[""all"", ""pending"", ""imminent"", ""none""],
        default=""imminent"",
    )
    parser.add_argument(""--postgres"", action=""store_true"")
    parser.add_argument(""--elasticsearch7"", action=""store_true"")
    parser.add_argument(""--elasticsearch8"", action=""store_true"")
    parser.add_argument(""--emailuser"", action=""store_true"")
    parser.add_argument(""--disabletimezone"", action=""store_true"")
    parser.add_argument(""--bench"", action=""store_true"")
    return parser


def parse_args(args=None):
    return make_parser().parse_known_args(args)


def runtests():
    args, rest = parse_args()

    only_wagtail = r""^wagtail(\.|$)""
    if args.deprecation == ""all"":
        # Show all deprecation warnings from all packages
        warnings.simplefilter(""default"", DeprecationWarning)
        warnings.simplefilter(""default"", PendingDeprecationWarning)
    elif args.deprecation == ""pending"":
        # Show all deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
        warnings.filterwarnings(
            ""default"", category=PendingDeprecationWarning, module=only_wagtail
        )
    elif args.deprecation == ""imminent"":
        # Show only imminent deprecation warnings from wagtail
        warnings.filterwarnings(
            ""default"", category=DeprecationWarning, module=only_wagtail
        )
    elif args.deprecation == ""none"":
        # Deprecation warnings are ignored by default
        pass

    if args.postgres:
        os.environ[""DATABASE_ENGINE""] = ""django.db.backends.postgresql""

    elif args.elasticsearch7:
        os.environ.setdefault(""ELASTICSEARCH_URL"", ""http://localhost:9200"")
        os.environ.setdefault(""ELASTICSEARCH_VERSION"", ""7"")
    elif args.elasticsearch8:
        os.environ.setdefault(""ELASTICSEARCH_URL"", ""http://localhost:9200"")
        os.environ.setdefault(""ELASTICSEARCH_VERSION"", ""8"")

    elif ""ELASTICSEARCH_URL"" in os.environ:
        # forcibly delete the ELASTICSEARCH_URL setting to skip those tests
        del os.environ[""ELASTICSEARCH_URL""]

    if args.emailuser:
        os.environ[""USE_EMAIL_USER_MODEL""] = ""1""

    if args.disabletimezone:
        os.environ[""DISABLE_TIMEZONE""] = ""1""

    if args.bench:
        benchmarks = [
            ""wagtail.admin.tests.benches"",
        ]

        argv = [sys.argv[0], ""test"", ""-v2""] + benchmarks + rest
    else:
        argv = [sys.argv[0], ""test""] + rest

    try:
        execute_from_command_line(argv)
    finally:
        from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT

        shutil.rmtree(STATIC_ROOT, ignore_errors=True)
        shutil.rmtree(MEDIA_ROOT, ignore_errors=True)


if __name__ == ""__main__"":
    runtests()
",9.0,0,0,0.0,13,1.0,71.0,0,
Calculator with simple ui.py,"# Program make a simple calculator


class Calculator:
    def __init__(self):
        pass

    def add(self, num1, num2):
        """"""
        This function adds two numbers.

        Examples:
        >>> add(2, 3)
        5
        >>> add(5, 9)
        14
        >>> add(-1, 2)
        1
        """"""
        return num1 + num2

    def subtract(self, num1, num2):
        """"""
        This function subtracts two numbers.

        Examples:
        >>> subtract(5, 3)
        2
        >>> subtract(9, 5)
        4
        >>> subtract(4, 9)
        -5
        """"""
        return num1 - num2

    def multiply(self, num1, num2):
        """"""
        This function multiplies two numbers.

        Examples:
        >>> multiply(4, 2)
        8
        >>> multiply(3, 3)
        9
        >>> multiply(9, 9)
        81
        """"""
        return num1 * num2

    def divide(self, num1, num2):
        """"""
        This function divides two numbers.

        Examples:
        >>> divide(4, 4)
        1
        >>> divide(6, 3)
        2
        >>> divide(9, 1)
        9
        """"""
        if num2 == 0:
            print(""Cannot divide by zero"")
        else:
            return num1 / num2


calculator = Calculator()


print(""1.Add"")
print(""2.Subtract"")
print(""3.Multiply"")
print(""4.Divide"")

while True:
    # Take input from the user
    choice = input(""Enter choice(1/2/3/4): "")

    # Check if choice is one of the four options
    if choice in (""1"", ""2"", ""3"", ""4""):
        num1 = float(input(""Enter first number: ""))
        num2 = float(input(""Enter second number: ""))

        if choice == ""1"":
            print(calculator.add(num1, num2))

        elif choice == ""2"":
            print(calculator.subtract(num1, num2))

        elif choice == ""3"":
            print(calculator.multiply(num1, num2))

        elif choice == ""4"":
            print(calculator.divide(num1, num2))
        break
    else:
        print(""Invalid Input"")
",9.0,0,1,0.0,12,1.0,35.0,0,
gunicorn.config.py,"#!/usr/bin/env python3

import logging
import os
import socket
import struct
import sys
import threading
import time

import structlog
from prometheus_client import CollectorRegistry, Gauge, multiprocess, start_http_server

loglevel = ""error""
keepalive = 120

# Set the timeout to something lower than any downstreams, such that if the
# timeout is hit, then the worker will be killed and respawned, which will then
# we able to pick up any connections that were previously pending on the socket
# and serve the requests before the downstream timeout.
timeout = 15

grateful_timeout = 120


METRICS_UPDATE_INTERVAL_SECONDS = int(os.getenv(""GUNICORN_METRICS_UPDATE_SECONDS"", 5))


def when_ready(server):
    """"""
    To ease being able to hide the /metrics endpoint when running in production,
    we serve the metrics on a separate port, using the
    prometheus_client.multiprocess Collector to pull in data from the worker
    processes.
    """"""
    registry = CollectorRegistry()
    multiprocess.MultiProcessCollector(registry)
    port = int(os.environ.get(""PROMETHEUS_METRICS_EXPORT_PORT"", 8001))
    start_http_server(port=port, registry=registry)

    # Start a thread in the Arbiter that will monitor the backlog on the sockets
    # Gunicorn is listening on.
    socket_monitor = SocketMonitor(server=server, registry=registry)
    socket_monitor.start()


def post_fork(server, worker):
    """"""
    Within each worker process, start a thread that will monitor the thread and
    connection pool.
    """"""
    worker_monitor = WorkerMonitor(worker=worker)
    worker_monitor.start()


def worker_exit(server, worker):
    """"""
    Ensure that we mark workers as dead with the prometheus_client such that
    any cleanup can happen.
    """"""
    multiprocess.mark_process_dead(worker.pid)


class SocketMonitor(threading.Thread):
    """"""
    We have enabled the statsd collector for Gunicorn, but this doesn't include
    the backlog due to concerns over portability, see
    https://github.com/benoitc/gunicorn/pull/2407

    Instead, we expose to Prometheus a gauge that will report the backlog size.

    We can then:

     1. use this to monitor how well the Gunicorn instances are keeping up with
        requests.
     2. use this metric to handle HPA scaling e.g. in Kubernetes

    """"""

    def __init__(self, server, registry):
        super().__init__()
        self.daemon = True
        self.server = server
        self.registry = registry

    def run(self):
        """"""
        Every X seconds, check to see how many connections are pending for each
        server socket.

        We label each individually, as limits such as `--backlog` will apply to
        each individually.
        """"""
        if sys.platform != ""linux"":
            # We use the assumption that we are on Linux to be able to get the
            # socket backlog, so if we're not on Linux, we return immediately.
            return

        backlog_gauge = Gauge(
            ""gunicorn_pending_connections"",
            ""The number of pending connections on all sockets. Linux only."",
            registry=self.registry,
            labelnames=[""listener""],
        )

        while True:
            for sock in self.server.LISTENERS:
                backlog = self.get_backlog(sock=sock)
                backlog_gauge.labels(listener=str(sock)).set(backlog)

            time.sleep(METRICS_UPDATE_INTERVAL_SECONDS)

    def get_backlog(self, sock):
        # tcp_info struct from include/uapi/linux/tcp.h
        fmt = ""B"" * 8 + ""I"" * 24
        tcp_info_struct = sock.getsockopt(socket.IPPROTO_TCP, socket.TCP_INFO, 104)
        # 12 is tcpi_unacked
        return struct.unpack(fmt, tcp_info_struct)[12]


class WorkerMonitor(threading.Thread):
    """"""
    There is a statsd logger support in Gunicorn that allows us to gather
    metrics e.g. on the number of workers, requests, request duration etc. See
    https://docs.gunicorn.org/en/stable/instrumentation.html for details.

    To get a better understanding of the pool utilization, number of accepted
    connections, we start a thread in head worker to report these via prometheus
    metrics.
    """"""

    def __init__(self, worker):
        super().__init__()
        self.daemon = True
        self.worker = worker

    def run(self):
        """"""
        Every X seconds, check the status of the Thread pool, as well as the
        """"""
        active_worker_connections = Gauge(
            ""gunicorn_active_worker_connections"",
            ""Number of active connections."",
            labelnames=[""pid""],
        )
        max_worker_connections = Gauge(
            ""gunicorn_max_worker_connections"",
            ""Maximum worker connections."",
            labelnames=[""pid""],
        )

        total_threads = Gauge(
            ""gunicorn_max_worker_threads"",
            ""Size of the thread pool per worker."",
            labelnames=[""pid""],
        )
        active_threads = Gauge(
            ""gunicorn_active_worker_threads"",
            ""Number of threads actively processing requests."",
            labelnames=[""pid""],
        )

        pending_requests = Gauge(
            ""gunicorn_pending_requests"",
            ""Number of requests that have been read from a connection but have not completed yet"",
            labelnames=[""pid""],
        )

        max_worker_connections.labels(pid=self.worker.pid).set(self.worker.cfg.worker_connections)
        total_threads.labels(pid=self.worker.pid).set(self.worker.cfg.threads)

        while True:
            active_worker_connections.labels(pid=self.worker.pid).set(self.worker.nr_conns)
            active_threads.labels(pid=self.worker.pid).set(min(self.worker.cfg.threads, len(self.worker.futures)))
            pending_requests.labels(pid=self.worker.pid).set(len(self.worker.futures))

            time.sleep(METRICS_UPDATE_INTERVAL_SECONDS)


LOGGING_FORMATTER_NAME = os.getenv(""LOGGING_FORMATTER_NAME"", ""default"")


# Setup stdlib logging to be handled by Structlog
def add_pid_and_tid(
    logger: logging.Logger, method_name: str, event_dict: structlog.types.EventDict
) -> structlog.types.EventDict:
    event_dict[""pid""] = os.getpid()
    event_dict[""tid""] = threading.get_ident()
    return event_dict


pre_chain = [
    # Add the log level and a timestamp to the event_dict if the log entry
    # is not from structlog.
    structlog.stdlib.add_log_level,
    structlog.stdlib.add_logger_name,
    add_pid_and_tid,
    structlog.processors.TimeStamper(fmt=""iso""),
]


# This is a copy the default logging config for gunicorn but with additions to:
#
#  1. non propagate loggers to the root handlers (otherwise we get duplicate log
#     lines)
#  2. use structlog for processing of log records
#
# See
# https://github.com/benoitc/gunicorn/blob/0b953b803786997d633d66c0f7c7b290df75e07c/gunicorn/glogging.py#L48
# for the default log settings.
logconfig_dict = {
    ""version"": 1,
    ""disable_existing_loggers"": True,
    ""formatters"": {
        ""default"": {
            ""()"": structlog.stdlib.ProcessorFormatter,
            ""processor"": structlog.dev.ConsoleRenderer(colors=True),
            ""foreign_pre_chain"": pre_chain,
        },
        ""json"": {
            ""()"": structlog.stdlib.ProcessorFormatter,
            ""processor"": structlog.processors.JSONRenderer(),
            ""foreign_pre_chain"": pre_chain,
        },
    },
    ""root"": {""level"": ""INFO"", ""handlers"": [""console""]},
    ""loggers"": {
        ""gunicorn.error"": {
            ""level"": ""INFO"",
            ""handlers"": [""error_console""],
            ""propagate"": False,
            ""qualname"": ""gunicorn.error"",
        },
        ""gunicorn.access"": {
            ""level"": ""INFO"",
            ""handlers"": [""console""],
            ""propagate"": False,
            ""qualname"": ""gunicorn.access"",
        },
    },
    ""handlers"": {
        ""error_console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": LOGGING_FORMATTER_NAME,
            ""stream"": ""ext://sys.stderr"",
        },
        ""console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": LOGGING_FORMATTER_NAME,
            ""stream"": ""ext://sys.stdout"",
        },
    },
}
",13.0,0,2,0.0,5,1.0,144.0,0,
baiduwenku_pro_1.py,"import requests
import re
import json
import os

session = requests.session()


def fetch_url(url):
    return session.get(url).content.decode('gbk')


def get_doc_id(url):
    return re.findall('view/(.*).html', url)[0]


def parse_type(content):
    return re.findall(r""docType.*?\:.*?\'(.*?)\'\,"", content)[0]


def parse_title(content):
    return re.findall(r""title.*?\:.*?\'(.*?)\'\,"", content)[0]


def parse_doc(content):
    result = ''
    url_list = re.findall('(https.*?0.json.*?)\\\\x22}', content)
    url_list = [addr.replace(""\\\\\\/"", ""/"") for addr in url_list]
    for url in url_list[:-5]:
        content = fetch_url(url)
        y = 0
        txtlists = re.findall('""c"":""(.*?)"".*?""y"":(.*?),', content)
        for item in txtlists:
            if not y == item[1]:
                y = item[1]
                n = '\n'
            else:
                n = ''
            result += n
            result += item[0].encode('utf-8').decode('unicode_escape', 'ignore')
    return result


def parse_txt(doc_id):
    content_url = 'https://wenku.baidu.com/api/doc/getdocinfo?callback=cb&doc_id=' + doc_id
    content = fetch_url(content_url)
    md5 = re.findall('""md5sum"":""(.*?)""', content)[0]
    pn = re.findall('""totalPageNum"":""(.*?)""', content)[0]
    rsign = re.findall('""rsign"":""(.*?)""', content)[0]
    content_url = 'https://wkretype.bdimg.com/retype/text/' + doc_id + '?rn=' + pn + '&type=txt' + md5 + '&rsign=' + rsign
    content = json.loads(fetch_url(content_url))
    result = ''
    for item in content:
        for i in item['parags']:
            result += i['c'].replace('\\r', '\r').replace('\\n', '\n')
    return result


def parse_other(doc_id):
    content_url = ""https://wenku.baidu.com/browse/getbcsurl?doc_id="" + doc_id + ""&pn=1&rn=99999&type=ppt""
    content = fetch_url(content_url)
    url_list = re.findall('{""zoom"":""(.*?)"",""page""', content)
    url_list = [item.replace(""\\"", '') for item in url_list]
    if not os.path.exists(doc_id):
        os.mkdir(doc_id)
    for index, url in enumerate(url_list):
        content = session.get(url).content
        path = os.path.join(doc_id, str(index) + '.jpg')
        with open(path, 'wb') as f:
            f.write(content)
    print(""图片保存在"" + doc_id + ""文件夹"")


def save_file(filename, content):
    with open(filename, 'w', encoding='utf8') as f:
        f.write(content)
        print('已保存为:' + filename)


# test_txt_url = 'https://wenku.baidu.com/view/cbb4af8b783e0912a3162a89.html?from=search'
# test_ppt_url = 'https://wenku.baidu.com/view/2b7046e3f78a6529657d5376.html?from=search'
# test_pdf_url = 'https://wenku.baidu.com/view/dd6e15c1227916888586d795.html?from=search'
# test_xls_url = 'https://wenku.baidu.com/view/eb4a5bb7312b3169a551a481.html?from=search'
def main():
    url = input('请输入要下载的文库URL地址')
    content = fetch_url(url)
    doc_id = get_doc_id(url)
    type = parse_type(content)
    title = parse_title(content)
    if type == 'doc':
        result = parse_doc(content)
        save_file(title + '.txt', result)
    elif type == 'txt':
        result = parse_txt(doc_id)
        save_file(title + '.txt', result)
    else:
        parse_other(doc_id)


if __name__ == ""__main__"":
    main()
",18.0,0,3,0.0,16,1.0,76.0,0,
Calculate resistance.py,"def res(R1, R2): 
       sum = R1 + R2 
       if option ==""series"": 
           return sum 
       elif option ==""parallel"" : 
           return (R1 * R2)/sum
       return 0
Resistance1 = int(input(""Enter R1 : "")) 
Resistance2 = int(input(""Enter R2 : "")) 
option = input(""Enter series or parallel :"") 
print(""\n"") 
R = res(Resistance1,Resistance2 ) 
if R==0:
    print('Wrong Input!!' )
else:
    print(""The total resistance is"", R)
",3.0,0,3,0.0,4,1.0,16.0,0,
