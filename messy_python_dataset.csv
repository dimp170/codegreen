repo_name,file_name,file_size,file_url,full_code
huggingface_transformers,huggingface_transformers_setup.py,16603,https://raw.githubusercontent.com/huggingface/transformers/main/setup.py,"# Copyright 2021 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Simple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/main/setup.py

To create the package for pypi.

1. Create the release branch named: v<RELEASE>-release, for example v4.19-release. For a patch release checkout the
   current release branch.

   If releasing on a special branch, copy the updated README.md on the main branch for the commit you will make
   for the post-release and run `make fix-copies` on the main branch as well.

2. Run `make pre-release` (or `make pre-patch` for a patch release) and commit these changes with the message:
   ""Release: <VERSION>"" and push.

3. Go back to the main branch and run `make post-release` then `make fix-copies`. Commit these changes with the
   message ""v<NEXT_VERSION>.dev.0"" and push to main.

# If you were just cutting the branch in preparation for a release, you can stop here for now.

4. Wait for the tests on the release branch to be completed and be green (otherwise revert and fix bugs)

5. On the release branch, add a tag in git to mark the release: ""git tag v<VERSION> -m 'Adds tag v<VERSION> for pypi' ""
   Push the tag to git: git push --tags origin v<RELEASE>-release

6. Build both the sources and the wheel. Do not change anything in setup.py between
   creating the wheel and the source distribution (obviously).

   Run `make build-release`. This will build the release and do some sanity checks for you. If this ends with an error
   message, you need to fix things before going further.

   You should now have a /dist directory with both .whl and .tar.gz source versions.

7. Check that everything looks correct by uploading the package to the pypi test server:

   twine upload dist/* -r testpypi
   (pypi suggest using twine as other methods upload files via plaintext.)
   You may have to specify the repository url, use the following command then:
   twine upload dist/* -r testpypi --repository-url=https://test.pypi.org/legacy/

   Check that you can install it in a virtualenv by running:
   pip install -i https://testpypi.python.org/pypi transformers

   Check you can run the following commands:
   python -c ""from transformers import pipeline; classifier = pipeline('text-classification'); print(classifier('What a nice release'))""
   python -c ""from transformers import *""
   python utils/check_build.py --check_lib

   If making a patch release, double check the bug you are patching is indeed resolved.

8. Upload the final version to actual pypi:
   twine upload dist/* -r pypi

9. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.
""""""

import os
import re
import shutil
from pathlib import Path

from setuptools import Command, find_packages, setup


# Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466
stale_egg_info = Path(__file__).parent / ""transformers.egg-info""
if stale_egg_info.exists():
    print(
        (
            ""Warning: {} exists.\n\n""
            ""If you recently updated transformers to 3.0 or later, this is expected,\n""
            ""but it may prevent transformers from installing in editable mode.\n\n""
            ""This directory is automatically generated by Python's packaging tools.\n""
            ""I will remove it now.\n\n""
            ""See https://github.com/pypa/pip/issues/5466 for details.\n""
        ).format(stale_egg_info)
    )
    shutil.rmtree(stale_egg_info)


# IMPORTANT:
# 1. all dependencies should be listed here with their version requirements if any
# 2. once modified, run: `make deps_table_update` to update src/transformers/dependency_versions_table.py
_deps = [
    ""Pillow>=10.0.1,<=15.0"",
    ""accelerate>=0.26.0"",
    ""av"",
    ""beautifulsoup4"",
    ""blobfile"",
    ""codecarbon>=2.8.1"",
    ""cookiecutter==1.7.3"",
    ""dataclasses"",
    ""datasets!=2.5.0"",
    ""deepspeed>=0.9.3"",
    ""diffusers"",
    ""dill<0.3.5"",
    ""evaluate>=0.2.0"",
    ""faiss-cpu"",
    ""fastapi"",
    ""filelock"",
    ""flax>=0.4.1,<=0.7.0"",
    ""fsspec<2023.10.0"",
    ""ftfy"",
    ""fugashi>=1.0"",
    ""GitPython<3.1.19"",
    ""hf-doc-builder>=0.3.0"",
    ""huggingface-hub>=0.26.0,<1.0"",
    ""importlib_metadata"",
    ""ipadic>=1.0.0,<2.0"",
    ""isort>=5.5.4"",
    ""jax>=0.4.1,<=0.4.13"",
    ""jaxlib>=0.4.1,<=0.4.13"",
    ""jieba"",
    ""jinja2>=3.1.0"",
    ""kenlm"",
    # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.
    ""keras>2.9,<2.16"",
    ""keras-nlp>=0.3.1,<0.14.0"",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.
    ""librosa"",
    ""nltk<=3.8.1"",
    ""natten>=0.14.6,<0.15.0"",
    ""numpy>=1.17"",
    ""onnxconverter-common"",
    ""onnxruntime-tools>=1.4.2"",
    ""onnxruntime>=1.4.0"",
    ""opencv-python"",
    ""optimum-benchmark>=0.3.0"",
    ""optuna"",
    ""optax>=0.0.8,<=0.1.4"",
    ""packaging>=20.0"",
    ""parameterized"",
    ""phonemizer"",
    ""protobuf"",
    ""psutil"",
    ""pyyaml>=5.1"",
    ""pydantic"",
    ""pytest>=7.2.0,<8.0.0"",
    ""pytest-asyncio"",
    ""pytest-timeout"",
    ""pytest-xdist"",
    ""python>=3.9.0"",
    ""ray[tune]>=2.7.0"",
    ""regex!=2019.12.17"",
    ""requests"",
    ""rhoknp>=1.1.0,<1.3.1"",
    ""rjieba"",
    ""rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1"",
    ""ruff==0.5.1"",
    ""sacrebleu>=1.4.12,<2.0.0"",
    ""sacremoses"",
    ""safetensors>=0.4.1"",
    ""sagemaker>=2.31.0"",
    ""schedulefree>=1.2.6"",
    ""scikit-learn"",
    ""scipy<1.13.0"",  # SciPy >= 1.13.0 is not supported with the current jax pin (`jax>=0.4.1,<=0.4.13`)
    ""sentencepiece>=0.1.91,!=0.1.92"",
    ""sigopt"",
    ""starlette"",
    ""sudachipy>=0.6.6"",
    ""sudachidict_core>=20220729"",
    ""tensorboard"",
    # TensorFlow pin. When changing this value, update examples/tensorflow/_tests_requirements.txt accordingly
    ""tensorflow-cpu>2.9,<2.16"",
    ""tensorflow>2.9,<2.16"",
    ""tensorflow-text<2.16"",
    ""tensorflow-probability<0.24"",
    ""tf2onnx"",
    ""timeout-decorator"",
    ""tiktoken"",
    ""timm<=1.0.11"",
    ""tokenizers>=0.21,<0.22"",
    ""torch>=2.0"",
    ""torchaudio"",
    ""torchvision"",
    ""pyctcdecode>=0.4.0"",
    ""tqdm>=4.27"",
    ""unidic>=1.0.2"",
    ""unidic_lite>=1.0.7"",
    ""urllib3<2.0.0"",
    ""uvicorn"",
    ""pytest-rich"",
    ""libcst"",
    ""rich"",
]


# this is a lookup table with items like:
#
# tokenizers: ""tokenizers==0.9.4""
# packaging: ""packaging""
#
# some of the values are versioned whereas others aren't.
deps = {b: a for a, b in (re.findall(r""^(([^!=<>~ ]+)(?:[!=<>~ ].*)?$)"", x)[0] for x in _deps)}

# since we save this data in src/transformers/dependency_versions_table.py it can be easily accessed from
# anywhere. If you need to quickly access the data from this table in a shell, you can do so easily with:
#
# python -c 'import sys; from transformers.dependency_versions_table import deps; \
# print("" "".join([ deps[x] for x in sys.argv[1:]]))' tokenizers datasets
#
# Just pass the desired package names to that script as it's shown with 2 packages above.
#
# If transformers is not yet installed and the work is done from the cloned repo remember to add `PYTHONPATH=src` to the script above
#
# You can then feed this for example to `pip`:
#
# pip install -U $(python -c 'import sys; from transformers.dependency_versions_table import deps; \
# print("" "".join([deps[x] for x in sys.argv[1:]]))' tokenizers datasets)
#


def deps_list(*pkgs):
    return [deps[pkg] for pkg in pkgs]


class DepsTableUpdateCommand(Command):
    """"""
    A custom distutils command that updates the dependency table.
    usage: python setup.py deps_table_update
    """"""

    description = ""build runtime dependency table""
    user_options = [
        # format: (long option, short option, description).
        (""dep-table-update"", None, ""updates src/transformers/dependency_versions_table.py""),
    ]

    def initialize_options(self):
        pass

    def finalize_options(self):
        pass

    def run(self):
        entries = ""\n"".join([f'    ""{k}"": ""{v}"",' for k, v in deps.items()])
        content = [
            ""# THIS FILE HAS BEEN AUTOGENERATED. To update:"",
            ""# 1. modify the `_deps` dict in setup.py"",
            ""# 2. run `make deps_table_update``"",
            ""deps = {"",
            entries,
            ""}"",
            """",
        ]
        target = ""src/transformers/dependency_versions_table.py""
        print(f""updating {target}"")
        with open(target, ""w"", encoding=""utf-8"", newline=""\n"") as f:
            f.write(""\n"".join(content))


extras = {}

extras[""ja""] = deps_list(""fugashi"", ""ipadic"", ""unidic_lite"", ""unidic"", ""sudachipy"", ""sudachidict_core"", ""rhoknp"")
extras[""sklearn""] = deps_list(""scikit-learn"")

extras[""tf""] = deps_list(""tensorflow"", ""onnxconverter-common"", ""tf2onnx"", ""tensorflow-text"", ""keras-nlp"")
extras[""tf-cpu""] = deps_list(
    ""keras"",
    ""tensorflow-cpu"",
    ""onnxconverter-common"",
    ""tf2onnx"",
    ""tensorflow-text"",
    ""keras-nlp"",
    ""tensorflow-probability"",
)

extras[""torch""] = deps_list(""torch"", ""accelerate"")
extras[""accelerate""] = deps_list(""accelerate"")

if os.name == ""nt"":  # windows
    extras[""retrieval""] = deps_list(""datasets"")  # faiss is not supported on windows
    extras[""flax""] = []  # jax is not supported on windows
else:
    extras[""retrieval""] = deps_list(""faiss-cpu"", ""datasets"")
    extras[""flax""] = deps_list(""jax"", ""jaxlib"", ""flax"", ""optax"", ""scipy"")

extras[""tokenizers""] = deps_list(""tokenizers"")
extras[""ftfy""] = deps_list(""ftfy"")
extras[""onnxruntime""] = deps_list(""onnxruntime"", ""onnxruntime-tools"")
extras[""onnx""] = deps_list(""onnxconverter-common"", ""tf2onnx"") + extras[""onnxruntime""]
extras[""modelcreation""] = deps_list(""cookiecutter"")

extras[""sagemaker""] = deps_list(""sagemaker"")
extras[""deepspeed""] = deps_list(""deepspeed"") + extras[""accelerate""]
extras[""optuna""] = deps_list(""optuna"")
extras[""ray""] = deps_list(""ray[tune]"")
extras[""sigopt""] = deps_list(""sigopt"")

extras[""integrations""] = extras[""optuna""] + extras[""ray""] + extras[""sigopt""]

extras[""serving""] = deps_list(""pydantic"", ""uvicorn"", ""fastapi"", ""starlette"")
extras[""audio""] = deps_list(""librosa"", ""pyctcdecode"", ""phonemizer"", ""kenlm"")
# `pip install "".[speech]""` is deprecated and `pip install "".[torch-speech]""` should be used instead
extras[""speech""] = deps_list(""torchaudio"") + extras[""audio""]
extras[""torch-speech""] = deps_list(""torchaudio"") + extras[""audio""]
extras[""tf-speech""] = extras[""audio""]
extras[""flax-speech""] = extras[""audio""]
extras[""vision""] = deps_list(""Pillow"")
extras[""timm""] = deps_list(""timm"")
extras[""torch-vision""] = deps_list(""torchvision"") + extras[""vision""]
extras[""natten""] = deps_list(""natten"")
extras[""codecarbon""] = deps_list(""codecarbon"")
extras[""video""] = deps_list(""av"")

extras[""sentencepiece""] = deps_list(""sentencepiece"", ""protobuf"")
extras[""tiktoken""] = deps_list(""tiktoken"", ""blobfile"")
extras[""testing""] = (
    deps_list(
        ""pytest"",
        ""pytest-asyncio"",
        ""pytest-rich"",
        ""pytest-xdist"",
        ""timeout-decorator"",
        ""parameterized"",
        ""psutil"",
        ""datasets"",
        ""dill"",
        ""evaluate"",
        ""pytest-timeout"",
        ""ruff"",
        ""sacrebleu"",
        ""rouge-score"",
        ""nltk"",
        ""GitPython"",
        ""sacremoses"",
        ""rjieba"",
        ""beautifulsoup4"",
        ""tensorboard"",
        ""pydantic"",
        ""sentencepiece"",
    )
    + extras[""retrieval""]
    + extras[""modelcreation""]
)

extras[""deepspeed-testing""] = extras[""deepspeed""] + extras[""testing""] + extras[""optuna""] + extras[""sentencepiece""]
extras[""ruff""] = deps_list(""ruff"")
extras[""quality""] = deps_list(""datasets"", ""isort"", ""ruff"", ""GitPython"", ""urllib3"", ""libcst"", ""rich"")

extras[""all""] = (
    extras[""tf""]
    + extras[""torch""]
    + extras[""flax""]
    + extras[""sentencepiece""]
    + extras[""tokenizers""]
    + extras[""torch-speech""]
    + extras[""vision""]
    + extras[""integrations""]
    + extras[""timm""]
    + extras[""torch-vision""]
    + extras[""codecarbon""]
    + extras[""accelerate""]
    + extras[""video""]
)


extras[""dev-torch""] = (
    extras[""testing""]
    + extras[""torch""]
    + extras[""sentencepiece""]
    + extras[""tokenizers""]
    + extras[""torch-speech""]
    + extras[""vision""]
    + extras[""integrations""]
    + extras[""timm""]
    + extras[""torch-vision""]
    + extras[""codecarbon""]
    + extras[""quality""]
    + extras[""ja""]
    + extras[""sklearn""]
    + extras[""modelcreation""]
    + extras[""onnxruntime""]
)
extras[""dev-tensorflow""] = (
    extras[""testing""]
    + extras[""tf""]
    + extras[""sentencepiece""]
    + extras[""tokenizers""]
    + extras[""vision""]
    + extras[""quality""]
    + extras[""sklearn""]
    + extras[""modelcreation""]
    + extras[""onnx""]
    + extras[""tf-speech""]
)
extras[""dev""] = (
    extras[""all""] + extras[""testing""] + extras[""quality""] + extras[""ja""] + extras[""sklearn""] + extras[""modelcreation""]
)

extras[""torchhub""] = deps_list(
    ""filelock"",
    ""huggingface-hub"",
    ""importlib_metadata"",
    ""numpy"",
    ""packaging"",
    ""protobuf"",
    ""regex"",
    ""requests"",
    ""sentencepiece"",
    ""torch"",
    ""tokenizers"",
    ""tqdm"",
)

extras[""agents""] = deps_list(
    ""diffusers"", ""accelerate"", ""datasets"", ""torch"", ""sentencepiece"", ""opencv-python"", ""Pillow""
)

extras[""benchmark""] = deps_list(""optimum-benchmark"")

# when modifying the following list, make sure to update src/transformers/dependency_versions_check.py
install_requires = [
    deps[""filelock""],  # filesystem locks, e.g., to prevent parallel downloads
    deps[""huggingface-hub""],
    deps[""numpy""],
    deps[""packaging""],  # utilities from PyPA to e.g., compare versions
    deps[""pyyaml""],  # used for the model cards metadata
    deps[""regex""],  # for OpenAI GPT
    deps[""requests""],  # for downloading models over HTTPS
    deps[""tokenizers""],
    deps[""safetensors""],
    deps[""tqdm""],  # progress bars in model download and training scripts
]

setup(
    name=""transformers"",
    version=""4.50.0.dev0"",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)
    author=""The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)"",
    author_email=""transformers@huggingface.co"",
    description=""State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow"",
    long_description=open(""README.md"", ""r"", encoding=""utf-8"").read(),
    long_description_content_type=""text/markdown"",
    keywords=""NLP vision speech deep learning transformer pytorch tensorflow jax BERT GPT-2 Wav2Vec2 ViT"",
    license=""Apache 2.0 License"",
    url=""https://github.com/huggingface/transformers"",
    package_dir={"""": ""src""},
    packages=find_packages(""src""),
    include_package_data=True,
    package_data={"""": [""**/*.cu"", ""**/*.cpp"", ""**/*.cuh"", ""**/*.h"", ""**/*.pyx""]},
    zip_safe=False,
    extras_require=extras,
    entry_points={""console_scripts"": [""transformers-cli=transformers.commands.transformers_cli:main""]},
    python_requires="">=3.9.0"",
    install_requires=list(install_requires),
    classifiers=[
        ""Development Status :: 5 - Production/Stable"",
        ""Intended Audience :: Developers"",
        ""Intended Audience :: Education"",
        ""Intended Audience :: Science/Research"",
        ""License :: OSI Approved :: Apache Software License"",
        ""Operating System :: OS Independent"",
        ""Programming Language :: Python :: 3"",
        ""Programming Language :: Python :: 3.9"",
        ""Programming Language :: Python :: 3.10"",
        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",
    ],
    cmdclass={""deps_table_update"": DepsTableUpdateCommand},
)

extras[""tests_torch""] = deps_list()
extras[""tests_tf""] = deps_list()
extras[""tests_flax""] = deps_list()
extras[""tests_hub""] = deps_list()
extras[""tests_pipelines_torch""] = deps_list()
extras[""tests_pipelines_tf""] = deps_list()
extras[""tests_onnx""] = deps_list()
extras[""tests_examples_torch""] = deps_list()
extras[""tests_examples_tf""] = deps_list()
extras[""tests_custom_tokenizers""] = deps_list()
extras[""tests_exotic_models""] = deps_list()
extras[""consistency""] = deps_list()
"
3b1b_manim,3b1b_manim_example_scenes.py,26507,https://raw.githubusercontent.com/3b1b/manim/master/example_scenes.py,"from manimlib import *
import numpy as np

# To watch one of these scenes, run the following:
# manimgl example_scenes.py OpeningManimExample
# Use -s to skip to the end and just save the final frame
# Use -w to write the animation to a file
# Use -o to write it to a file and open it once done
# Use -n <number> to skip ahead to the n'th animation of a scene.


class OpeningManimExample(Scene):
    def construct(self):
        intro_words = Text(""""""
            The original motivation for manim was to
            better illustrate mathematical functions
            as transformations.
        """""")
        intro_words.to_edge(UP)

        self.play(Write(intro_words))
        self.wait(2)

        # Linear transform
        grid = NumberPlane((-10, 10), (-5, 5))
        matrix = [[1, 1], [0, 1]]
        linear_transform_words = VGroup(
            Text(""This is what the matrix""),
            IntegerMatrix(matrix),
            Text(""looks like"")
        )
        linear_transform_words.arrange(RIGHT)
        linear_transform_words.to_edge(UP)
        linear_transform_words.set_backstroke(width=5)

        self.play(
            ShowCreation(grid),
            FadeTransform(intro_words, linear_transform_words)
        )
        self.wait()
        self.play(grid.animate.apply_matrix(matrix), run_time=3)
        self.wait()

        # Complex map
        c_grid = ComplexPlane()
        moving_c_grid = c_grid.copy()
        moving_c_grid.prepare_for_nonlinear_transform()
        c_grid.set_stroke(BLUE_E, 1)
        c_grid.add_coordinate_labels(font_size=24)
        complex_map_words = TexText(""""""
            Or thinking of the plane as $\\mathds{C}$,\\\\
            this is the map $z \\rightarrow z^2$
        """""")
        complex_map_words.to_corner(UR)
        complex_map_words.set_backstroke(width=5)

        self.play(
            FadeOut(grid),
            Write(c_grid, run_time=3),
            FadeIn(moving_c_grid),
            FadeTransform(linear_transform_words, complex_map_words),
        )
        self.wait()
        self.play(
            moving_c_grid.animate.apply_complex_function(lambda z: z**2),
            run_time=6,
        )
        self.wait(2)


class AnimatingMethods(Scene):
    def construct(self):
        grid = Tex(R""\pi"").get_grid(10, 10, height=4)
        self.add(grid)

        # You can animate the application of mobject methods with the
        # "".animate"" syntax:
        self.play(grid.animate.shift(LEFT))

        # Both of those will interpolate between the mobject's initial
        # state and whatever happens when you apply that method.
        # For this example, calling grid.shift(LEFT) would shift the
        # grid one unit to the left, but both of the previous calls to
        # ""self.play"" animate that motion.

        # The same applies for any method, including those setting colors.
        self.play(grid.animate.set_color(YELLOW))
        self.wait()
        self.play(grid.animate.set_submobject_colors_by_gradient(BLUE, GREEN))
        self.wait()
        self.play(grid.animate.set_height(TAU - MED_SMALL_BUFF))
        self.wait()

        # The method Mobject.apply_complex_function lets you apply arbitrary
        # complex functions, treating the points defining the mobject as
        # complex numbers.
        self.play(grid.animate.apply_complex_function(np.exp), run_time=5)
        self.wait()

        # Even more generally, you could apply Mobject.apply_function,
        # which takes in functions form R^3 to R^3
        self.play(
            grid.animate.apply_function(
                lambda p: [
                    p[0] + 0.5 * math.sin(p[1]),
                    p[1] + 0.5 * math.sin(p[0]),
                    p[2]
                ]
            ),
            run_time=5,
        )
        self.wait()


class TextExample(Scene):
    def construct(self):
        # To run this scene properly, you should have ""Consolas"" font in your computer
        # for full usage, you can see https://github.com/3b1b/manim/pull/680
        text = Text(""Here is a text"", font=""Consolas"", font_size=90)
        difference = Text(
            """"""
            The most important difference between Text and TexText is that\n
            you can change the font more easily, but can't use the LaTeX grammar
            """""",
            font=""Arial"", font_size=24,
            # t2c is a dict that you can choose color for different text
            t2c={""Text"": BLUE, ""TexText"": BLUE, ""LaTeX"": ORANGE}
        )
        VGroup(text, difference).arrange(DOWN, buff=1)
        self.play(Write(text))
        self.play(FadeIn(difference, UP))
        self.wait(3)

        fonts = Text(
            ""And you can also set the font according to different words"",
            font=""Arial"",
            t2f={""font"": ""Consolas"", ""words"": ""Consolas""},
            t2c={""font"": BLUE, ""words"": GREEN}
        )
        fonts.set_width(FRAME_WIDTH - 1)
        slant = Text(
            ""And the same as slant and weight"",
            font=""Consolas"",
            t2s={""slant"": ITALIC},
            t2w={""weight"": BOLD},
            t2c={""slant"": ORANGE, ""weight"": RED}
        )
        VGroup(fonts, slant).arrange(DOWN, buff=0.8)
        self.play(FadeOut(text), FadeOut(difference, shift=DOWN))
        self.play(Write(fonts))
        self.wait()
        self.play(Write(slant))
        self.wait()


class TexTransformExample(Scene):
    def construct(self):
        # Tex to color map
        t2c = {
            ""A"": BLUE,
            ""B"": TEAL,
            ""C"": GREEN,
        }
        # Configuration to pass along to each Tex mobject
        kw = dict(font_size=72, t2c=t2c)
        lines = VGroup(
            Tex(""A^2 + B^2 = C^2"", **kw),
            Tex(""A^2 = C^2 - B^2"", **kw),
            Tex(""A^2 = (C + B)(C - B)"", **kw),
            Tex(R""A = \sqrt{(C + B)(C - B)}"", **kw),
        )
        lines.arrange(DOWN, buff=LARGE_BUFF)

        self.add(lines[0])
        # The animation TransformMatchingStrings will line up parts
        # of the source and target which have matching substring strings.
        # Here, giving it a little path_arc makes each part rotate into
        # their final positions, which feels appropriate for the idea of
        # rearranging an equation
        self.play(
            TransformMatchingStrings(
                lines[0].copy(), lines[1],
                # matched_keys specifies which substring should
                # line up. If it's not specified, the animation
                # will align the longest matching substrings.
                # In this case, the substring ""^2 = C^2"" would
                # trip it up
                matched_keys=[""A^2"", ""B^2"", ""C^2""],
                # When you want a substring from the source
                # to go to a non-equal substring from the target,
                # use the key map.
                key_map={""+"": ""-""},
                path_arc=90 * DEG,
            ),
        )
        self.wait()
        self.play(TransformMatchingStrings(
            lines[1].copy(), lines[2],
            matched_keys=[""A^2""]
        ))
        self.wait()
        self.play(
            TransformMatchingStrings(
                lines[2].copy(), lines[3],
                key_map={""2"": R""\sqrt""},
                path_arc=-30 * DEG,
            ),
        )
        self.wait(2)
        self.play(LaggedStartMap(FadeOut, lines, shift=2 * RIGHT))

        # TransformMatchingShapes will try to line up all pieces of a
        # source mobject with those of a target, regardless of the
        # what Mobject type they are.
        source = Text(""the morse code"", height=1)
        target = Text(""here come dots"", height=1)
        saved_source = source.copy()

        self.play(Write(source))
        self.wait()
        kw = dict(run_time=3, path_arc=PI / 2)
        self.play(TransformMatchingShapes(source, target, **kw))
        self.wait()
        self.play(TransformMatchingShapes(target, saved_source, **kw))
        self.wait()


class TexIndexing(Scene):
    def construct(self):
        # You can index into Tex mobject (or other StringMobjects) by substrings
        equation = Tex(R""e^{\pi i} = -1"", font_size=144)

        self.add(equation)
        self.play(FlashAround(equation[""e""]))
        self.wait()
        self.play(Indicate(equation[R""\pi""]))
        self.wait()
        self.play(TransformFromCopy(
            equation[R""e^{\pi i}""].copy().set_opacity(0.5),
            equation[""-1""],
            path_arc=-PI / 2,
            run_time=3
        ))
        self.play(FadeOut(equation))

        # Or regular expressions
        equation = Tex(""A^2 + B^2 = C^2"", font_size=144)

        self.play(Write(equation))
        for part in equation[re.compile(r""\w\^2"")]:
            self.play(FlashAround(part))
        self.wait()
        self.play(FadeOut(equation))
        
        # Indexing by substrings like this may not work when
        # the order in which Latex draws symbols does not match
        # the order in which they show up in the string.
        # For example, here the infinity is drawn before the sigma
        # so we don't get the desired behavior.
        equation = Tex(R""\sum_{n = 1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}"", font_size=72)
        self.play(FadeIn(equation))
        self.play(equation[R""\infty""].animate.set_color(RED))  # Doesn't hit the infinity
        self.wait()
        self.play(FadeOut(equation))

        # However you can always fix this by explicitly passing in
        # a string you might want to isolate later. Also, using
        # \over instead of \frac helps to avoid the issue for fractions
        equation = Tex(
            R""\sum_{n = 1}^\infty {1 \over n^2} = {\pi^2 \over 6}"",
            # Explicitly mark ""\infty"" as a substring you might want to access
            isolate=[R""\infty""],
            font_size=72
        )
        self.play(FadeIn(equation))
        self.play(equation[R""\infty""].animate.set_color(RED))  # Got it!
        self.wait()
        self.play(FadeOut(equation))


class UpdatersExample(Scene):
    def construct(self):
        square = Square()
        square.set_fill(BLUE_E, 1)

        # On all frames, the constructor Brace(square, UP) will
        # be called, and the mobject brace will set its data to match
        # that of the newly constructed object
        brace = always_redraw(Brace, square, UP)

        label = TexText(""Width = 0.00"")
        number = label.make_number_changeable(""0.00"")

        # This ensures that the method deicmal.next_to(square)
        # is called on every frame
        label.always.next_to(brace, UP)
        # You could also write the following equivalent line
        # label.add_updater(lambda m: m.next_to(brace, UP))

        # If the argument itself might change, you can use f_always,
        # for which the arguments following the initial Mobject method
        # should be functions returning arguments to that method.
        # The following line ensures thst decimal.set_value(square.get_y())
        # is called every frame
        number.f_always.set_value(square.get_width)
        # You could also write the following equivalent line
        # number.add_updater(lambda m: m.set_value(square.get_width()))

        self.add(square, brace, label)

        # Notice that the brace and label track with the square
        self.play(
            square.animate.scale(2),
            rate_func=there_and_back,
            run_time=2,
        )
        self.wait()
        self.play(
            square.animate.set_width(5, stretch=True),
            run_time=3,
        )
        self.wait()
        self.play(
            square.animate.set_width(2),
            run_time=3
        )
        self.wait()

        # In general, you can alway call Mobject.add_updater, and pass in
        # a function that you want to be called on every frame.  The function
        # should take in either one argument, the mobject, or two arguments,
        # the mobject and the amount of time since the last frame.
        now = self.time
        w0 = square.get_width()
        square.add_updater(
            lambda m: m.set_width(w0 * math.sin(self.time - now) + w0)
        )
        self.wait(4 * PI)


class CoordinateSystemExample(Scene):
    def construct(self):
        axes = Axes(
            # x-axis ranges from -1 to 10, with a default step size of 1
            x_range=(-1, 10),
            # y-axis ranges from -2 to 2 with a step size of 0.5
            y_range=(-2, 2, 0.5),
            # The axes will be stretched so as to match the specified
            # height and width
            height=6,
            width=10,
            # Axes is made of two NumberLine mobjects.  You can specify
            # their configuration with axis_config
            axis_config=dict(
                stroke_color=GREY_A,
                stroke_width=2,
                numbers_to_exclude=[0],
            ),
            # Alternatively, you can specify configuration for just one
            # of them, like this.
            y_axis_config=dict(
                big_tick_numbers=[-2, 2],
            )
        )
        # Keyword arguments of add_coordinate_labels can be used to
        # configure the DecimalNumber mobjects which it creates and
        # adds to the axes
        axes.add_coordinate_labels(
            font_size=20,
            num_decimal_places=1,
        )
        self.add(axes)

        # Axes descends from the CoordinateSystem class, meaning
        # you can call call axes.coords_to_point, abbreviated to
        # axes.c2p, to associate a set of coordinates with a point,
        # like so:
        dot = Dot(color=RED)
        dot.move_to(axes.c2p(0, 0))
        self.play(FadeIn(dot, scale=0.5))
        self.play(dot.animate.move_to(axes.c2p(3, 2)))
        self.wait()
        self.play(dot.animate.move_to(axes.c2p(5, 0.5)))
        self.wait()

        # Similarly, you can call axes.point_to_coords, or axes.p2c
        # print(axes.p2c(dot.get_center()))

        # We can draw lines from the axes to better mark the coordinates
        # of a given point.
        # Here, the always_redraw command means that on each new frame
        # the lines will be redrawn
        h_line = always_redraw(lambda: axes.get_h_line(dot.get_left()))
        v_line = always_redraw(lambda: axes.get_v_line(dot.get_bottom()))

        self.play(
            ShowCreation(h_line),
            ShowCreation(v_line),
        )
        self.play(dot.animate.move_to(axes.c2p(3, -2)))
        self.wait()
        self.play(dot.animate.move_to(axes.c2p(1, 1)))
        self.wait()

        # If we tie the dot to a particular set of coordinates, notice
        # that as we move the axes around it respects the coordinate
        # system defined by them.
        f_always(dot.move_to, lambda: axes.c2p(1, 1))
        self.play(
            axes.animate.scale(0.75).to_corner(UL),
            run_time=2,
        )
        self.wait()
        self.play(FadeOut(VGroup(axes, dot, h_line, v_line)))

        # Other coordinate systems you can play around with include
        # ThreeDAxes, NumberPlane, and ComplexPlane.


class GraphExample(Scene):
    def construct(self):
        axes = Axes((-3, 10), (-1, 8), height=6)
        axes.add_coordinate_labels()

        self.play(Write(axes, lag_ratio=0.01, run_time=1))

        # Axes.get_graph will return the graph of a function
        sin_graph = axes.get_graph(
            lambda x: 2 * math.sin(x),
            color=BLUE,
        )
        # By default, it draws it so as to somewhat smoothly interpolate
        # between sampled points (x, f(x)).  If the graph is meant to have
        # a corner, though, you can set use_smoothing to False
        relu_graph = axes.get_graph(
            lambda x: max(x, 0),
            use_smoothing=False,
            color=YELLOW,
        )
        # For discontinuous functions, you can specify the point of
        # discontinuity so that it does not try to draw over the gap.
        step_graph = axes.get_graph(
            lambda x: 2.0 if x > 3 else 1.0,
            discontinuities=[3],
            color=GREEN,
        )

        # Axes.get_graph_label takes in either a string or a mobject.
        # If it's a string, it treats it as a LaTeX expression.  By default
        # it places the label next to the graph near the right side, and
        # has it match the color of the graph
        sin_label = axes.get_graph_label(sin_graph, ""\\sin(x)"")
        relu_label = axes.get_graph_label(relu_graph, Text(""ReLU""))
        step_label = axes.get_graph_label(step_graph, Text(""Step""), x=4)

        self.play(
            ShowCreation(sin_graph),
            FadeIn(sin_label, RIGHT),
        )
        self.wait(2)
        self.play(
            ReplacementTransform(sin_graph, relu_graph),
            FadeTransform(sin_label, relu_label),
        )
        self.wait()
        self.play(
            ReplacementTransform(relu_graph, step_graph),
            FadeTransform(relu_label, step_label),
        )
        self.wait()

        parabola = axes.get_graph(lambda x: 0.25 * x**2)
        parabola.set_stroke(BLUE)
        self.play(
            FadeOut(step_graph),
            FadeOut(step_label),
            ShowCreation(parabola)
        )
        self.wait()

        # You can use axes.input_to_graph_point, abbreviated
        # to axes.i2gp, to find a particular point on a graph
        dot = Dot(color=RED)
        dot.move_to(axes.i2gp(2, parabola))
        self.play(FadeIn(dot, scale=0.5))

        # A value tracker lets us animate a parameter, usually
        # with the intent of having other mobjects update based
        # on the parameter
        x_tracker = ValueTracker(2)
        dot.add_updater(lambda d: d.move_to(axes.i2gp(x_tracker.get_value(), parabola)))

        self.play(x_tracker.animate.set_value(4), run_time=3)
        self.play(x_tracker.animate.set_value(-2), run_time=3)
        self.wait()


class TexAndNumbersExample(Scene):
    def construct(self):
        axes = Axes((-3, 3), (-3, 3), unit_size=1)
        axes.to_edge(DOWN)
        axes.add_coordinate_labels(font_size=16)
        circle = Circle(radius=2)
        circle.set_stroke(YELLOW, 3)
        circle.move_to(axes.get_origin())
        self.add(axes, circle)

        # When numbers show up in tex, they can be readily
        # replaced with DecimalMobjects so that methods like
        # get_value and set_value can be called on them, and
        # animations like ChangeDecimalToValue can be called
        # on them.
        tex = Tex(""x^2 + y^2 = 4.00"")
        tex.next_to(axes, UP, buff=0.5)
        value = tex.make_number_changeable(""4.00"")


        # This will tie the right hand side of our equation to
        # the square of the radius of the circle
        value.add_updater(lambda v: v.set_value(circle.get_radius()**2))
        self.add(tex)

        text = Text(""""""
            You can manipulate numbers
            in Tex mobjects
        """""", font_size=30)
        text.next_to(tex, RIGHT, buff=1.5)
        arrow = Arrow(text, tex)
        self.add(text, arrow)

        self.play(
            circle.animate.set_height(2.0),
            run_time=4,
            rate_func=there_and_back,
        )

        # By default, tex.make_number_changeable replaces the first occurance
        # of the number,but by passing replace_all=True it replaces all and
        # returns a group of the results
        exponents = tex.make_number_changeable(""2"", replace_all=True)
        self.play(
            LaggedStartMap(
                FlashAround, exponents,
                lag_ratio=0.2, buff=0.1, color=RED
            ),
            exponents.animate.set_color(RED)
        )

        def func(x, y):
            # Switch from manim coords to axes coords
            xa, ya = axes.point_to_coords(np.array([x, y, 0]))
            return xa**4 + ya**4 - 4

        new_curve = ImplicitFunction(func)
        new_curve.match_style(circle)
        circle.rotate(angle_of_vector(new_curve.get_start()))  # Align
        value.clear_updaters()

        self.play(
            *(ChangeDecimalToValue(exp, 4) for exp in exponents),
            ReplacementTransform(circle.copy(), new_curve),
            circle.animate.set_stroke(width=1, opacity=0.5),
        )


class SurfaceExample(ThreeDScene):
    def construct(self):
        surface_text = Text(""For 3d scenes, try using surfaces"")
        surface_text.fix_in_frame()
        surface_text.to_edge(UP)
        self.add(surface_text)
        self.wait(0.1)

        torus1 = Torus(r1=1, r2=1)
        torus2 = Torus(r1=3, r2=1)
        sphere = Sphere(radius=3, resolution=torus1.resolution)
        # You can texture a surface with up to two images, which will
        # be interpreted as the side towards the light, and away from
        # the light.  These can be either urls, or paths to a local file
        # in whatever you've set as the image directory in
        # the custom_config.yml file

        # day_texture = ""EarthTextureMap""
        # night_texture = ""NightEarthTextureMap""
        day_texture = ""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Whole_world_-_land_and_oceans.jpg/1280px-Whole_world_-_land_and_oceans.jpg""
        night_texture = ""https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/The_earth_at_night.jpg/1280px-The_earth_at_night.jpg""

        surfaces = [
            TexturedSurface(surface, day_texture, night_texture)
            for surface in [sphere, torus1, torus2]
        ]

        for mob in surfaces:
            mob.shift(IN)
            mob.mesh = SurfaceMesh(mob)
            mob.mesh.set_stroke(BLUE, 1, opacity=0.5)

        surface = surfaces[0]

        self.play(
            FadeIn(surface),
            ShowCreation(surface.mesh, lag_ratio=0.01, run_time=3),
        )
        for mob in surfaces:
            mob.add(mob.mesh)
        surface.save_state()
        self.play(Rotate(surface, PI / 2), run_time=2)
        for mob in surfaces[1:]:
            mob.rotate(PI / 2)

        self.play(
            Transform(surface, surfaces[1]),
            run_time=3
        )

        self.play(
            Transform(surface, surfaces[2]),
            # Move camera frame during the transition
            self.frame.animate.increment_phi(-10 * DEG),
            self.frame.animate.increment_theta(-20 * DEG),
            run_time=3
        )
        # Add ambient rotation
        self.frame.add_updater(lambda m, dt: m.increment_theta(-0.1 * dt))

        # Play around with where the light is
        light_text = Text(""You can move around the light source"")
        light_text.move_to(surface_text)
        light_text.fix_in_frame()

        self.play(FadeTransform(surface_text, light_text))
        light = self.camera.light_source
        light_dot = GlowDot(color=WHITE, radius=0.5)
        light_dot.always.move_to(light)
        self.add(light, light_dot)
        light.save_state()
        self.play(light.animate.move_to(3 * IN), run_time=5)
        self.play(light.animate.shift(10 * OUT), run_time=5)

        drag_text = Text(""Try moving the mouse while pressing d or f"")
        drag_text.move_to(light_text)
        drag_text.fix_in_frame()

        self.play(FadeTransform(light_text, drag_text))
        self.wait()


class InteractiveDevelopment(Scene):
    def construct(self):
        circle = Circle()
        circle.set_fill(BLUE, opacity=0.5)
        circle.set_stroke(BLUE_E, width=4)
        square = Square()

        self.play(ShowCreation(square))
        self.wait()

        # This opens an iPython terminal where you can keep writing
        # lines as if they were part of this construct method.
        # In particular, 'square', 'circle' and 'self' will all be
        # part of the local namespace in that terminal.
        self.embed()

        # Try copying and pasting some of the lines below into
        # the interactive shell
        self.play(ReplacementTransform(square, circle))
        self.wait()
        self.play(circle.animate.stretch(4, 0))
        self.play(Rotate(circle, 90 * DEG))
        self.play(circle.animate.shift(2 * RIGHT).scale(0.25))

        text = Text(""""""
            In general, using the interactive shell
            is very helpful when developing new scenes
        """""")
        self.play(Write(text))

        # In the interactive shell, you can just type
        # play, add, remove, clear, wait, save_state and restore,
        # instead of self.play, self.add, self.remove, etc.

        # To interact with the window, type touch().  You can then
        # scroll in the window, or zoom by holding down 'z' while scrolling,
        # and change camera perspective by holding down 'd' while moving
        # the mouse.  Press 'r' to reset to the standard camera position.
        # Press 'q' to stop interacting with the window and go back to
        # typing new commands into the shell.

        # In principle you can customize a scene to be responsive to
        # mouse and keyboard interactions
        always(circle.move_to, self.mouse_point)


class ControlsExample(Scene):
    drag_to_pan = False

    def setup(self):
        self.textbox = Textbox()
        self.checkbox = Checkbox()
        self.color_picker = ColorSliders()
        self.panel = ControlPanel(
            Text(""Text"", font_size=24), self.textbox, Line(),
            Text(""Show/Hide Text"", font_size=24), self.checkbox, Line(),
            Text(""Color of Text"", font_size=24), self.color_picker
        )
        self.add(self.panel)

    def construct(self):
        text = Text(""text"", font_size=96)

        def text_updater(old_text):
            assert(isinstance(old_text, Text))
            new_text = Text(self.textbox.get_value(), font_size=old_text.font_size)
            # new_text.align_data_and_family(old_text)
            new_text.move_to(old_text)
            if self.checkbox.get_value():
                new_text.set_fill(
                    color=self.color_picker.get_picked_color(),
                    opacity=self.color_picker.get_picked_opacity()
                )
            else:
                new_text.set_opacity(0)
            old_text.become(new_text)

        text.add_updater(text_updater)

        self.add(MotionMobject(text))

        self.textbox.set_value(""Manim"")
        # self.wait(60)
        # self.embed()


# See https://github.com/3b1b/videos for many, many more
"
binary-husky_gpt_academic,binary-husky_gpt_academic_config.py,17785,https://raw.githubusercontent.com/binary-husky/gpt_academic/master/config.py,"""""""
    以下所有配置也都支持利用环境变量覆写，环境变量配置格式见docker-compose.yml。
    读取优先级：环境变量 > config_private.py > config.py
    --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---
    All the following configurations also support using environment variables to override,
    and the environment variable configuration format can be seen in docker-compose.yml.
    Configuration reading priority: environment variable > config_private.py > config.py
""""""

# [step 1-1]>> ( 接入GPT等模型 ) API_KEY = ""sk-123456789xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx123456789""。极少数情况下，还需要填写组织（格式如org-123456789abcdefghijklmno的），请向下翻，找 API_ORG 设置项
API_KEY = ""在此处填写APIKEY""    # 可同时填写多个API-KEY，用英文逗号分割，例如API_KEY = ""sk-openaikey1,sk-openaikey2,fkxxxx-api2dkey3,azure-apikey4""

# [step 1-2]>> ( 接入通义 qwen-max ) 接入通义千问在线大模型，api-key获取地址 https://dashscope.console.aliyun.com/
DASHSCOPE_API_KEY = """" # 阿里灵积云API_KEY

# [step 1-3]>> ( 接入 deepseek-reasoner, 即 deepseek-r1 ) 深度求索(DeepSeek) API KEY，默认请求地址为""https://api.deepseek.com/v1/chat/completions""
DEEPSEEK_API_KEY = """"

# [step 2]>> 改为True应用代理。如果使用本地或无地域限制的大模型时，此处不修改；如果直接在海外服务器部署，此处不修改
USE_PROXY = False
if USE_PROXY:
    """"""
    代理网络的地址，打开你的代理软件查看代理协议(socks5h / http)、地址(localhost)和端口(11284)
    填写格式是 [协议]://  [地址] :[端口]，填写之前不要忘记把USE_PROXY改成True，如果直接在海外服务器部署，此处不修改
            <配置教程&视频教程> https://github.com/binary-husky/gpt_academic/issues/1>
    [协议] 常见协议无非socks5h/http; 例如 v2**y 和 ss* 的默认本地协议是socks5h; 而cl**h 的默认本地协议是http
    [地址] 填localhost或者127.0.0.1（localhost意思是代理软件安装在本机上）
    [端口] 在代理软件的设置里找。虽然不同的代理软件界面不一样，但端口号都应该在最显眼的位置上
    """"""
    proxies = {
        #          [协议]://  [地址]  :[端口]
        ""http"":  ""socks5h://localhost:11284"",  # 再例如  ""http"":  ""http://127.0.0.1:7890"",
        ""https"": ""socks5h://localhost:11284"",  # 再例如  ""https"": ""http://127.0.0.1:7890"",
    }
else:
    proxies = None

# [step 3]>> 模型选择是 (注意: LLM_MODEL是默认选中的模型, 它*必须*被包含在AVAIL_LLM_MODELS列表中 )
LLM_MODEL = ""gpt-3.5-turbo-16k"" # 可选 ↓↓↓
AVAIL_LLM_MODELS = [""qwen-max"", ""o1-mini"", ""o1-mini-2024-09-12"", ""o1"", ""o1-2024-12-17"", ""o1-preview"", ""o1-preview-2024-09-12"",
                    ""gpt-4-1106-preview"", ""gpt-4-turbo-preview"", ""gpt-4-vision-preview"",
                    ""gpt-4o"", ""gpt-4o-mini"", ""gpt-4-turbo"", ""gpt-4-turbo-2024-04-09"",
                    ""gpt-3.5-turbo-1106"", ""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""azure-gpt-3.5"",
                    ""gpt-4"", ""gpt-4-32k"", ""azure-gpt-4"", ""glm-4"", ""glm-4v"", ""glm-3-turbo"",
                    ""gemini-1.5-pro"", ""chatglm3"", ""chatglm4"",
                    ""deepseek-chat"", ""deepseek-coder"", ""deepseek-reasoner""
                    ]

EMBEDDING_MODEL = ""text-embedding-3-small""

# --- --- --- ---
# P.S. 其他可用的模型还包括
# AVAIL_LLM_MODELS = [
#   ""glm-4-0520"", ""glm-4-air"", ""glm-4-airx"", ""glm-4-flash"",
#   ""qianfan"", ""deepseekcoder"",
#   ""spark"", ""sparkv2"", ""sparkv3"", ""sparkv3.5"", ""sparkv4"",
#   ""qwen-turbo"", ""qwen-plus"", ""qwen-local"",
#   ""moonshot-v1-128k"", ""moonshot-v1-32k"", ""moonshot-v1-8k"",
#   ""gpt-3.5-turbo-0613"", ""gpt-3.5-turbo-16k-0613"", ""gpt-3.5-turbo-0125"", ""gpt-4o-2024-05-13""
#   ""claude-3-haiku-20240307"",""claude-3-sonnet-20240229"",""claude-3-opus-20240229"", ""claude-2.1"", ""claude-instant-1.2"",
#   ""moss"", ""llama2"", ""chatglm_onnx"", ""internlm"", ""jittorllms_pangualpha"", ""jittorllms_llama"",
#   ""deepseek-chat"" ,""deepseek-coder"",
#   ""gemini-1.5-flash"",
#   ""yi-34b-chat-0205"",""yi-34b-chat-200k"",""yi-large"",""yi-medium"",""yi-spark"",""yi-large-turbo"",""yi-large-preview"",
#   ""grok-beta"",
# ]
# --- --- --- ---
# 此外，您还可以在接入one-api/vllm/ollama/Openroute时，
# 使用""one-api-*"",""vllm-*"",""ollama-*"",""openrouter-*""前缀直接使用非标准方式接入的模型，例如
# AVAIL_LLM_MODELS = [""one-api-claude-3-sonnet-20240229(max_token=100000)"", ""ollama-phi3(max_token=4096)"",""openrouter-openai/gpt-4o-mini"",""openrouter-openai/chatgpt-4o-latest""]
# --- --- --- ---


# --------------- 以下配置可以优化体验 ---------------

# 重新URL重新定向，实现更换API_URL的作用（高危设置! 常规情况下不要修改! 通过修改此设置，您将把您的API-KEY和对话隐私完全暴露给您设定的中间人！）
# 格式: API_URL_REDIRECT = {""https://api.openai.com/v1/chat/completions"": ""在这里填写重定向的api.openai.com的URL""}
# 举例: API_URL_REDIRECT = {""https://api.openai.com/v1/chat/completions"": ""https://reverse-proxy-url/v1/chat/completions"", ""http://localhost:11434/api/chat"": ""在这里填写您ollama的URL""}
API_URL_REDIRECT = {}


# 多线程函数插件中，默认允许多少路线程同时访问OpenAI。Free trial users的限制是每分钟3次，Pay-as-you-go users的限制是每分钟3500次
# 一言以蔽之：免费（5刀）用户填3，OpenAI绑了信用卡的用户可以填 16 或者更高。提高限制请查询：https://platform.openai.com/docs/guides/rate-limits/overview
DEFAULT_WORKER_NUM = 8


# 色彩主题, 可选 [""Default"", ""Chuanhu-Small-and-Beautiful"", ""High-Contrast""]
# 更多主题, 请查阅Gradio主题商店: https://huggingface.co/spaces/gradio/theme-gallery 可选 [""Gstaff/Xkcd"", ""NoCrypt/Miku"", ...]
THEME = ""Default""
AVAIL_THEMES = [""Default"", ""Chuanhu-Small-and-Beautiful"", ""High-Contrast"", ""Gstaff/Xkcd"", ""NoCrypt/Miku""]

FONT = ""Theme-Default-Font""
AVAIL_FONTS = [
    ""默认值(Theme-Default-Font)"", 
    ""宋体(SimSun)"",  
    ""黑体(SimHei)"",  
    ""楷体(KaiTi)"",  
    ""仿宋(FangSong)"",  
    ""华文细黑(STHeiti Light)"",
    ""华文楷体(STKaiti)"",  
    ""华文仿宋(STFangsong)"",  
    ""华文宋体(STSong)"",  
    ""华文中宋(STZhongsong)"",  
    ""华文新魏(STXinwei)"",  
    ""华文隶书(STLiti)"", 
    # 备注：以下字体需要网络支持，您可以自定义任意您喜欢的字体，如下所示，需要满足的格式为 ""字体昵称(字体英文真名@字体css下载链接)"" 
    ""思源宋体(Source Han Serif CN VF@https://chinese-fonts-cdn.deno.dev/packages/syst/dist/SourceHanSerifCN/result.css)"",
    ""月星楷(Moon Stars Kai HW@https://chinese-fonts-cdn.deno.dev/packages/moon-stars-kai/dist/MoonStarsKaiHW-Regular/result.css)"",
    ""珠圆体(MaokenZhuyuanTi@https://chinese-fonts-cdn.deno.dev/packages/mkzyt/dist/猫啃珠圆体/result.css)"",
    ""平方萌萌哒(PING FANG MENG MNEG DA@https://chinese-fonts-cdn.deno.dev/packages/pfmmd/dist/平方萌萌哒/result.css)"",
    ""Helvetica"",
    ""ui-sans-serif"",
    ""sans-serif"",
    ""system-ui""
]


# 默认的系统提示词（system prompt）
INIT_SYS_PROMPT = ""Serve me as a writing and programming assistant.""


# 对话窗的高度 （仅在LAYOUT=""TOP-DOWN""时生效）
CHATBOT_HEIGHT = 1115


# 代码高亮
CODE_HIGHLIGHT = True


# 窗口布局
LAYOUT = ""LEFT-RIGHT""   # ""LEFT-RIGHT""（左右布局） # ""TOP-DOWN""（上下布局）


# 暗色模式 / 亮色模式
DARK_MODE = True


# 发送请求到OpenAI后，等待多久判定为超时
TIMEOUT_SECONDS = 30


# 网页的端口, -1代表随机端口
WEB_PORT = -1


# 是否自动打开浏览器页面
AUTO_OPEN_BROWSER = True


# 如果OpenAI不响应（网络卡顿、代理失败、KEY失效），重试的次数限制
MAX_RETRY = 2


# 插件分类默认选项
DEFAULT_FN_GROUPS = ['对话', '编程', '学术', '智能体']


# 定义界面上“询问多个GPT模型”插件应该使用哪些模型，请从AVAIL_LLM_MODELS中选择，并在不同模型之间用`&`间隔，例如""gpt-3.5-turbo&chatglm3&azure-gpt-4""
MULTI_QUERY_LLM_MODELS = ""gpt-3.5-turbo&chatglm3""


# 选择本地模型变体（只有当AVAIL_LLM_MODELS包含了对应本地模型时，才会起作用）
# 如果你选择Qwen系列的模型，那么请在下面的QWEN_MODEL_SELECTION中指定具体的模型
# 也可以是具体的模型路径
QWEN_LOCAL_MODEL_SELECTION = ""Qwen/Qwen-1_8B-Chat-Int8""


# 百度千帆（LLM_MODEL=""qianfan""）
BAIDU_CLOUD_API_KEY = ''
BAIDU_CLOUD_SECRET_KEY = ''
BAIDU_CLOUD_QIANFAN_MODEL = 'ERNIE-Bot'    # 可选 ""ERNIE-Bot-4""(文心大模型4.0), ""ERNIE-Bot""(文心一言), ""ERNIE-Bot-turbo"", ""BLOOMZ-7B"", ""Llama-2-70B-Chat"", ""Llama-2-13B-Chat"", ""Llama-2-7B-Chat"", ""ERNIE-Speed-128K"", ""ERNIE-Speed-8K"", ""ERNIE-Lite-8K""


# 如果使用ChatGLM3或ChatGLM4本地模型，请把 LLM_MODEL=""chatglm3"" 或LLM_MODEL=""chatglm4""，并在此处指定模型路径
CHATGLM_LOCAL_MODEL_PATH = ""THUDM/glm-4-9b-chat"" # 例如""/home/hmp/ChatGLM3-6B/""

# 如果使用ChatGLM2微调模型，请把 LLM_MODEL=""chatglmft""，并在此处指定模型路径
CHATGLM_PTUNING_CHECKPOINT = """" # 例如""/home/hmp/ChatGLM2-6B/ptuning/output/6b-pt-128-1e-2/checkpoint-100""


# 本地LLM模型如ChatGLM的执行方式 CPU/GPU
LOCAL_MODEL_DEVICE = ""cpu"" # 可选 ""cuda""
LOCAL_MODEL_QUANT = ""FP16"" # 默认 ""FP16"" ""INT4"" 启用量化INT4版本 ""INT8"" 启用量化INT8版本


# 设置gradio的并行线程数（不需要修改）
CONCURRENT_COUNT = 100


# 是否在提交时自动清空输入框
AUTO_CLEAR_TXT = False


# 加一个live2d装饰
ADD_WAIFU = False


# 设置用户名和密码（不需要修改）（相关功能不稳定，与gradio版本和网络都相关，如果本地使用不建议加这个）
# [(""username"", ""password""), (""username2"", ""password2""), ...]
AUTHENTICATION = []


# 如果需要在二级路径下运行（常规情况下，不要修改!!）
# （举例 CUSTOM_PATH = ""/gpt_academic""，可以让软件运行在 http://ip:port/gpt_academic/ 下。）
CUSTOM_PATH = ""/""


# HTTPS 秘钥和证书（不需要修改）
SSL_KEYFILE = """"
SSL_CERTFILE = """"


# 极少数情况下，openai的官方KEY需要伴随组织编码（格式如org-xxxxxxxxxxxxxxxxxxxxxxxx）使用
API_ORG = """"


# 如果需要使用Slack Claude，使用教程详情见 request_llms/README.md
SLACK_CLAUDE_BOT_ID = ''
SLACK_CLAUDE_USER_TOKEN = ''


# 如果需要使用AZURE（方法一：单个azure模型部署）详情请见额外文档 docs\use_azure.md
AZURE_ENDPOINT = ""https://你亲手写的api名称.openai.azure.com/""
AZURE_API_KEY = ""填入azure openai api的密钥""    # 建议直接在API_KEY处填写，该选项即将被弃用
AZURE_ENGINE = ""填入你亲手写的部署名""            # 读 docs\use_azure.md


# 如果需要使用AZURE（方法二：多个azure模型部署+动态切换）详情请见额外文档 docs\use_azure.md
AZURE_CFG_ARRAY = {}


# 阿里云实时语音识别 配置难度较高
# 参考 https://github.com/binary-husky/gpt_academic/blob/master/docs/use_audio.md
ENABLE_AUDIO = False
ALIYUN_TOKEN=""""     # 例如 f37f30e0f9934c34a992f6f64f7eba4f
ALIYUN_APPKEY=""""    # 例如 RoPlZrM88DnAFkZK
ALIYUN_ACCESSKEY="""" # （无需填写）
ALIYUN_SECRET=""""    # （无需填写）


# GPT-SOVITS 文本转语音服务的运行地址（将语言模型的生成文本朗读出来）
TTS_TYPE = ""EDGE_TTS"" # EDGE_TTS / LOCAL_SOVITS_API / DISABLE
GPT_SOVITS_URL = """"
EDGE_TTS_VOICE = ""zh-CN-XiaoxiaoNeural""


# 接入讯飞星火大模型 https://console.xfyun.cn/services/iat
XFYUN_APPID = ""00000000""
XFYUN_API_SECRET = ""bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb""
XFYUN_API_KEY = ""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa""


# 接入智谱大模型
ZHIPUAI_API_KEY = """"
ZHIPUAI_MODEL = """" # 此选项已废弃，不再需要填写


# Claude API KEY
ANTHROPIC_API_KEY = """"


# 月之暗面 API KEY
MOONSHOT_API_KEY = """"


# 零一万物(Yi Model) API KEY
YIMODEL_API_KEY = """"


# 紫东太初大模型 https://ai-maas.wair.ac.cn
TAICHU_API_KEY = """"

# Grok API KEY
GROK_API_KEY = """"

# Mathpix 拥有执行PDF的OCR功能，但是需要注册账号
MATHPIX_APPID = """"
MATHPIX_APPKEY = """"


# DOC2X的PDF解析服务，注册账号并获取API KEY: https://doc2x.noedgeai.com/login
DOC2X_API_KEY = """"


# 自定义API KEY格式
CUSTOM_API_KEY_PATTERN = """"


# Google Gemini API-Key
GEMINI_API_KEY = ''


# HUGGINGFACE的TOKEN，下载LLAMA时起作用 https://huggingface.co/docs/hub/security-tokens
HUGGINGFACE_ACCESS_TOKEN = ""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""


# GROBID服务器地址（填写多个可以均衡负载），用于高质量地读取PDF文档
# 获取方法：复制以下空间https://huggingface.co/spaces/qingxu98/grobid，设为public，然后GROBID_URL = ""https://(你的hf用户名如qingxu98)-(你的填写的空间名如grobid).hf.space""
GROBID_URLS = [
    ""https://qingxu98-grobid.hf.space"",""https://qingxu98-grobid2.hf.space"",""https://qingxu98-grobid3.hf.space"",
    ""https://qingxu98-grobid4.hf.space"",""https://qingxu98-grobid5.hf.space"", ""https://qingxu98-grobid6.hf.space"",
    ""https://qingxu98-grobid7.hf.space"", ""https://qingxu98-grobid8.hf.space"",
]


# Searxng互联网检索服务（这是一个huggingface空间，请前往huggingface复制该空间，然后把自己新的空间地址填在这里）
SEARXNG_URLS = [ f""https://kaletianlre-beardvs{i}dd.hf.space/"" for i in range(1,5) ]


# 是否允许通过自然语言描述修改本页的配置，该功能具有一定的危险性，默认关闭
ALLOW_RESET_CONFIG = False


# 在使用AutoGen插件时，是否使用Docker容器运行代码
AUTOGEN_USE_DOCKER = False


# 临时的上传文件夹位置，请尽量不要修改
PATH_PRIVATE_UPLOAD = ""private_upload""


# 日志文件夹的位置，请尽量不要修改
PATH_LOGGING = ""gpt_log""


# 存储翻译好的arxiv论文的路径，请尽量不要修改
ARXIV_CACHE_DIR = ""gpt_log/arxiv_cache""


# 除了连接OpenAI之外，还有哪些场合允许使用代理，请尽量不要修改
WHEN_TO_USE_PROXY = [""Connect_OpenAI"", ""Download_LLM"", ""Download_Gradio_Theme"", ""Connect_Grobid"",
                     ""Warmup_Modules"", ""Nougat_Download"", ""AutoGen"", ""Connect_OpenAI_Embedding""]


# 启用插件热加载
PLUGIN_HOT_RELOAD = False


# 自定义按钮的最大数量限制
NUM_CUSTOM_BASIC_BTN = 4


# 媒体智能体的服务地址（这是一个huggingface空间，请前往huggingface复制该空间，然后把自己新的空间地址填在这里）
DAAS_SERVER_URLS = [ f""https://niuziniu-biligpt{i}.hf.space/stream"" for i in range(1,5) ]


# 在互联网搜索组件中，负责将搜索结果整理成干净的Markdown
JINA_API_KEY = """"

""""""
--------------- 配置关联关系说明 ---------------

在线大模型配置关联关系示意图
│
├── ""gpt-3.5-turbo"" 等openai模型
│   ├── API_KEY
│   ├── CUSTOM_API_KEY_PATTERN（不常用）
│   ├── API_ORG（不常用）
│   └── API_URL_REDIRECT（不常用）
│
├── ""azure-gpt-3.5"" 等azure模型（单个azure模型，不需要动态切换）
│   ├── API_KEY
│   ├── AZURE_ENDPOINT
│   ├── AZURE_API_KEY
│   ├── AZURE_ENGINE
│   └── API_URL_REDIRECT
│
├── ""azure-gpt-3.5"" 等azure模型（多个azure模型，需要动态切换，高优先级）
│   └── AZURE_CFG_ARRAY
│
├── ""spark"" 星火认知大模型 spark & sparkv2
│   ├── XFYUN_APPID
│   ├── XFYUN_API_SECRET
│   └── XFYUN_API_KEY
│
├── ""claude-3-opus-20240229"" 等claude模型
│   └── ANTHROPIC_API_KEY
│
├── ""stack-claude""
│   ├── SLACK_CLAUDE_BOT_ID
│   └── SLACK_CLAUDE_USER_TOKEN
│
├── ""qianfan"" 百度千帆大模型库
│   ├── BAIDU_CLOUD_QIANFAN_MODEL
│   ├── BAIDU_CLOUD_API_KEY
│   └── BAIDU_CLOUD_SECRET_KEY
│
├── ""glm-4"", ""glm-3-turbo"", ""zhipuai"" 智谱AI大模型
│   └── ZHIPUAI_API_KEY
│
├── ""yi-34b-chat-0205"", ""yi-34b-chat-200k"" 等零一万物(Yi Model)大模型
│   └── YIMODEL_API_KEY
│
├── ""qwen-turbo"" 等通义千问大模型
│   └──  DASHSCOPE_API_KEY
│
├── ""Gemini""
│   └──  GEMINI_API_KEY
│
└── ""one-api-...(max_token=...)"" 用一种更方便的方式接入one-api多模型管理界面
    ├── AVAIL_LLM_MODELS
    ├── API_KEY
    └── API_URL_REDIRECT


本地大模型示意图
│
├── ""chatglm4""
├── ""chatglm3""
├── ""chatglm""
├── ""chatglm_onnx""
├── ""chatglmft""
├── ""internlm""
├── ""moss""
├── ""jittorllms_pangualpha""
├── ""jittorllms_llama""
├── ""deepseekcoder""
├── ""qwen-local""
├──  RWKV的支持见Wiki
└── ""llama2""


用户图形界面布局依赖关系示意图
│
├── CHATBOT_HEIGHT 对话窗的高度
├── CODE_HIGHLIGHT 代码高亮
├── LAYOUT 窗口布局
├── DARK_MODE 暗色模式 / 亮色模式
├── DEFAULT_FN_GROUPS 插件分类默认选项
├── THEME 色彩主题
├── AUTO_CLEAR_TXT 是否在提交时自动清空输入框
├── ADD_WAIFU 加一个live2d装饰
└── ALLOW_RESET_CONFIG 是否允许通过自然语言描述修改本页的配置，该功能具有一定的危险性


插件在线服务配置依赖关系示意图
│
├── 互联网检索
│   └── SEARXNG_URLS
│
├── 语音功能
│   ├── ENABLE_AUDIO
│   ├── ALIYUN_TOKEN
│   ├── ALIYUN_APPKEY
│   ├── ALIYUN_ACCESSKEY
│   └── ALIYUN_SECRET
│
└── PDF文档精准解析
    ├── GROBID_URLS
    ├── MATHPIX_APPID
    └── MATHPIX_APPKEY


""""""
"
ultralytics_yolov5,ultralytics_yolov5_detect.py,23761,https://raw.githubusercontent.com/ultralytics/yolov5/master/detect.py,"# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license
""""""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/LNwODJXcvt4'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlpackage          # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
""""""

import argparse
import csv
import os
import platform
import sys
from pathlib import Path

import torch

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from ultralytics.utils.plotting import Annotator, colors, save_one_box

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from utils.general import (
    LOGGER,
    Profile,
    check_file,
    check_img_size,
    check_imshow,
    check_requirements,
    colorstr,
    cv2,
    increment_path,
    non_max_suppression,
    print_args,
    scale_boxes,
    strip_optimizer,
    xyxy2xywh,
)
from utils.torch_utils import select_device, smart_inference_mode


@smart_inference_mode()
def run(
    weights=ROOT / ""yolov5s.pt"",  # model path or triton URL
    source=ROOT / ""data/images"",  # file/dir/URL/glob/screen/0(webcam)
    data=ROOT / ""data/coco128.yaml"",  # dataset.yaml path
    imgsz=(640, 640),  # inference size (height, width)
    conf_thres=0.25,  # confidence threshold
    iou_thres=0.45,  # NMS IOU threshold
    max_det=1000,  # maximum detections per image
    device="""",  # cuda device, i.e. 0 or 0,1,2,3 or cpu
    view_img=False,  # show results
    save_txt=False,  # save results to *.txt
    save_format=0,  # save boxes coordinates in YOLO format or Pascal-VOC format (0 for YOLO and 1 for Pascal-VOC)
    save_csv=False,  # save results in CSV format
    save_conf=False,  # save confidences in --save-txt labels
    save_crop=False,  # save cropped prediction boxes
    nosave=False,  # do not save images/videos
    classes=None,  # filter by class: --class 0, or --class 0 2 3
    agnostic_nms=False,  # class-agnostic NMS
    augment=False,  # augmented inference
    visualize=False,  # visualize features
    update=False,  # update all models
    project=ROOT / ""runs/detect"",  # save results to project/name
    name=""exp"",  # save results to project/name
    exist_ok=False,  # existing project/name ok, do not increment
    line_thickness=3,  # bounding box thickness (pixels)
    hide_labels=False,  # hide labels
    hide_conf=False,  # hide confidences
    half=False,  # use FP16 half-precision inference
    dnn=False,  # use OpenCV DNN for ONNX inference
    vid_stride=1,  # video frame-rate stride
):
    """"""
    Runs YOLOv5 detection inference on various sources like images, videos, directories, streams, etc.

    Args:
        weights (str | Path): Path to the model weights file or a Triton URL. Default is 'yolov5s.pt'.
        source (str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam
            index. Default is 'data/images'.
        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.
        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).
        conf_thres (float): Confidence threshold for detections. Default is 0.25.
        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.
        max_det (int): Maximum number of detections per image. Default is 1000.
        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the
            best available device.
        view_img (bool): If True, display inference results using OpenCV. Default is False.
        save_txt (bool): If True, save results in a text file. Default is False.
        save_csv (bool): If True, save results in a CSV file. Default is False.
        save_conf (bool): If True, include confidence scores in the saved results. Default is False.
        save_crop (bool): If True, save cropped prediction boxes. Default is False.
        nosave (bool): If True, do not save inference images or videos. Default is False.
        classes (list[int]): List of class indices to filter detections by. Default is None.
        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.
        augment (bool): If True, use augmented inference. Default is False.
        visualize (bool): If True, visualize feature maps. Default is False.
        update (bool): If True, update all models' weights. Default is False.
        project (str | Path): Directory to save results. Default is 'runs/detect'.
        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.
        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is
            False.
        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.
        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.
        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.
        half (bool): If True, use FP16 half-precision inference. Default is False.
        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.
        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.

    Returns:
        None

    Examples:
        ```python
        from ultralytics import run

        # Run inference on an image
        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')

        # Run inference on a video with specific confidence threshold
        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')
        ```
    """"""
    source = str(source)
    save_img = not nosave and not source.endswith("".txt"")  # save inference images
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    is_url = source.lower().startswith((""rtsp://"", ""rtmp://"", ""http://"", ""https://""))
    webcam = source.isnumeric() or source.endswith("".streams"") or (is_url and not is_file)
    screenshot = source.lower().startswith(""screen"")
    if is_url and is_file:
        source = check_file(source)  # download

    # Directories
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
    (save_dir / ""labels"" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

    # Load model
    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    # Dataloader
    bs = 1  # batch_size
    if webcam:
        view_img = check_imshow(warn=True)
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
        bs = len(dataset)
    elif screenshot:
        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)
    else:
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))
    for path, im, im0s, vid_cap, s in dataset:
        with dt[0]:
            im = torch.from_numpy(im).to(model.device)
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim
            if model.xml and im.shape[0] > 1:
                ims = torch.chunk(im, im.shape[0], 0)

        # Inference
        with dt[1]:
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            if model.xml and im.shape[0] > 1:
                pred = None
                for image in ims:
                    if pred is None:
                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)
                    else:
                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)
                pred = [pred, None]
            else:
                pred = model(im, augment=augment, visualize=visualize)
        # NMS
        with dt[2]:
            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Second-stage classifier (optional)
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

        # Define the path for the CSV file
        csv_path = save_dir / ""predictions.csv""

        # Create or append to the CSV file
        def write_to_csv(image_name, prediction, confidence):
            """"""Writes prediction data for an image to a CSV file, appending if the file exists.""""""
            data = {""Image Name"": image_name, ""Prediction"": prediction, ""Confidence"": confidence}
            file_exists = os.path.isfile(csv_path)
            with open(csv_path, mode=""a"", newline="""") as f:
                writer = csv.DictWriter(f, fieldnames=data.keys())
                if not file_exists:
                    writer.writeheader()
                writer.writerow(data)

        # Process predictions
        for i, det in enumerate(pred):  # per image
            seen += 1
            if webcam:  # batch_size >= 1
                p, im0, frame = path[i], im0s[i].copy(), dataset.count
                s += f""{i}: ""
            else:
                p, im0, frame = path, im0s.copy(), getattr(dataset, ""frame"", 0)

            p = Path(p)  # to Path
            save_path = str(save_dir / p.name)  # im.jpg
            txt_path = str(save_dir / ""labels"" / p.stem) + ("""" if dataset.mode == ""image"" else f""_{frame}"")  # im.txt
            s += ""{:g}x{:g} "".format(*im.shape[2:])  # print string
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            imc = im0.copy() if save_crop else im0  # for save_crop
            annotator = Annotator(im0, line_width=line_thickness, example=str(names))
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                for c in det[:, 5].unique():
                    n = (det[:, 5] == c).sum()  # detections per class
                    s += f""{n} {names[int(c)]}{'s' * (n > 1)}, ""  # add to string

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    c = int(cls)  # integer class
                    label = names[c] if hide_conf else f""{names[c]}""
                    confidence = float(conf)
                    confidence_str = f""{confidence:.2f}""

                    if save_csv:
                        write_to_csv(p.name, label, confidence_str)

                    if save_txt:  # Write to file
                        if save_format == 0:
                            coords = (
                                (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()
                            )  # normalized xywh
                        else:
                            coords = (torch.tensor(xyxy).view(1, 4) / gn).view(-1).tolist()  # xyxy
                        line = (cls, *coords, conf) if save_conf else (cls, *coords)  # label format
                        with open(f""{txt_path}.txt"", ""a"") as f:
                            f.write((""%g "" * len(line)).rstrip() % line + ""\n"")

                    if save_img or save_crop or view_img:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (names[c] if hide_conf else f""{names[c]} {conf:.2f}"")
                        annotator.box_label(xyxy, label, color=colors(c, True))
                    if save_crop:
                        save_one_box(xyxy, imc, file=save_dir / ""crops"" / names[c] / f""{p.stem}.jpg"", BGR=True)

            # Stream results
            im0 = annotator.result()
            if view_img:
                if platform.system() == ""Linux"" and p not in windows:
                    windows.append(p)
                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)
                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])
                cv2.imshow(str(p), im0)
                cv2.waitKey(1)  # 1 millisecond

            # Save results (image with detections)
            if save_img:
                if dataset.mode == ""image"":
                    cv2.imwrite(save_path, im0)
                else:  # 'video' or 'stream'
                    if vid_path[i] != save_path:  # new video
                        vid_path[i] = save_path
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            vid_writer[i].release()  # release previous video writer
                        if vid_cap:  # video
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        else:  # stream
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                        save_path = str(Path(save_path).with_suffix("".mp4""))  # force *.mp4 suffix on results videos
                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*""mp4v""), fps, (w, h))
                    vid_writer[i].write(im0)

        # Print time (inference-only)
        LOGGER.info(f""{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1e3:.1f}ms"")

    # Print results
    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image
    LOGGER.info(f""Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}"" % t)
    if save_txt or save_img:
        s = f""\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}"" if save_txt else """"
        LOGGER.info(f""Results saved to {colorstr('bold', save_dir)}{s}"")
    if update:
        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)


def parse_opt():
    """"""
    Parse command-line arguments for YOLOv5 detection, allowing custom inference options and model configurations.

    Args:
        --weights (str | list[str], optional): Model path or Triton URL. Defaults to ROOT / 'yolov5s.pt'.
        --source (str, optional): File/dir/URL/glob/screen/0(webcam). Defaults to ROOT / 'data/images'.
        --data (str, optional): Dataset YAML path. Provides dataset configuration information.
        --imgsz (list[int], optional): Inference size (height, width). Defaults to [640].
        --conf-thres (float, optional): Confidence threshold. Defaults to 0.25.
        --iou-thres (float, optional): NMS IoU threshold. Defaults to 0.45.
        --max-det (int, optional): Maximum number of detections per image. Defaults to 1000.
        --device (str, optional): CUDA device, i.e., '0' or '0,1,2,3' or 'cpu'. Defaults to """".
        --view-img (bool, optional): Flag to display results. Defaults to False.
        --save-txt (bool, optional): Flag to save results to *.txt files. Defaults to False.
        --save-csv (bool, optional): Flag to save results in CSV format. Defaults to False.
        --save-conf (bool, optional): Flag to save confidences in labels saved via --save-txt. Defaults to False.
        --save-crop (bool, optional): Flag to save cropped prediction boxes. Defaults to False.
        --nosave (bool, optional): Flag to prevent saving images/videos. Defaults to False.
        --classes (list[int], optional): List of classes to filter results by, e.g., '--classes 0 2 3'. Defaults to None.
        --agnostic-nms (bool, optional): Flag for class-agnostic NMS. Defaults to False.
        --augment (bool, optional): Flag for augmented inference. Defaults to False.
        --visualize (bool, optional): Flag for visualizing features. Defaults to False.
        --update (bool, optional): Flag to update all models in the model directory. Defaults to False.
        --project (str, optional): Directory to save results. Defaults to ROOT / 'runs/detect'.
        --name (str, optional): Sub-directory name for saving results within --project. Defaults to 'exp'.
        --exist-ok (bool, optional): Flag to allow overwriting if the project/name already exists. Defaults to False.
        --line-thickness (int, optional): Thickness (in pixels) of bounding boxes. Defaults to 3.
        --hide-labels (bool, optional): Flag to hide labels in the output. Defaults to False.
        --hide-conf (bool, optional): Flag to hide confidences in the output. Defaults to False.
        --half (bool, optional): Flag to use FP16 half-precision inference. Defaults to False.
        --dnn (bool, optional): Flag to use OpenCV DNN for ONNX inference. Defaults to False.
        --vid-stride (int, optional): Video frame-rate stride, determining the number of frames to skip in between
            consecutive frames. Defaults to 1.

    Returns:
        argparse.Namespace: Parsed command-line arguments as an argparse.Namespace object.

    Example:
        ```python
        from ultralytics import YOLOv5
        args = YOLOv5.parse_opt()
        ```
    """"""
    parser = argparse.ArgumentParser()
    parser.add_argument(""--weights"", nargs=""+"", type=str, default=ROOT / ""yolov5s.pt"", help=""model path or triton URL"")
    parser.add_argument(""--source"", type=str, default=ROOT / ""data/images"", help=""file/dir/URL/glob/screen/0(webcam)"")
    parser.add_argument(""--data"", type=str, default=ROOT / ""data/coco128.yaml"", help=""(optional) dataset.yaml path"")
    parser.add_argument(""--imgsz"", ""--img"", ""--img-size"", nargs=""+"", type=int, default=[640], help=""inference size h,w"")
    parser.add_argument(""--conf-thres"", type=float, default=0.25, help=""confidence threshold"")
    parser.add_argument(""--iou-thres"", type=float, default=0.45, help=""NMS IoU threshold"")
    parser.add_argument(""--max-det"", type=int, default=1000, help=""maximum detections per image"")
    parser.add_argument(""--device"", default="""", help=""cuda device, i.e. 0 or 0,1,2,3 or cpu"")
    parser.add_argument(""--view-img"", action=""store_true"", help=""show results"")
    parser.add_argument(""--save-txt"", action=""store_true"", help=""save results to *.txt"")
    parser.add_argument(
        ""--save-format"",
        type=int,
        default=0,
        help=""whether to save boxes coordinates in YOLO format or Pascal-VOC format when save-txt is True, 0 for YOLO and 1 for Pascal-VOC"",
    )
    parser.add_argument(""--save-csv"", action=""store_true"", help=""save results in CSV format"")
    parser.add_argument(""--save-conf"", action=""store_true"", help=""save confidences in --save-txt labels"")
    parser.add_argument(""--save-crop"", action=""store_true"", help=""save cropped prediction boxes"")
    parser.add_argument(""--nosave"", action=""store_true"", help=""do not save images/videos"")
    parser.add_argument(""--classes"", nargs=""+"", type=int, help=""filter by class: --classes 0, or --classes 0 2 3"")
    parser.add_argument(""--agnostic-nms"", action=""store_true"", help=""class-agnostic NMS"")
    parser.add_argument(""--augment"", action=""store_true"", help=""augmented inference"")
    parser.add_argument(""--visualize"", action=""store_true"", help=""visualize features"")
    parser.add_argument(""--update"", action=""store_true"", help=""update all models"")
    parser.add_argument(""--project"", default=ROOT / ""runs/detect"", help=""save results to project/name"")
    parser.add_argument(""--name"", default=""exp"", help=""save results to project/name"")
    parser.add_argument(""--exist-ok"", action=""store_true"", help=""existing project/name ok, do not increment"")
    parser.add_argument(""--line-thickness"", default=3, type=int, help=""bounding box thickness (pixels)"")
    parser.add_argument(""--hide-labels"", default=False, action=""store_true"", help=""hide labels"")
    parser.add_argument(""--hide-conf"", default=False, action=""store_true"", help=""hide confidences"")
    parser.add_argument(""--half"", action=""store_true"", help=""use FP16 half-precision inference"")
    parser.add_argument(""--dnn"", action=""store_true"", help=""use OpenCV DNN for ONNX inference"")
    parser.add_argument(""--vid-stride"", type=int, default=1, help=""video frame-rate stride"")
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt


def main(opt):
    """"""
    Executes YOLOv5 model inference based on provided command-line arguments, validating dependencies before running.

    Args:
        opt (argparse.Namespace): Command-line arguments for YOLOv5 detection. See function `parse_opt` for details.

    Returns:
        None

    Note:
        This function performs essential pre-execution checks and initiates the YOLOv5 detection process based on user-specified
        options. Refer to the usage guide and examples for more information about different sources and formats at:
        https://github.com/ultralytics/ultralytics

    Example usage:

    ```python
    if __name__ == ""__main__"":
        opt = parse_opt()
        main(opt)
    ```
    """"""
    check_requirements(ROOT / ""requirements.txt"", exclude=(""tensorboard"", ""thop""))
    run(**vars(opt))


if __name__ == ""__main__"":
    opt = parse_opt()
    main(opt)
"
ultralytics_yolov5,ultralytics_yolov5_hubconf.py,23994,https://raw.githubusercontent.com/ultralytics/yolov5/master/hubconf.py,"# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license
""""""
PyTorch Hub models https://pytorch.org/hub/ultralytics_yolov5.

Usage:
    import torch
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # official model
    model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s')  # from branch
    model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')  # custom/local model
    model = torch.hub.load('.', 'custom', 'yolov5s.pt', source='local')  # local repo
""""""

import torch


def _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):
    """"""
    Creates or loads a YOLOv5 model, with options for pretrained weights and model customization.

    Args:
        name (str): Model name (e.g., 'yolov5s') or path to the model checkpoint (e.g., 'path/to/best.pt').
        pretrained (bool, optional): If True, loads pretrained weights into the model. Defaults to True.
        channels (int, optional): Number of input channels the model expects. Defaults to 3.
        classes (int, optional): Number of classes the model is expected to detect. Defaults to 80.
        autoshape (bool, optional): If True, applies the YOLOv5 .autoshape() wrapper for various input formats. Defaults to True.
        verbose (bool, optional): If True, prints detailed information during the model creation/loading process. Defaults to True.
        device (str | torch.device | None, optional): Device to use for model parameters (e.g., 'cpu', 'cuda'). If None, selects
            the best available device. Defaults to None.

    Returns:
        (DetectMultiBackend | AutoShape): The loaded YOLOv5 model, potentially wrapped with AutoShape if specified.

    Examples:
        ```python
        import torch
        from ultralytics import _create

        # Load an official YOLOv5s model with pretrained weights
        model = _create('yolov5s')

        # Load a custom model from a local checkpoint
        model = _create('path/to/custom_model.pt', pretrained=False)

        # Load a model with specific input channels and classes
        model = _create('yolov5s', channels=1, classes=10)
        ```

    Notes:
        For more information on model loading and customization, visit the
        [YOLOv5 PyTorch Hub Documentation](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading).
    """"""
    from pathlib import Path

    from models.common import AutoShape, DetectMultiBackend
    from models.experimental import attempt_load
    from models.yolo import ClassificationModel, DetectionModel, SegmentationModel
    from utils.downloads import attempt_download
    from utils.general import LOGGER, ROOT, check_requirements, intersect_dicts, logging
    from utils.torch_utils import select_device

    if not verbose:
        LOGGER.setLevel(logging.WARNING)
    check_requirements(ROOT / ""requirements.txt"", exclude=(""opencv-python"", ""tensorboard"", ""thop""))
    name = Path(name)
    path = name.with_suffix("".pt"") if name.suffix == """" and not name.is_dir() else name  # checkpoint path
    try:
        device = select_device(device)
        if pretrained and channels == 3 and classes == 80:
            try:
                model = DetectMultiBackend(path, device=device, fuse=autoshape)  # detection model
                if autoshape:
                    if model.pt and isinstance(model.model, ClassificationModel):
                        LOGGER.warning(
                            ""WARNING ⚠️ YOLOv5 ClassificationModel is not yet AutoShape compatible. ""
                            ""You must pass torch tensors in BCHW to this model, i.e. shape(1,3,224,224).""
                        )
                    elif model.pt and isinstance(model.model, SegmentationModel):
                        LOGGER.warning(
                            ""WARNING ⚠️ YOLOv5 SegmentationModel is not yet AutoShape compatible. ""
                            ""You will not be able to run inference with this model.""
                        )
                    else:
                        model = AutoShape(model)  # for file/URI/PIL/cv2/np inputs and NMS
            except Exception:
                model = attempt_load(path, device=device, fuse=False)  # arbitrary model
        else:
            cfg = list((Path(__file__).parent / ""models"").rglob(f""{path.stem}.yaml""))[0]  # model.yaml path
            model = DetectionModel(cfg, channels, classes)  # create model
            if pretrained:
                ckpt = torch.load(attempt_download(path), map_location=device)  # load
                csd = ckpt[""model""].float().state_dict()  # checkpoint state_dict as FP32
                csd = intersect_dicts(csd, model.state_dict(), exclude=[""anchors""])  # intersect
                model.load_state_dict(csd, strict=False)  # load
                if len(ckpt[""model""].names) == classes:
                    model.names = ckpt[""model""].names  # set class names attribute
        if not verbose:
            LOGGER.setLevel(logging.INFO)  # reset to default
        return model.to(device)

    except Exception as e:
        help_url = ""https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading""
        s = f""{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.""
        raise Exception(s) from e


def custom(path=""path/to/model.pt"", autoshape=True, _verbose=True, device=None):
    """"""
    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.

    Args:
        path (str): Path to the custom model file (e.g., 'path/to/model.pt').
        autoshape (bool): Apply YOLOv5 .autoshape() wrapper to model if True, enabling compatibility with various input
            types (default is True).
        _verbose (bool): If True, prints all informational messages to the screen; otherwise, operates silently
            (default is True).
        device (str | torch.device | None): Device to load the model on, e.g., 'cpu', 'cuda', torch.device('cuda:0'), etc.
            (default is None, which automatically selects the best available device).

    Returns:
        torch.nn.Module: A YOLOv5 model loaded with the specified parameters.

    Notes:
        For more details on loading models from PyTorch Hub:
        https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading

    Examples:
        ```python
        # Load model from a given path with autoshape enabled on the best available device
        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')

        # Load model from a local path without autoshape on the CPU device
        model = torch.hub.load('.', 'custom', 'yolov5s.pt', source='local', autoshape=False, device='cpu')
        ```
    """"""
    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)


def yolov5n(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Instantiates the YOLOv5-nano model with options for pretraining, input channels, class count, autoshaping,
    verbosity, and device.

    Args:
        pretrained (bool): If True, loads pretrained weights into the model. Defaults to True.
        channels (int): Number of input channels for the model. Defaults to 3.
        classes (int): Number of classes for object detection. Defaults to 80.
        autoshape (bool): If True, applies the YOLOv5 .autoshape() wrapper to the model for various formats (file/URI/PIL/
            cv2/np) and non-maximum suppression (NMS) during inference. Defaults to True.
        _verbose (bool): If True, prints detailed information to the screen. Defaults to True.
        device (str | torch.device | None): Specifies the device to use for model computation. If None, uses the best device
            available (i.e., GPU if available, otherwise CPU). Defaults to None.

    Returns:
        DetectionModel | ClassificationModel | SegmentationModel: The instantiated YOLOv5-nano model, potentially with
            pretrained weights and autoshaping applied.

    Notes:
        For further details on loading models from PyTorch Hub, refer to [PyTorch Hub models](https://pytorch.org/hub/
        ultralytics_yolov5).

    Examples:
        ```python
        import torch
        from ultralytics import yolov5n

        # Load the YOLOv5-nano model with defaults
        model = yolov5n()

        # Load the YOLOv5-nano model with a specific device
        model = yolov5n(device='cuda')
        ```
    """"""
    return _create(""yolov5n"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Create a YOLOv5-small (yolov5s) model with options for pretraining, input channels, class count, autoshaping,
    verbosity, and device configuration.

    Args:
        pretrained (bool, optional): Flag to load pretrained weights into the model. Defaults to True.
        channels (int, optional): Number of input channels. Defaults to 3.
        classes (int, optional): Number of model classes. Defaults to 80.
        autoshape (bool, optional): Whether to wrap the model with YOLOv5's .autoshape() for handling various input formats.
            Defaults to True.
        _verbose (bool, optional): Flag to print detailed information regarding model loading. Defaults to True.
        device (str | torch.device | None, optional): Device to use for model computation, can be 'cpu', 'cuda', or
            torch.device instances. If None, automatically selects the best available device. Defaults to None.

    Returns:
        torch.nn.Module: The YOLOv5-small model configured and loaded according to the specified parameters.

    Example:
        ```python
        import torch

        # Load the official YOLOv5-small model with pretrained weights
        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

        # Load the YOLOv5-small model from a specific branch
        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s')

        # Load a custom YOLOv5-small model from a local checkpoint
        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')

        # Load a local YOLOv5-small model specifying source as local repository
        model = torch.hub.load('.', 'custom', 'yolov5s.pt', source='local')
        ```

    Notes:
        For more details on model loading and customization, visit
        the [YOLOv5 PyTorch Hub Documentation](https://pytorch.org/hub/ultralytics_yolov5).
    """"""
    return _create(""yolov5s"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Instantiates the YOLOv5-medium model with customizable pretraining, channel count, class count, autoshaping,
    verbosity, and device.

    Args:
        pretrained (bool, optional): Whether to load pretrained weights into the model. Default is True.
        channels (int, optional): Number of input channels. Default is 3.
        classes (int, optional): Number of model classes. Default is 80.
        autoshape (bool, optional): Apply YOLOv5 .autoshape() wrapper to the model for handling various input formats.
            Default is True.
        _verbose (bool, optional): Whether to print detailed information to the screen. Default is True.
        device (str | torch.device | None, optional): Device specification to use for model parameters (e.g., 'cpu', 'cuda').
            Default is None.

    Returns:
        torch.nn.Module: The instantiated YOLOv5-medium model.

    Usage Example:
        ```python
        import torch

        model = torch.hub.load('ultralytics/yolov5', 'yolov5m')  # Load YOLOv5-medium from Ultralytics repository
        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5m')  # Load from the master branch
        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m.pt')  # Load a custom/local YOLOv5-medium model
        model = torch.hub.load('.', 'custom', 'yolov5m.pt', source='local')  # Load from a local repository
        ```

    For more information, visit https://pytorch.org/hub/ultralytics_yolov5.
    """"""
    return _create(""yolov5m"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Creates YOLOv5-large model with options for pretraining, channels, classes, autoshaping, verbosity, and device
    selection.

    Args:
        pretrained (bool): Load pretrained weights into the model. Default is True.
        channels (int): Number of input channels. Default is 3.
        classes (int): Number of model classes. Default is 80.
        autoshape (bool): Apply YOLOv5 .autoshape() wrapper to model. Default is True.
        _verbose (bool): Print all information to screen. Default is True.
        device (str | torch.device | None): Device to use for model parameters, e.g., 'cpu', 'cuda', or a torch.device instance.
            Default is None.

    Returns:
        YOLOv5 model (torch.nn.Module): The YOLOv5-large model instantiated with specified configurations and possibly
        pretrained weights.

    Examples:
        ```python
        import torch
        model = torch.hub.load('ultralytics/yolov5', 'yolov5l')
        ```

    Notes:
        For additional details, refer to the PyTorch Hub models documentation:
        https://pytorch.org/hub/ultralytics_yolov5
    """"""
    return _create(""yolov5l"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Perform object detection using the YOLOv5-xlarge model with options for pretraining, input channels, class count,
    autoshaping, verbosity, and device specification.

    Args:
        pretrained (bool): If True, loads pretrained weights into the model. Defaults to True.
        channels (int): Number of input channels for the model. Defaults to 3.
        classes (int): Number of model classes for object detection. Defaults to 80.
        autoshape (bool): If True, applies the YOLOv5 .autoshape() wrapper for handling different input formats. Defaults to
            True.
        _verbose (bool): If True, prints detailed information during model loading. Defaults to True.
        device (str | torch.device | None): Device specification for computing the model, e.g., 'cpu', 'cuda:0', torch.device('cuda').
            Defaults to None.

    Returns:
        torch.nn.Module: The YOLOv5-xlarge model loaded with the specified parameters, optionally with pretrained weights and
        autoshaping applied.

    Example:
        ```python
        import torch
        model = torch.hub.load('ultralytics/yolov5', 'yolov5x')
        ```

    For additional details, refer to the official YOLOv5 PyTorch Hub models documentation:
    https://pytorch.org/hub/ultralytics_yolov5
    """"""
    return _create(""yolov5x"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Creates YOLOv5-nano-P6 model with options for pretraining, channels, classes, autoshaping, verbosity, and device.

    Args:
        pretrained (bool, optional): If True, loads pretrained weights into the model. Default is True.
        channels (int, optional): Number of input channels. Default is 3.
        classes (int, optional): Number of model classes. Default is 80.
        autoshape (bool, optional): If True, applies the YOLOv5 .autoshape() wrapper to the model. Default is True.
        _verbose (bool, optional): If True, prints all information to screen. Default is True.
        device (str | torch.device | None, optional): Device to use for model parameters. Can be 'cpu', 'cuda', or None.
            Default is None.

    Returns:
        torch.nn.Module: YOLOv5-nano-P6 model loaded with the specified configurations.

    Example:
        ```python
        import torch
        model = yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device='cuda')
        ```

    Notes:
        For more information on PyTorch Hub models, visit: https://pytorch.org/hub/ultralytics_yolov5
    """"""
    return _create(""yolov5n6"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Instantiate the YOLOv5-small-P6 model with options for pretraining, input channels, number of classes, autoshaping,
    verbosity, and device selection.

    Args:
        pretrained (bool): If True, loads pretrained weights. Default is True.
        channels (int): Number of input channels. Default is 3.
        classes (int): Number of object detection classes. Default is 80.
        autoshape (bool): If True, applies YOLOv5 .autoshape() wrapper to the model, allowing for varied input formats.
            Default is True.
        _verbose (bool): If True, prints detailed information during model loading. Default is True.
        device (str | torch.device | None): Device specification for model parameters (e.g., 'cpu', 'cuda', or torch.device).
            Default is None, which selects an available device automatically.

    Returns:
        torch.nn.Module: The YOLOv5-small-P6 model instance.

    Usage:
        ```python
        import torch

        model = torch.hub.load('ultralytics/yolov5', 'yolov5s6')
        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s6')  # load from a specific branch
        model = torch.hub.load('ultralytics/yolov5', 'custom', 'path/to/yolov5s6.pt')  # custom/local model
        model = torch.hub.load('.', 'custom', 'path/to/yolov5s6.pt', source='local')  # local repo model
        ```

    Notes:
        - For more information, refer to the PyTorch Hub models documentation at https://pytorch.org/hub/ultralytics_yolov5

    Raises:
        Exception: If there is an error during model creation or loading, with a suggestion to visit the YOLOv5
            tutorials for help.
    """"""
    return _create(""yolov5s6"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Create YOLOv5-medium-P6 model with options for pretraining, channel count, class count, autoshaping, verbosity, and
    device.

    Args:
        pretrained (bool): If True, loads pretrained weights. Default is True.
        channels (int): Number of input channels. Default is 3.
        classes (int): Number of model classes. Default is 80.
        autoshape (bool): Apply YOLOv5 .autoshape() wrapper to the model for file/URI/PIL/cv2/np inputs and NMS.
            Default is True.
        _verbose (bool): If True, prints detailed information to the screen. Default is True.
        device (str | torch.device | None): Device to use for model parameters. Default is None, which uses the
            best available device.

    Returns:
        torch.nn.Module: The YOLOv5-medium-P6 model.

    Refer to the PyTorch Hub models documentation: https://pytorch.org/hub/ultralytics_yolov5 for additional details.

    Example:
        ```python
        import torch

        # Load YOLOv5-medium-P6 model
        model = torch.hub.load('ultralytics/yolov5', 'yolov5m6')
        ```

    Notes:
        - The model can be loaded with pre-trained weights for better performance on specific tasks.
        - The autoshape feature simplifies input handling by allowing various popular data formats.
    """"""
    return _create(""yolov5m6"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Instantiate the YOLOv5-large-P6 model with options for pretraining, channel and class counts, autoshaping,
    verbosity, and device selection.

    Args:
        pretrained (bool, optional): If True, load pretrained weights into the model. Default is True.
        channels (int, optional): Number of input channels. Default is 3.
        classes (int, optional): Number of model classes. Default is 80.
        autoshape (bool, optional): If True, apply YOLOv5 .autoshape() wrapper to the model for input flexibility. Default is True.
        _verbose (bool, optional): If True, print all information to the screen. Default is True.
        device (str | torch.device | None, optional): Device to use for model parameters, e.g., 'cpu', 'cuda', or torch.device.
            If None, automatically selects the best available device. Default is None.

    Returns:
        torch.nn.Module: The instantiated YOLOv5-large-P6 model.

    Example:
        ```python
        import torch
        model = torch.hub.load('ultralytics/yolov5', 'yolov5l6')  # official model
        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5l6')  # from specific branch
        model = torch.hub.load('ultralytics/yolov5', 'custom', 'path/to/yolov5l6.pt')  # custom/local model
        model = torch.hub.load('.', 'custom', 'path/to/yolov5l6.pt', source='local')  # local repository
        ```

    Note:
        Refer to [PyTorch Hub Documentation](https://pytorch.org/hub/ultralytics_yolov5) for additional usage instructions.
    """"""
    return _create(""yolov5l6"", pretrained, channels, classes, autoshape, _verbose, device)


def yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):
    """"""
    Creates the YOLOv5-xlarge-P6 model with options for pretraining, number of input channels, class count, autoshaping,
    verbosity, and device selection.

    Args:
        pretrained (bool): If True, loads pretrained weights into the model. Default is True.
        channels (int): Number of input channels. Default is 3.
        classes (int): Number of model classes. Default is 80.
        autoshape (bool): If True, applies YOLOv5 .autoshape() wrapper to the model. Default is True.
        _verbose (bool): If True, prints all information to the screen. Default is True.
        device (str | torch.device | None): Device to use for model parameters, can be a string, torch.device object, or
            None for default device selection. Default is None.

    Returns:
        torch.nn.Module: The instantiated YOLOv5-xlarge-P6 model.

    Example:
        ```python
        import torch
        model = torch.hub.load('ultralytics/yolov5', 'yolov5x6')  # load the YOLOv5-xlarge-P6 model
        ```

    Note:
        For more information on YOLOv5 models, visit the official documentation:
        https://docs.ultralytics.com/yolov5
    """"""
    return _create(""yolov5x6"", pretrained, channels, classes, autoshape, _verbose, device)


if __name__ == ""__main__"":
    import argparse
    from pathlib import Path

    import numpy as np
    from PIL import Image

    from utils.general import cv2, print_args

    # Argparser
    parser = argparse.ArgumentParser()
    parser.add_argument(""--model"", type=str, default=""yolov5s"", help=""model name"")
    opt = parser.parse_args()
    print_args(vars(opt))

    # Model
    model = _create(name=opt.model, pretrained=True, channels=3, classes=80, autoshape=True, verbose=True)
    # model = custom(path='path/to/model.pt')  # custom

    # Images
    imgs = [
        ""data/images/zidane.jpg"",  # filename
        Path(""data/images/zidane.jpg""),  # Path
        ""https://ultralytics.com/images/zidane.jpg"",  # URI
        cv2.imread(""data/images/bus.jpg"")[:, :, ::-1],  # OpenCV
        Image.open(""data/images/bus.jpg""),  # PIL
        np.zeros((320, 640, 3)),
    ]  # numpy

    # Inference
    results = model(imgs, size=320)  # batched inference

    # Results
    results.print()
    results.save()
"
oobabooga_text-generation-webui,oobabooga_text-generation-webui_download-model.py,16824,https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/download-model.py,"'''
Downloads models from Hugging Face to models/username_modelname.

Example:
python download-model.py facebook/opt-1.3b

'''

import argparse
import base64
import datetime
import hashlib
import json
import os
import re
import sys
from multiprocessing import Array
from pathlib import Path
from time import sleep

import requests
import tqdm
from requests.adapters import HTTPAdapter
from requests.exceptions import ConnectionError, RequestException, Timeout
from tqdm.contrib.concurrent import thread_map

base = os.environ.get(""HF_ENDPOINT"") or ""https://huggingface.co""


class ModelDownloader:
    def __init__(self, max_retries=7):
        self.max_retries = max_retries
        self.session = self.get_session()
        self._progress_bar_slots = None

    def get_session(self):
        session = requests.Session()
        if self.max_retries:
            session.mount('https://cdn-lfs.huggingface.co', HTTPAdapter(max_retries=self.max_retries))
            session.mount('https://huggingface.co', HTTPAdapter(max_retries=self.max_retries))

        if os.getenv('HF_USER') is not None and os.getenv('HF_PASS') is not None:
            session.auth = (os.getenv('HF_USER'), os.getenv('HF_PASS'))

        try:
            from huggingface_hub import get_token
            token = get_token()
        except ImportError:
            token = os.getenv(""HF_TOKEN"")

        if token is not None:
            session.headers = {'authorization': f'Bearer {token}'}

        return session

    def sanitize_model_and_branch_names(self, model, branch):
        if model[-1] == '/':
            model = model[:-1]

        if model.startswith(base + '/'):
            model = model[len(base) + 1:]

        model_parts = model.split("":"")
        model = model_parts[0] if len(model_parts) > 0 else model
        branch = model_parts[1] if len(model_parts) > 1 else branch

        if branch is None:
            branch = ""main""
        else:
            pattern = re.compile(r""^[a-zA-Z0-9._-]+$"")
            if not pattern.match(branch):
                raise ValueError(
                    ""Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed."")

        return model, branch

    def get_download_links_from_huggingface(self, model, branch, text_only=False, specific_file=None, exclude_pattern=None):
        session = self.session
        page = f""/api/models/{model}/tree/{branch}""
        cursor = b""""

        links = []
        sha256 = []
        classifications = []
        has_pytorch = False
        has_pt = False
        has_gguf = False
        has_safetensors = False
        is_lora = False
        while True:
            url = f""{base}{page}"" + (f""?cursor={cursor.decode()}"" if cursor else """")
            r = session.get(url, timeout=10)
            r.raise_for_status()
            content = r.content

            dict = json.loads(content)
            if len(dict) == 0:
                break

            for i in range(len(dict)):
                fname = dict[i]['path']
                if specific_file not in [None, ''] and fname != specific_file:
                    continue

                # Exclude files matching the exclude pattern
                if exclude_pattern is not None and re.match(exclude_pattern, fname):
                    continue

                if not is_lora and fname.endswith(('adapter_config.json', 'adapter_model.bin')):
                    is_lora = True

                is_pytorch = re.match(r""(pytorch|adapter|gptq)_model.*\.bin"", fname)
                is_safetensors = re.match(r"".*\.safetensors"", fname)
                is_pt = re.match(r"".*\.pt"", fname)
                is_gguf = re.match(r"".*\.gguf"", fname)
                is_tiktoken = re.match(r"".*\.tiktoken"", fname)
                is_tokenizer = re.match(r""(tokenizer|ice|spiece).*\.model"", fname) or is_tiktoken
                is_text = re.match(r"".*\.(txt|json|py|md)"", fname) or is_tokenizer
                if any((is_pytorch, is_safetensors, is_pt, is_gguf, is_tokenizer, is_text)):
                    if 'lfs' in dict[i]:
                        sha256.append([fname, dict[i]['lfs']['oid']])

                    if is_text:
                        links.append(f""{base}/{model}/resolve/{branch}/{fname}"")
                        classifications.append('text')
                        continue

                    if not text_only:
                        links.append(f""{base}/{model}/resolve/{branch}/{fname}"")
                        if is_safetensors:
                            has_safetensors = True
                            classifications.append('safetensors')
                        elif is_pytorch:
                            has_pytorch = True
                            classifications.append('pytorch')
                        elif is_pt:
                            has_pt = True
                            classifications.append('pt')
                        elif is_gguf:
                            has_gguf = True
                            classifications.append('gguf')

            cursor = base64.b64encode(f'{{""file_name"":""{dict[-1][""path""]}""}}'.encode()) + b':50'
            cursor = base64.b64encode(cursor)
            cursor = cursor.replace(b'=', b'%3D')

        # If both pytorch and safetensors are available, download safetensors only
        # Also if GGUF and safetensors are available, download only safetensors
        if (has_pytorch or has_pt or has_gguf) and has_safetensors:
            has_gguf = False
            for i in range(len(classifications) - 1, -1, -1):
                if classifications[i] in ['pytorch', 'pt', 'gguf']:
                    links.pop(i)

        # For GGUF, try to download only the Q4_K_M if no specific file is specified.
        if has_gguf and specific_file is None:
            has_q4km = False
            for i in range(len(classifications) - 1, -1, -1):
                if 'q4_k_m' in links[i].lower():
                    has_q4km = True

            if has_q4km:
                for i in range(len(classifications) - 1, -1, -1):
                    if 'q4_k_m' not in links[i].lower():
                        links.pop(i)
            else:
                for i in range(len(classifications) - 1, -1, -1):
                    if links[i].lower().endswith('.gguf'):
                        links.pop(i)

        is_llamacpp = has_gguf and specific_file is not None
        return links, sha256, is_lora, is_llamacpp

    def get_output_folder(self, model, branch, is_lora, is_llamacpp=False, model_dir=None):
        if model_dir:
            base_folder = model_dir
        else:
            base_folder = 'models' if not is_lora else 'loras'

        # If the model is of type GGUF, save directly in the base_folder
        if is_llamacpp:
            return Path(base_folder)

        output_folder = f""{'_'.join(model.split('/')[-2:])}""
        if branch != 'main':
            output_folder += f'_{branch}'

        output_folder = Path(base_folder) / output_folder
        return output_folder

    @property
    def progress_bar_slots(self):
        if self._progress_bar_slots is None:
            raise RuntimeError(""Progress bar slots not initialized. Start download threads first."")

        return self._progress_bar_slots

    def initialize_progress_bar_slots(self, num_threads):
        self._progress_bar_slots = Array(""B"", [0] * num_threads)

    def get_progress_bar_position(self):
        with self.progress_bar_slots.get_lock():
            for i in range(len(self.progress_bar_slots)):
                if self.progress_bar_slots[i] == 0:
                    self.progress_bar_slots[i] = 1
                    return i

        return 0  # fallback

    def release_progress_bar_position(self, slot):
        with self.progress_bar_slots.get_lock():
            self.progress_bar_slots[slot] = 0

    def get_single_file(self, url, output_folder, start_from_scratch=False):
        filename = Path(url.rsplit('/', 1)[1])
        output_path = output_folder / filename
        progress_bar_position = self.get_progress_bar_position()

        max_retries = self.max_retries
        attempt = 0
        try:
            while attempt < max_retries:
                attempt += 1
                session = self.session
                headers = {}
                mode = 'wb'

                try:
                    if output_path.exists() and not start_from_scratch:
                        # Resume download
                        r = session.get(url, stream=True, timeout=20)
                        total_size = int(r.headers.get('content-length', 0))
                        if output_path.stat().st_size >= total_size:
                            return

                        headers = {'Range': f'bytes={output_path.stat().st_size}-'}
                        mode = 'ab'

                    with session.get(url, stream=True, headers=headers, timeout=30) as r:
                        r.raise_for_status()  # If status is not 2xx, raise an error
                        total_size = int(r.headers.get('content-length', 0))
                        block_size = 1024 * 1024  # 1MB

                        filename_str = str(filename)  # Convert PosixPath to string if necessary

                        tqdm_kwargs = {
                            'total': total_size,
                            'unit': 'B',
                            'unit_scale': True,
                            'unit_divisor': 1024,
                            'bar_format': '{desc}{percentage:3.0f}%|{bar:50}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
                            'desc': f""{filename_str}: "",
                            'position': progress_bar_position,
                            'leave': False
                        }

                        if 'COLAB_GPU' in os.environ:
                            tqdm_kwargs.update({
                                'position': 0,
                                'leave': True
                            })

                        with open(output_path, mode) as f:
                            with tqdm.tqdm(**tqdm_kwargs) as t:
                                count = 0
                                for data in r.iter_content(block_size):
                                    f.write(data)
                                    t.update(len(data))
                                    if total_size != 0 and self.progress_bar is not None:
                                        count += len(data)
                                        self.progress_bar(float(count) / float(total_size), f""{filename_str}"")

                        break  # Exit loop if successful
                except (RequestException, ConnectionError, Timeout) as e:
                    print(f""Error downloading {filename}: {e}."")
                    print(f""That was attempt {attempt}/{max_retries}."", end=' ')
                    if attempt < max_retries:
                        print(f""Retry begins in {2 ** attempt} seconds."")
                        sleep(2 ** attempt)
                    else:
                        print(""Failed to download after the maximum number of attempts."")
        finally:
            self.release_progress_bar_position(progress_bar_position)

    def start_download_threads(self, file_list, output_folder, start_from_scratch=False, threads=4):
        self.initialize_progress_bar_slots(threads)
        tqdm.tqdm.set_lock(tqdm.tqdm.get_lock())
        try:
            thread_map(
                lambda url: self.get_single_file(url, output_folder, start_from_scratch=start_from_scratch),
                file_list,
                max_workers=threads,
                disable=True
            )
        finally:
            print(f""\nDownload of {len(file_list)} files to {output_folder} completed."")

    def download_model_files(self, model, branch, links, sha256, output_folder, progress_bar=None, start_from_scratch=False, threads=4, specific_file=None, is_llamacpp=False):
        self.progress_bar = progress_bar

        # Create the folder and writing the metadata
        output_folder.mkdir(parents=True, exist_ok=True)

        if not is_llamacpp:
            metadata = f'url: https://huggingface.co/{model}\n' \
                       f'branch: {branch}\n' \
                       f'download date: {datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")}\n'

            sha256_str = '\n'.join([f'    {item[1]} {item[0]}' for item in sha256])
            if sha256_str:
                metadata += f'sha256sum:\n{sha256_str}'

            metadata += '\n'
            (output_folder / 'huggingface-metadata.txt').write_text(metadata)

        if specific_file:
            print(f""Downloading {specific_file} to {output_folder}"")
        else:
            print(f""Downloading the model to {output_folder}"")

        self.start_download_threads(links, output_folder, start_from_scratch=start_from_scratch, threads=threads)

    def check_model_files(self, model, branch, links, sha256, output_folder):
        # Validate the checksums
        validated = True
        for i in range(len(sha256)):
            fpath = (output_folder / sha256[i][0])

            if not fpath.exists():
                print(f""The following file is missing: {fpath}"")
                validated = False
                continue

            with open(output_folder / sha256[i][0], ""rb"") as f:
                bytes = f.read()
                file_hash = hashlib.sha256(bytes).hexdigest()
                if file_hash != sha256[i][1]:
                    print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')
                    validated = False
                else:
                    print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')

        if validated:
            print('[+] Validated checksums of all model files!')
        else:
            print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('MODEL', type=str, default=None, nargs='?')
    parser.add_argument('--branch', type=str, default='main', help='Name of the Git branch to download from.')
    parser.add_argument('--threads', type=int, default=4, help='Number of files to download simultaneously.')
    parser.add_argument('--text-only', action='store_true', help='Only download text files (txt/json).')
    parser.add_argument('--specific-file', type=str, default=None, help='Name of the specific file to download (if not provided, downloads all).')
    parser.add_argument('--exclude-pattern', type=str, default=None, help='Regex pattern to exclude files from download.')
    parser.add_argument('--output', type=str, default=None, help='Save the model files to this folder.')
    parser.add_argument('--model-dir', type=str, default=None, help='Save the model files to a subfolder of this folder instead of the default one (text-generation-webui/models).')
    parser.add_argument('--clean', action='store_true', help='Does not resume the previous download.')
    parser.add_argument('--check', action='store_true', help='Validates the checksums of model files.')
    parser.add_argument('--max-retries', type=int, default=7, help='Max retries count when get error in download time.')
    args = parser.parse_args()

    branch = args.branch
    model = args.MODEL
    specific_file = args.specific_file
    exclude_pattern = args.exclude_pattern

    if model is None:
        print(""Error: Please specify the model you'd like to download (e.g. 'python download-model.py facebook/opt-1.3b')."")
        sys.exit()

    downloader = ModelDownloader(max_retries=args.max_retries)
    # Clean up the model/branch names
    try:
        model, branch = downloader.sanitize_model_and_branch_names(model, branch)
    except ValueError as err_branch:
        print(f""Error: {err_branch}"")
        sys.exit()

    # Get the download links from Hugging Face
    links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(
        model, branch, text_only=args.text_only, specific_file=specific_file, exclude_pattern=exclude_pattern
    )

    # Get the output folder
    if args.output:
        output_folder = Path(args.output)
    else:
        output_folder = downloader.get_output_folder(model, branch, is_lora, is_llamacpp=is_llamacpp, model_dir=args.model_dir)

    if args.check:
        # Check previously downloaded files
        downloader.check_model_files(model, branch, links, sha256, output_folder)
    else:
        # Download files
        downloader.download_model_files(
            model, branch, links, sha256, output_folder,
            specific_file=specific_file, threads=args.threads, is_llamacpp=is_llamacpp
        )
"
RVC-Boss_GPT-SoVITS,RVC-Boss_GPT-SoVITS_api_v2.py,17963,https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/api_v2.py,"""""""
# WebAPI文档

` python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/tts_infer.yaml `

## 执行参数:
    `-a` - `绑定地址, 默认""127.0.0.1""`
    `-p` - `绑定端口, 默认9880`
    `-c` - `TTS配置文件路径, 默认""GPT_SoVITS/configs/tts_infer.yaml""`

## 调用:

### 推理

endpoint: `/tts`
GET:
```
http://127.0.0.1:9880/tts?text=先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。&text_lang=zh&ref_audio_path=archive_jingyuan_1.wav&prompt_lang=zh&prompt_text=我是「罗浮」云骑将军景元。不必拘谨，「将军」只是一时的身份，你称呼我景元便可&text_split_method=cut5&batch_size=1&media_type=wav&streaming_mode=true
```

POST:
```json
{
    ""text"": """",                   # str.(required) text to be synthesized
    ""text_lang: """",               # str.(required) language of the text to be synthesized
    ""ref_audio_path"": """",         # str.(required) reference audio path
    ""aux_ref_audio_paths"": [],    # list.(optional) auxiliary reference audio paths for multi-speaker tone fusion
    ""prompt_text"": """",            # str.(optional) prompt text for the reference audio
    ""prompt_lang"": """",            # str.(required) language of the prompt text for the reference audio
    ""top_k"": 5,                   # int. top k sampling
    ""top_p"": 1,                   # float. top p sampling
    ""temperature"": 1,             # float. temperature for sampling
    ""text_split_method"": ""cut0"",  # str. text split method, see text_segmentation_method.py for details.
    ""batch_size"": 1,              # int. batch size for inference
    ""batch_threshold"": 0.75,      # float. threshold for batch splitting.
    ""split_bucket: True,          # bool. whether to split the batch into multiple buckets.
    ""speed_factor"":1.0,           # float. control the speed of the synthesized audio.
    ""streaming_mode"": False,      # bool. whether to return a streaming response.
    ""seed"": -1,                   # int. random seed for reproducibility.
    ""parallel_infer"": True,       # bool. whether to use parallel inference.
    ""repetition_penalty"": 1.35    # float. repetition penalty for T2S model.
}
```

RESP:
成功: 直接返回 wav 音频流， http code 200
失败: 返回包含错误信息的 json, http code 400

### 命令控制

endpoint: `/control`

command:
""restart"": 重新运行
""exit"": 结束运行

GET:
```
http://127.0.0.1:9880/control?command=restart
```
POST:
```json
{
    ""command"": ""restart""
}
```

RESP: 无


### 切换GPT模型

endpoint: `/set_gpt_weights`

GET:
```
http://127.0.0.1:9880/set_gpt_weights?weights_path=GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt
```
RESP: 
成功: 返回""success"", http code 200
失败: 返回包含错误信息的 json, http code 400


### 切换Sovits模型

endpoint: `/set_sovits_weights`

GET:
```
http://127.0.0.1:9880/set_sovits_weights?weights_path=GPT_SoVITS/pretrained_models/s2G488k.pth
```

RESP: 
成功: 返回""success"", http code 200
失败: 返回包含错误信息的 json, http code 400
    
""""""
import os
import sys
import traceback
from typing import Generator

now_dir = os.getcwd()
sys.path.append(now_dir)
sys.path.append(""%s/GPT_SoVITS"" % (now_dir))

import argparse
import subprocess
import wave
import signal
import numpy as np
import soundfile as sf
from fastapi import FastAPI, Request, HTTPException, Response
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi import FastAPI, UploadFile, File
import uvicorn
from io import BytesIO
from tools.i18n.i18n import I18nAuto
from GPT_SoVITS.TTS_infer_pack.TTS import TTS, TTS_Config
from GPT_SoVITS.TTS_infer_pack.text_segmentation_method import get_method_names as get_cut_method_names
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
# print(sys.path)
i18n = I18nAuto()
cut_method_names = get_cut_method_names()

parser = argparse.ArgumentParser(description=""GPT-SoVITS api"")
parser.add_argument(""-c"", ""--tts_config"", type=str, default=""GPT_SoVITS/configs/tts_infer.yaml"", help=""tts_infer路径"")
parser.add_argument(""-a"", ""--bind_addr"", type=str, default=""127.0.0.1"", help=""default: 127.0.0.1"")
parser.add_argument(""-p"", ""--port"", type=int, default=""9880"", help=""default: 9880"")
args = parser.parse_args()
config_path = args.tts_config
# device = args.device
port = args.port
host = args.bind_addr
argv = sys.argv

if config_path in [None, """"]:
    config_path = ""GPT-SoVITS/configs/tts_infer.yaml""

tts_config = TTS_Config(config_path)
print(tts_config)
tts_pipeline = TTS(tts_config)

APP = FastAPI()
class TTS_Request(BaseModel):
    text: str = None
    text_lang: str = None
    ref_audio_path: str = None
    aux_ref_audio_paths: list = None
    prompt_lang: str = None
    prompt_text: str = """"
    top_k:int = 5
    top_p:float = 1
    temperature:float = 1
    text_split_method:str = ""cut5""
    batch_size:int = 1
    batch_threshold:float = 0.75
    split_bucket:bool = True
    speed_factor:float = 1.0
    fragment_interval:float = 0.3
    seed:int = -1
    media_type:str = ""wav""
    streaming_mode:bool = False
    parallel_infer:bool = True
    repetition_penalty:float = 1.35

### modify from https://github.com/RVC-Boss/GPT-SoVITS/pull/894/files
def pack_ogg(io_buffer:BytesIO, data:np.ndarray, rate:int):
    with sf.SoundFile(io_buffer, mode='w', samplerate=rate, channels=1, format='ogg') as audio_file:
        audio_file.write(data)
    return io_buffer


def pack_raw(io_buffer:BytesIO, data:np.ndarray, rate:int):
    io_buffer.write(data.tobytes())
    return io_buffer


def pack_wav(io_buffer:BytesIO, data:np.ndarray, rate:int):
    io_buffer = BytesIO()
    sf.write(io_buffer, data, rate, format='wav')
    return io_buffer

def pack_aac(io_buffer:BytesIO, data:np.ndarray, rate:int):
    process = subprocess.Popen([
        'ffmpeg',
        '-f', 's16le',  # 输入16位有符号小端整数PCM
        '-ar', str(rate),  # 设置采样率
        '-ac', '1',  # 单声道
        '-i', 'pipe:0',  # 从管道读取输入
        '-c:a', 'aac',  # 音频编码器为AAC
        '-b:a', '192k',  # 比特率
        '-vn',  # 不包含视频
        '-f', 'adts',  # 输出AAC数据流格式
        'pipe:1'  # 将输出写入管道
    ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out, _ = process.communicate(input=data.tobytes())
    io_buffer.write(out)
    return io_buffer

def pack_audio(io_buffer:BytesIO, data:np.ndarray, rate:int, media_type:str):
    if media_type == ""ogg"":
        io_buffer = pack_ogg(io_buffer, data, rate)
    elif media_type == ""aac"":
        io_buffer = pack_aac(io_buffer, data, rate)
    elif media_type == ""wav"":
        io_buffer = pack_wav(io_buffer, data, rate)
    else:
        io_buffer = pack_raw(io_buffer, data, rate)
    io_buffer.seek(0)
    return io_buffer



# from https://huggingface.co/spaces/coqui/voice-chat-with-mistral/blob/main/app.py
def wave_header_chunk(frame_input=b"""", channels=1, sample_width=2, sample_rate=32000):
    # This will create a wave header then append the frame input
    # It should be first on a streaming wav file
    # Other frames better should not have it (else you will hear some artifacts each chunk start)
    wav_buf = BytesIO()
    with wave.open(wav_buf, ""wb"") as vfout:
        vfout.setnchannels(channels)
        vfout.setsampwidth(sample_width)
        vfout.setframerate(sample_rate)
        vfout.writeframes(frame_input)

    wav_buf.seek(0)
    return wav_buf.read()


def handle_control(command:str):
    if command == ""restart"":
        os.execl(sys.executable, sys.executable, *argv)
    elif command == ""exit"":
        os.kill(os.getpid(), signal.SIGTERM)
        exit(0)


def check_params(req:dict):
    text:str = req.get(""text"", """")
    text_lang:str = req.get(""text_lang"", """")
    ref_audio_path:str = req.get(""ref_audio_path"", """")
    streaming_mode:bool = req.get(""streaming_mode"", False)
    media_type:str = req.get(""media_type"", ""wav"")
    prompt_lang:str = req.get(""prompt_lang"", """")
    text_split_method:str = req.get(""text_split_method"", ""cut5"")

    if ref_audio_path in [None, """"]:
        return JSONResponse(status_code=400, content={""message"": ""ref_audio_path is required""})
    if text in [None, """"]:
        return JSONResponse(status_code=400, content={""message"": ""text is required""})
    if (text_lang in [None, """"]) :
        return JSONResponse(status_code=400, content={""message"": ""text_lang is required""})
    elif text_lang.lower() not in tts_config.languages:
        return JSONResponse(status_code=400, content={""message"": f""text_lang: {text_lang} is not supported in version {tts_config.version}""})
    if (prompt_lang in [None, """"]) :
        return JSONResponse(status_code=400, content={""message"": ""prompt_lang is required""})
    elif prompt_lang.lower() not in tts_config.languages:
        return JSONResponse(status_code=400, content={""message"": f""prompt_lang: {prompt_lang} is not supported in version {tts_config.version}""})
    if media_type not in [""wav"", ""raw"", ""ogg"", ""aac""]:
        return JSONResponse(status_code=400, content={""message"": f""media_type: {media_type} is not supported""})
    elif media_type == ""ogg"" and  not streaming_mode:
        return JSONResponse(status_code=400, content={""message"": ""ogg format is not supported in non-streaming mode""})
    
    if text_split_method not in cut_method_names:
        return JSONResponse(status_code=400, content={""message"": f""text_split_method:{text_split_method} is not supported""})

    return None

async def tts_handle(req:dict):
    """"""
    Text to speech handler.
    
    Args:
        req (dict): 
            {
                ""text"": """",                   # str.(required) text to be synthesized
                ""text_lang: """",               # str.(required) language of the text to be synthesized
                ""ref_audio_path"": """",         # str.(required) reference audio path
                ""aux_ref_audio_paths"": [],    # list.(optional) auxiliary reference audio paths for multi-speaker synthesis
                ""prompt_text"": """",            # str.(optional) prompt text for the reference audio
                ""prompt_lang"": """",            # str.(required) language of the prompt text for the reference audio
                ""top_k"": 5,                   # int. top k sampling
                ""top_p"": 1,                   # float. top p sampling
                ""temperature"": 1,             # float. temperature for sampling
                ""text_split_method"": ""cut5"",  # str. text split method, see text_segmentation_method.py for details.
                ""batch_size"": 1,              # int. batch size for inference
                ""batch_threshold"": 0.75,      # float. threshold for batch splitting.
                ""split_bucket: True,          # bool. whether to split the batch into multiple buckets.
                ""speed_factor"":1.0,           # float. control the speed of the synthesized audio.
                ""fragment_interval"":0.3,      # float. to control the interval of the audio fragment.
                ""seed"": -1,                   # int. random seed for reproducibility.
                ""media_type"": ""wav"",          # str. media type of the output audio, support ""wav"", ""raw"", ""ogg"", ""aac"".
                ""streaming_mode"": False,      # bool. whether to return a streaming response.
                ""parallel_infer"": True,       # bool.(optional) whether to use parallel inference.
                ""repetition_penalty"": 1.35    # float.(optional) repetition penalty for T2S model.          
            }
    returns:
        StreamingResponse: audio stream response.
    """"""
    
    streaming_mode = req.get(""streaming_mode"", False)
    return_fragment = req.get(""return_fragment"", False)
    media_type = req.get(""media_type"", ""wav"")

    check_res = check_params(req)
    if check_res is not None:
        return check_res

    if streaming_mode or return_fragment:
        req[""return_fragment""] = True
    
    try:
        tts_generator=tts_pipeline.run(req)
        
        if streaming_mode:
            def streaming_generator(tts_generator:Generator, media_type:str):
                if media_type == ""wav"":
                    yield wave_header_chunk()
                    media_type = ""raw""
                for sr, chunk in tts_generator:
                    yield pack_audio(BytesIO(), chunk, sr, media_type).getvalue()
            # _media_type = f""audio/{media_type}"" if not (streaming_mode and media_type in [""wav"", ""raw""]) else f""audio/x-{media_type}""
            return StreamingResponse(streaming_generator(tts_generator, media_type, ), media_type=f""audio/{media_type}"")
    
        else:
            sr, audio_data = next(tts_generator)
            audio_data = pack_audio(BytesIO(), audio_data, sr, media_type).getvalue()
            return Response(audio_data, media_type=f""audio/{media_type}"")
    except Exception as e:
        return JSONResponse(status_code=400, content={""message"": f""tts failed"", ""Exception"": str(e)})
    





@APP.get(""/control"")
async def control(command: str = None):
    if command is None:
        return JSONResponse(status_code=400, content={""message"": ""command is required""})
    handle_control(command)



@APP.get(""/tts"")
async def tts_get_endpoint(
                        text: str = None,
                        text_lang: str = None,
                        ref_audio_path: str = None,
                        aux_ref_audio_paths:list = None,
                        prompt_lang: str = None,
                        prompt_text: str = """",
                        top_k:int = 5,
                        top_p:float = 1,
                        temperature:float = 1,
                        text_split_method:str = ""cut0"",
                        batch_size:int = 1,
                        batch_threshold:float = 0.75,
                        split_bucket:bool = True,
                        speed_factor:float = 1.0,
                        fragment_interval:float = 0.3,
                        seed:int = -1,
                        media_type:str = ""wav"",
                        streaming_mode:bool = False,
                        parallel_infer:bool = True,
                        repetition_penalty:float = 1.35
                        ):
    req = {
        ""text"": text,
        ""text_lang"": text_lang.lower(),
        ""ref_audio_path"": ref_audio_path,
        ""aux_ref_audio_paths"": aux_ref_audio_paths,
        ""prompt_text"": prompt_text,
        ""prompt_lang"": prompt_lang.lower(),
        ""top_k"": top_k,
        ""top_p"": top_p,
        ""temperature"": temperature,
        ""text_split_method"": text_split_method,
        ""batch_size"":int(batch_size),
        ""batch_threshold"":float(batch_threshold),
        ""speed_factor"":float(speed_factor),
        ""split_bucket"":split_bucket,
        ""fragment_interval"":fragment_interval,
        ""seed"":seed,
        ""media_type"":media_type,
        ""streaming_mode"":streaming_mode,
        ""parallel_infer"":parallel_infer,
        ""repetition_penalty"":float(repetition_penalty)
    }
    return await tts_handle(req)
                

@APP.post(""/tts"")
async def tts_post_endpoint(request: TTS_Request):
    req = request.dict()
    return await tts_handle(req)


@APP.get(""/set_refer_audio"")
async def set_refer_aduio(refer_audio_path: str = None):
    try:
        tts_pipeline.set_ref_audio(refer_audio_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={""message"": f""set refer audio failed"", ""Exception"": str(e)})
    return JSONResponse(status_code=200, content={""message"": ""success""})


# @APP.post(""/set_refer_audio"")
# async def set_refer_aduio_post(audio_file: UploadFile = File(...)):
#     try:
#         # 检查文件类型，确保是音频文件
#         if not audio_file.content_type.startswith(""audio/""):
#             return JSONResponse(status_code=400, content={""message"": ""file type is not supported""})
        
#         os.makedirs(""uploaded_audio"", exist_ok=True)
#         save_path = os.path.join(""uploaded_audio"", audio_file.filename)
#         # 保存音频文件到服务器上的一个目录
#         with open(save_path , ""wb"") as buffer:
#             buffer.write(await audio_file.read())
            
#         tts_pipeline.set_ref_audio(save_path)
#     except Exception as e:
#         return JSONResponse(status_code=400, content={""message"": f""set refer audio failed"", ""Exception"": str(e)})
#     return JSONResponse(status_code=200, content={""message"": ""success""})

@APP.get(""/set_gpt_weights"")
async def set_gpt_weights(weights_path: str = None):
    try:
        if weights_path in ["""", None]:
            return JSONResponse(status_code=400, content={""message"": ""gpt weight path is required""})
        tts_pipeline.init_t2s_weights(weights_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={""message"": f""change gpt weight failed"", ""Exception"": str(e)})

    return JSONResponse(status_code=200, content={""message"": ""success""})


@APP.get(""/set_sovits_weights"")
async def set_sovits_weights(weights_path: str = None):
    try:
        if weights_path in ["""", None]:
            return JSONResponse(status_code=400, content={""message"": ""sovits weight path is required""})
        tts_pipeline.init_vits_weights(weights_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={""message"": f""change sovits weight failed"", ""Exception"": str(e)})
    return JSONResponse(status_code=200, content={""message"": ""success""})



if __name__ == ""__main__"":
    try:
        if host == 'None':   # 在调用时使用 -a None 参数，可以让api监听双栈
            host = None
        uvicorn.run(app=APP, host=host, port=port, workers=1)
    except Exception as e:
        traceback.print_exc()
        os.kill(os.getpid(), signal.SIGTERM)
        exit(0)
"
oobabooga_text-generation-webui,oobabooga_text-generation-webui_one_click.py,20253,https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/one_click.py,"import argparse
import glob
import hashlib
import json
import os
import platform
import re
import signal
import site
import subprocess
import sys

# Remove the '# ' from the following lines as needed for your AMD GPU on Linux
# os.environ[""ROCM_PATH""] = '/opt/rocm'
# os.environ[""HSA_OVERRIDE_GFX_VERSION""] = '10.3.0'
# os.environ[""HCC_AMDGPU_TARGET""] = 'gfx1030'


# Define the required PyTorch version
TORCH_VERSION = ""2.4.1""
TORCHVISION_VERSION = ""0.19.1""
TORCHAUDIO_VERSION = ""2.4.1""

# Environment
script_dir = os.getcwd()
conda_env_path = os.path.join(script_dir, ""installer_files"", ""env"")

# Command-line flags
cmd_flags_path = os.path.join(script_dir, ""CMD_FLAGS.txt"")
if os.path.exists(cmd_flags_path):
    with open(cmd_flags_path, 'r') as f:
        CMD_FLAGS = ' '.join(line.strip().rstrip('\\').strip() for line in f if line.strip().rstrip('\\').strip() and not line.strip().startswith('#'))
else:
    CMD_FLAGS = ''

flags = f""{' '.join([flag for flag in sys.argv[1:] if flag != '--update-wizard'])} {CMD_FLAGS}""


def signal_handler(sig, frame):
    sys.exit(0)


signal.signal(signal.SIGINT, signal_handler)


def is_linux():
    return sys.platform.startswith(""linux"")


def is_windows():
    return sys.platform.startswith(""win"")


def is_macos():
    return sys.platform.startswith(""darwin"")


def is_x86_64():
    return platform.machine() == ""x86_64""


def cpu_has_avx2():
    try:
        import cpuinfo

        info = cpuinfo.get_cpu_info()
        if 'avx2' in info['flags']:
            return True
        else:
            return False
    except:
        return True


def cpu_has_amx():
    try:
        import cpuinfo

        info = cpuinfo.get_cpu_info()
        if 'amx' in info['flags']:
            return True
        else:
            return False
    except:
        return True


def torch_version():
    site_packages_path = None
    for sitedir in site.getsitepackages():
        if ""site-packages"" in sitedir and conda_env_path in sitedir:
            site_packages_path = sitedir
            break

    if site_packages_path:
        torch_version_file = open(os.path.join(site_packages_path, 'torch', 'version.py')).read().splitlines()
        torver = [line for line in torch_version_file if line.startswith('__version__')][0].split('__version__ = ')[1].strip(""'"")
    else:
        from torch import __version__ as torver

    return torver


def update_pytorch():
    print_big_message(""Checking for PyTorch updates."")
    torver = torch_version()
    base_cmd = f""python -m pip install --upgrade torch=={TORCH_VERSION} torchvision=={TORCHVISION_VERSION} torchaudio=={TORCHAUDIO_VERSION}""

    if ""+cu118"" in torver:
        install_cmd = f""{base_cmd} --index-url https://download.pytorch.org/whl/cu118""
    elif ""+cu"" in torver:
        install_cmd = f""{base_cmd} --index-url https://download.pytorch.org/whl/cu121""
    elif ""+rocm"" in torver:
        install_cmd = f""{base_cmd} --index-url https://download.pytorch.org/whl/rocm6.1""
    elif ""+cpu"" in torver:
        install_cmd = f""{base_cmd} --index-url https://download.pytorch.org/whl/cpu""
    elif ""+cxx11"" in torver:
        intel_extension = ""intel-extension-for-pytorch==2.1.10+xpu"" if is_linux() else ""intel-extension-for-pytorch==2.1.10""
        install_cmd = f""{base_cmd} {intel_extension} --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/""
    else:
        install_cmd = base_cmd

    run_cmd(install_cmd, assert_success=True, environment=True)


def is_installed():
    site_packages_path = None
    for sitedir in site.getsitepackages():
        if ""site-packages"" in sitedir and conda_env_path in sitedir:
            site_packages_path = sitedir
            break

    if site_packages_path:
        return os.path.isfile(os.path.join(site_packages_path, 'torch', '__init__.py'))
    else:
        return os.path.isdir(conda_env_path)


def check_env():
    # If we have access to conda, we are probably in an environment
    conda_exist = run_cmd(""conda"", environment=True, capture_output=True).returncode == 0
    if not conda_exist:
        print(""Conda is not installed. Exiting..."")
        sys.exit(1)

    # Ensure this is a new environment and not the base environment
    if os.environ[""CONDA_DEFAULT_ENV""] == ""base"":
        print(""Create an environment for this project and activate it. Exiting..."")
        sys.exit(1)


def get_current_commit():
    result = run_cmd(""git rev-parse HEAD"", capture_output=True, environment=True)
    return result.stdout.decode('utf-8').strip()


def clear_cache():
    run_cmd(""conda clean -a -y"", environment=True)
    run_cmd(""python -m pip cache purge"", environment=True)


def print_big_message(message):
    message = message.strip()
    lines = message.split('\n')
    print(""\n\n*******************************************************************"")
    for line in lines:
        print(""*"", line)

    print(""*******************************************************************\n\n"")


def calculate_file_hash(file_path):
    p = os.path.join(script_dir, file_path)
    if os.path.isfile(p):
        with open(p, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()
    else:
        return ''


def run_cmd(cmd, assert_success=False, environment=False, capture_output=False, env=None):
    # Use the conda environment
    if environment:
        if is_windows():
            conda_bat_path = os.path.join(script_dir, ""installer_files"", ""conda"", ""condabin"", ""conda.bat"")
            cmd = f'""{conda_bat_path}"" activate ""{conda_env_path}"" >nul && {cmd}'
        else:
            conda_sh_path = os.path.join(script_dir, ""installer_files"", ""conda"", ""etc"", ""profile.d"", ""conda.sh"")
            cmd = f'. ""{conda_sh_path}"" && conda activate ""{conda_env_path}"" && {cmd}'

    # Set executable to None for Windows, bash for everything else
    executable = None if is_windows() else 'bash'

    # Run shell commands
    result = subprocess.run(cmd, shell=True, capture_output=capture_output, env=env, executable=executable)

    # Assert the command ran successfully
    if assert_success and result.returncode != 0:
        print(f""Command '{cmd}' failed with exit status code '{str(result.returncode)}'.\n\nExiting now.\nTry running the start/update script again."")
        sys.exit(1)

    return result


def generate_alphabetic_sequence(index):
    result = ''
    while index >= 0:
        index, remainder = divmod(index, 26)
        result = chr(ord('A') + remainder) + result
        index -= 1

    return result


def get_user_choice(question, options_dict):
    print()
    print(question)
    print()

    for key, value in options_dict.items():
        print(f""{key}) {value}"")

    print()

    choice = input(""Input> "").upper()
    while choice not in options_dict.keys():
        print(""Invalid choice. Please try again."")
        choice = input(""Input> "").upper()

    return choice


def install_webui():
    # Ask the user for the GPU vendor
    if ""GPU_CHOICE"" in os.environ:
        choice = os.environ[""GPU_CHOICE""].upper()
        print_big_message(f""Selected GPU choice \""{choice}\"" based on the GPU_CHOICE environment variable."")

        # Warn about changed meanings and handle old NVIDIA choice
        if choice == ""B"":
            print_big_message(""Warning: GPU_CHOICE='B' now means 'NVIDIA (CUDA 11.8)' in the new version."")
        elif choice == ""C"":
            print_big_message(""Warning: GPU_CHOICE='C' now means 'AMD' in the new version."")
        elif choice == ""D"":
            print_big_message(""Warning: GPU_CHOICE='D' now means 'Apple M Series' in the new version."")
        elif choice == ""A"" and ""USE_CUDA118"" in os.environ:
            choice = ""B"" if os.environ.get(""USE_CUDA118"", """").lower() in (""yes"", ""y"", ""true"", ""1"", ""t"", ""on"") else ""A""
    else:
        choice = get_user_choice(
            ""What is your GPU?"",
            {
                'A': 'NVIDIA - CUDA 12.1 (recommended)',
                'B': 'NVIDIA - CUDA 11.8 (legacy GPUs)',
                'C': 'AMD - Linux/macOS only, requires ROCm 6.1',
                'D': 'Apple M Series',
                'E': 'Intel Arc (beta)',
                'N': 'CPU mode'
            },
        )

    # Convert choices to GPU names for compatibility
    gpu_choice_to_name = {
        ""A"": ""NVIDIA"",
        ""B"": ""NVIDIA"",
        ""C"": ""AMD"",
        ""D"": ""APPLE"",
        ""E"": ""INTEL"",
        ""N"": ""NONE""
    }

    selected_gpu = gpu_choice_to_name[choice]
    use_cuda118 = (choice == ""B"")  # CUDA version is now determined by menu choice

    # Write a flag to CMD_FLAGS.txt for CPU mode
    if selected_gpu == ""NONE"":
        with open(cmd_flags_path, 'r+') as cmd_flags_file:
            if ""--cpu"" not in cmd_flags_file.read():
                print_big_message(""Adding the --cpu flag to CMD_FLAGS.txt."")
                cmd_flags_file.write(""\n--cpu\n"")

    # Handle CUDA version display
    elif any((is_windows(), is_linux())) and selected_gpu == ""NVIDIA"":
        if use_cuda118:
            print(""CUDA: 11.8"")
        else:
            print(""CUDA: 12.1"")

    # No PyTorch for AMD on Windows (?)
    elif is_windows() and selected_gpu == ""AMD"":
        print(""PyTorch setup on Windows is not implemented yet. Exiting..."")
        sys.exit(1)

    # Find the Pytorch installation command
    install_pytorch = f""python -m pip install torch=={TORCH_VERSION} torchvision=={TORCHVISION_VERSION} torchaudio=={TORCHAUDIO_VERSION} ""

    if selected_gpu == ""NVIDIA"":
        if use_cuda118 == 'Y':
            install_pytorch += ""--index-url https://download.pytorch.org/whl/cu118""
        else:
            install_pytorch += ""--index-url https://download.pytorch.org/whl/cu121""
    elif selected_gpu == ""AMD"":
        install_pytorch += ""--index-url https://download.pytorch.org/whl/rocm6.1""
    elif selected_gpu in [""APPLE"", ""NONE""]:
        install_pytorch += ""--index-url https://download.pytorch.org/whl/cpu""
    elif selected_gpu == ""INTEL"":
        if is_linux():
            install_pytorch = ""python -m pip install torch==2.1.0a0 torchvision==0.16.0a0 torchaudio==2.1.0a0 intel-extension-for-pytorch==2.1.10+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/""
        else:
            install_pytorch = ""python -m pip install torch==2.1.0a0 torchvision==0.16.0a0 torchaudio==2.1.0a0 intel-extension-for-pytorch==2.1.10 --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/""

    # Install Git and then Pytorch
    print_big_message(""Installing PyTorch."")
    run_cmd(f""conda install -y -k ninja git && {install_pytorch} && python -m pip install py-cpuinfo==9.0.0"", assert_success=True, environment=True)

    if selected_gpu == ""INTEL"":
        # Install oneAPI dependencies via conda
        print_big_message(""Installing Intel oneAPI runtime libraries."")
        run_cmd(""conda install -y -c https://software.repos.intel.com/python/conda/ -c conda-forge dpcpp-cpp-rt=2024.0 mkl-dpcpp=2024.0"")
        # Install libuv required by Intel-patched torch
        run_cmd(""conda install -y libuv"")

    # Install the webui requirements
    update_requirements(initial_installation=True, pull=False)


def get_extensions_names():
    return [foldername for foldername in os.listdir('extensions') if os.path.isfile(os.path.join('extensions', foldername, 'requirements.txt'))]


def install_extensions_requirements():
    print_big_message(""Installing extensions requirements.\nSome of these may fail on Windows.\nDon\'t worry if you see error messages, as they will not affect the main program."")
    extensions = get_extensions_names()
    for i, extension in enumerate(extensions):
        print(f""\n\n--- [{i + 1}/{len(extensions)}]: {extension}\n\n"")
        extension_req_path = os.path.join(""extensions"", extension, ""requirements.txt"")
        run_cmd(f""python -m pip install -r {extension_req_path} --upgrade"", assert_success=False, environment=True)


def update_requirements(initial_installation=False, pull=True):
    # Create .git directory if missing
    if not os.path.exists(os.path.join(script_dir, "".git"")):
        run_cmd(
            ""git init -b main && git remote add origin https://github.com/oobabooga/text-generation-webui && ""
            ""git fetch && git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main && ""
            ""git reset --hard origin/main && git branch --set-upstream-to=origin/main"",
            environment=True,
            assert_success=True
        )

    torver = torch_version()
    if ""+rocm"" in torver:
        requirements_file = ""requirements_amd"" + (""_noavx2"" if not cpu_has_avx2() else """") + "".txt""
    elif ""+cpu"" in torver or ""+cxx11"" in torver:
        requirements_file = ""requirements_cpu_only"" + (""_noavx2"" if not cpu_has_avx2() else """") + "".txt""
    elif is_macos():
        requirements_file = ""requirements_apple_"" + (""intel"" if is_x86_64() else ""silicon"") + "".txt""
    else:
        requirements_file = ""requirements"" + (""_noavx2"" if not cpu_has_avx2() else """") + "".txt""

    # Load state from JSON file
    state_file = '.installer_state.json'
    current_commit = get_current_commit()
    wheels_changed = False
    if os.path.exists(state_file):
        with open(state_file, 'r') as f:
            last_state = json.load(f)

        if 'wheels_changed' in last_state or last_state.get('last_installed_commit') != current_commit:
            wheels_changed = True
    else:
        wheels_changed = True

    if pull:
        # Read .whl lines before pulling
        before_pull_whl_lines = []
        if os.path.exists(requirements_file):
            with open(requirements_file, 'r') as f:
                before_pull_whl_lines = [line for line in f if '.whl' in line]

        print_big_message('Updating the local copy of the repository with ""git pull""')

        # Hash files before pulling
        files_to_check = [
            'start_linux.sh', 'start_macos.sh', 'start_windows.bat', 'start_wsl.bat',
            'update_wizard_linux.sh', 'update_wizard_macos.sh', 'update_wizard_windows.bat', 'update_wizard_wsl.bat',
            'one_click.py'
        ]
        before_hashes = {file: calculate_file_hash(file) for file in files_to_check}

        # Perform the git pull
        run_cmd(""git pull --autostash"", assert_success=True, environment=True)

        # Check hashes after pulling
        after_hashes = {file: calculate_file_hash(file) for file in files_to_check}
        if os.path.exists(requirements_file):
            with open(requirements_file, 'r') as f:
                after_pull_whl_lines = [line for line in f if '.whl' in line]

        wheels_changed = wheels_changed or (before_pull_whl_lines != after_pull_whl_lines)

        # Check for changes to installer files
        for file in files_to_check:
            if before_hashes[file] != after_hashes[file]:
                print_big_message(f""File '{file}' was updated during 'git pull'. Please run the script again."")

                # Save state before exiting
                current_state = {}
                if wheels_changed:
                    current_state['wheels_changed'] = True

                with open(state_file, 'w') as f:
                    json.dump(current_state, f)

                sys.exit(1)

    # Save current state
    current_state = {'last_installed_commit': current_commit}
    with open(state_file, 'w') as f:
        json.dump(current_state, f)

    if os.environ.get(""INSTALL_EXTENSIONS"", """").lower() in (""yes"", ""y"", ""true"", ""1"", ""t"", ""on""):
        install_extensions_requirements()

    # Update PyTorch
    if not initial_installation:
        update_pytorch()

    print_big_message(f""Installing webui requirements from file: {requirements_file}"")
    print(f""TORCH: {torver}\n"")

    # Prepare the requirements file
    textgen_requirements = open(requirements_file).read().splitlines()

    if not initial_installation and not wheels_changed:
        textgen_requirements = [line for line in textgen_requirements if '.whl' not in line]

    if ""+cu118"" in torver:
        textgen_requirements = [
            req.replace('+cu121', '+cu118').replace('+cu122', '+cu118')
            for req in textgen_requirements
            if ""autoawq"" not in req.lower()
        ]

    if is_windows() and ""+cu118"" in torver:  # No flash-attention on Windows for CUDA 11
        textgen_requirements = [req for req in textgen_requirements if 'oobabooga/flash-attention' not in req]

    with open('temp_requirements.txt', 'w') as file:
        file.write('\n'.join(textgen_requirements))

    # Workaround for git+ packages not updating properly.
    git_requirements = [req for req in textgen_requirements if req.startswith(""git+"")]
    for req in git_requirements:
        url = req.replace(""git+"", """")
        package_name = url.split(""/"")[-1].split(""@"")[0].rstrip("".git"")
        run_cmd(f""python -m pip uninstall -y {package_name}"", environment=True)
        print(f""Uninstalled {package_name}"")

    # Install/update the project requirements
    run_cmd(""python -m pip install -r temp_requirements.txt --upgrade"", assert_success=True, environment=True)

    # Clean up
    os.remove('temp_requirements.txt')
    clear_cache()


def launch_webui():
    run_cmd(f""python server.py {flags}"", environment=True)


if __name__ == ""__main__"":
    # Verifies we are in a conda environment
    check_env()

    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('--update-wizard', action='store_true', help='Launch a menu with update options.')
    args, _ = parser.parse_known_args()

    if args.update_wizard:
        while True:
            choice = get_user_choice(
                ""What would you like to do?"",
                {
                    'A': 'Update the web UI',
                    'B': 'Install/update extensions requirements',
                    'C': 'Revert local changes to repository files with \""git reset --hard\""',
                    'N': 'Nothing (exit)'
                },
            )

            if choice == 'A':
                update_requirements()
            elif choice == 'B':
                choices = {'A': 'All extensions'}
                for i, name in enumerate(get_extensions_names()):
                    key = generate_alphabetic_sequence(i + 1)
                    choices[key] = name

                choice = get_user_choice(""What extension?"", choices)

                if choice == 'A':
                    install_extensions_requirements()
                else:
                    extension_req_path = os.path.join(""extensions"", choices[choice], ""requirements.txt"")
                    run_cmd(f""python -m pip install -r {extension_req_path} --upgrade"", assert_success=False, environment=True)

                update_requirements(pull=False)
            elif choice == 'C':
                run_cmd(""git reset --hard"", assert_success=True, environment=True)
            elif choice == 'N':
                sys.exit()
    else:
        if not is_installed():
            install_webui()
            os.chdir(script_dir)

        if os.environ.get(""LAUNCH_AFTER_INSTALL"", """").lower() in (""no"", ""n"", ""false"", ""0"", ""f"", ""off""):
            print_big_message(""Will now exit due to LAUNCH_AFTER_INSTALL."")
            sys.exit()

        # Check if a model has been downloaded yet
        if '--model-dir' in flags:
            # Splits on ' ' or '=' while maintaining spaces within quotes
            flags_list = re.split(' +(?=(?:[^\""]*\""[^\""]*\"")*[^\""]*$)|=', flags)
            model_dir = [flags_list[(flags_list.index(flag) + 1)] for flag in flags_list if flag == '--model-dir'][0].strip('""\'')
        else:
            model_dir = 'models'

        if len([item for item in glob.glob(f'{model_dir}/*') if not item.endswith(('.txt', '.yaml'))]) == 0:
            print_big_message(""You haven't downloaded any model yet.\nOnce the web UI launches, head over to the \""Model\"" tab and download one."")

        # Workaround for llama-cpp-python loading paths in CUDA env vars even if they do not exist
        conda_path_bin = os.path.join(conda_env_path, ""bin"")
        if not os.path.exists(conda_path_bin):
            os.mkdir(conda_path_bin)

        # Launch the webui
        launch_webui()
"
pandas-dev_pandas,pandas-dev_pandas_setup.py,21960,https://raw.githubusercontent.com/pandas-dev/pandas/main/setup.py,"#!/usr/bin/env python3

""""""
Parts of this file were taken from the pyzmq project
(https://github.com/zeromq/pyzmq) which have been permitted for use under the
BSD license. Parts are from lxml (https://github.com/lxml/lxml)
""""""

import argparse
import multiprocessing
import os
from os.path import join as pjoin
import platform
import shutil
import sys
from sysconfig import get_config_vars

import numpy
from pkg_resources import parse_version
from setuptools import (
    Command,
    Extension,
    setup,
)
from setuptools.command.build_ext import build_ext as _build_ext
import versioneer

cmdclass = versioneer.get_cmdclass()


def is_platform_windows():
    return sys.platform in (""win32"", ""cygwin"")


def is_platform_mac():
    return sys.platform == ""darwin""


# note: sync with pyproject.toml, environment.yml and asv.conf.json
min_cython_ver = ""3.0""

try:
    from Cython import (
        Tempita,
        __version__ as _CYTHON_VERSION,
    )
    from Cython.Build import cythonize

    _CYTHON_INSTALLED = parse_version(_CYTHON_VERSION) >= parse_version(min_cython_ver)
except ImportError:
    _CYTHON_VERSION = None
    _CYTHON_INSTALLED = False
    cythonize = lambda x, *args, **kwargs: x  # dummy func


_pxi_dep_template = {
    ""algos"": [""_libs/algos_common_helper.pxi.in"", ""_libs/algos_take_helper.pxi.in""],
    ""hashtable"": [
        ""_libs/hashtable_class_helper.pxi.in"",
        ""_libs/hashtable_func_helper.pxi.in"",
        ""_libs/khash_for_primitive_helper.pxi.in"",
    ],
    ""index"": [""_libs/index_class_helper.pxi.in""],
    ""sparse"": [""_libs/sparse_op_helper.pxi.in""],
    ""interval"": [""_libs/intervaltree.pxi.in""],
}

_pxifiles = []
_pxi_dep = {}
for module, files in _pxi_dep_template.items():
    pxi_files = [pjoin(""pandas"", x) for x in files]
    _pxifiles.extend(pxi_files)
    _pxi_dep[module] = pxi_files


class build_ext(_build_ext):
    @classmethod
    def render_templates(cls, pxifiles) -> None:
        for pxifile in pxifiles:
            # build pxifiles first, template extension must be .pxi.in
            assert pxifile.endswith("".pxi.in"")
            outfile = pxifile[:-3]

            if (
                os.path.exists(outfile)
                and os.stat(pxifile).st_mtime < os.stat(outfile).st_mtime
            ):
                # if .pxi.in is not updated, no need to output .pxi
                continue

            with open(pxifile, encoding=""utf-8"") as f:
                tmpl = f.read()
            pyxcontent = Tempita.sub(tmpl)

            with open(outfile, ""w"", encoding=""utf-8"") as f:
                f.write(pyxcontent)

    def build_extensions(self) -> None:
        # if building from c files, don't need to
        # generate template output
        if _CYTHON_INSTALLED:
            self.render_templates(_pxifiles)

        super().build_extensions()


class CleanCommand(Command):
    """"""Custom command to clean the .so and .pyc files.""""""

    user_options = [(""all"", ""a"", """")]

    def initialize_options(self) -> None:
        self.all = True
        self._clean_me = []
        self._clean_trees = []

        base = pjoin(""pandas"", ""_libs"", ""src"")
        parser = pjoin(base, ""parser"")
        vendored = pjoin(base, ""vendored"")
        dt = pjoin(base, ""datetime"")
        ujson_python = pjoin(vendored, ""ujson"", ""python"")
        ujson_lib = pjoin(vendored, ""ujson"", ""lib"")
        self._clean_exclude = [
            pjoin(vendored, ""numpy"", ""datetime"", ""np_datetime.c""),
            pjoin(vendored, ""numpy"", ""datetime"", ""np_datetime_strings.c""),
            pjoin(dt, ""date_conversions.c""),
            pjoin(parser, ""tokenizer.c""),
            pjoin(parser, ""io.c""),
            pjoin(ujson_python, ""ujson.c""),
            pjoin(ujson_python, ""objToJSON.c""),
            pjoin(ujson_python, ""JSONtoObj.c""),
            pjoin(ujson_lib, ""ultrajsonenc.c""),
            pjoin(ujson_lib, ""ultrajsondec.c""),
            pjoin(dt, ""pd_datetime.c""),
            pjoin(parser, ""pd_parser.c""),
        ]

        for root, dirs, files in os.walk(""pandas""):
            for f in files:
                filepath = pjoin(root, f)
                if filepath in self._clean_exclude:
                    continue

                if os.path.splitext(f)[-1] in (
                    "".pyc"",
                    "".so"",
                    "".o"",
                    "".pyo"",
                    "".pyd"",
                    "".c"",
                    "".cpp"",
                    "".orig"",
                ):
                    self._clean_me.append(filepath)
            self._clean_trees.append(pjoin(root, d) for d in dirs if d == ""__pycache__"")

        # clean the generated pxi files
        for pxifile in _pxifiles:
            pxifile_replaced = pxifile.replace("".pxi.in"", "".pxi"")
            self._clean_me.append(pxifile_replaced)

        self._clean_trees.append(d for d in (""build"", ""dist"") if os.path.exists(d))

    def finalize_options(self) -> None:
        pass

    def run(self) -> None:
        for clean_me in self._clean_me:
            try:
                os.unlink(clean_me)
            except OSError:
                pass
        for clean_tree in self._clean_trees:
            try:
                shutil.rmtree(clean_tree)
            except OSError:
                pass


# we need to inherit from the versioneer
# class as it encodes the version info
sdist_class = cmdclass[""sdist""]


class CheckSDist(sdist_class):
    """"""Custom sdist that ensures Cython has compiled all pyx files to c.""""""

    _pyxfiles = [
        ""pandas/_libs/arrays.pyx"",
        ""pandas/_libs/lib.pyx"",
        ""pandas/_libs/hashtable.pyx"",
        ""pandas/_libs/tslib.pyx"",
        ""pandas/_libs/index.pyx"",
        ""pandas/_libs/internals.pyx"",
        ""pandas/_libs/algos.pyx"",
        ""pandas/_libs/join.pyx"",
        ""pandas/_libs/indexing.pyx"",
        ""pandas/_libs/interval.pyx"",
        ""pandas/_libs/hashing.pyx"",
        ""pandas/_libs/missing.pyx"",
        ""pandas/_libs/testing.pyx"",
        ""pandas/_libs/sparse.pyx"",
        ""pandas/_libs/ops.pyx"",
        ""pandas/_libs/parsers.pyx"",
        ""pandas/_libs/tslibs/base.pyx"",
        ""pandas/_libs/tslibs/ccalendar.pyx"",
        ""pandas/_libs/tslibs/dtypes.pyx"",
        ""pandas/_libs/tslibs/period.pyx"",
        ""pandas/_libs/tslibs/strptime.pyx"",
        ""pandas/_libs/tslibs/np_datetime.pyx"",
        ""pandas/_libs/tslibs/timedeltas.pyx"",
        ""pandas/_libs/tslibs/timestamps.pyx"",
        ""pandas/_libs/tslibs/timezones.pyx"",
        ""pandas/_libs/tslibs/conversion.pyx"",
        ""pandas/_libs/tslibs/fields.pyx"",
        ""pandas/_libs/tslibs/offsets.pyx"",
        ""pandas/_libs/tslibs/parsing.pyx"",
        ""pandas/_libs/tslibs/tzconversion.pyx"",
        ""pandas/_libs/tslibs/vectorized.pyx"",
        ""pandas/_libs/window/indexers.pyx"",
        ""pandas/_libs/writers.pyx"",
        ""pandas/_libs/sas.pyx"",
        ""pandas/_libs/byteswap.pyx"",
    ]

    _cpp_pyxfiles = [
        ""pandas/_libs/window/aggregations.pyx"",
    ]

    def initialize_options(self) -> None:
        sdist_class.initialize_options(self)

    def run(self) -> None:
        if ""cython"" in cmdclass:
            self.run_command(""cython"")
        else:
            # If we are not running cython then
            # compile the extensions correctly
            pyx_files = [(self._pyxfiles, ""c""), (self._cpp_pyxfiles, ""cpp"")]

            for pyxfiles, extension in pyx_files:
                for pyxfile in pyxfiles:
                    sourcefile = pyxfile[:-3] + extension
                    msg = (
                        f""{extension}-source file '{sourcefile}' not found.\n""
                        ""Run 'setup.py cython' before sdist.""
                    )
                    assert os.path.isfile(sourcefile), msg
        sdist_class.run(self)


class CheckingBuildExt(build_ext):
    """"""
    Subclass build_ext to get clearer report if Cython is necessary.
    """"""

    def check_cython_extensions(self, extensions) -> None:
        for ext in extensions:
            for src in ext.sources:
                if not os.path.exists(src):
                    print(f""{ext.name}: -> [{ext.sources}]"")
                    raise Exception(
                        f""""""Cython-generated file '{src}' not found.
                Cython is required to compile pandas from a development branch.
                Please install Cython or download a release package of pandas.
                """"""
                    )

    def build_extensions(self) -> None:
        self.check_cython_extensions(self.extensions)
        build_ext.build_extensions(self)


class CythonCommand(build_ext):
    """"""
    Custom command subclassed from Cython.Distutils.build_ext
    to compile pyx->c, and stop there. All this does is override the
    C-compile method build_extension() with a no-op.
    """"""

    def build_extension(self, ext) -> None:
        pass


class DummyBuildSrc(Command):
    """"""numpy's build_src command interferes with Cython's build_ext.""""""

    user_options = []

    def initialize_options(self) -> None:
        self.py_modules_dict = {}

    def finalize_options(self) -> None:
        pass

    def run(self) -> None:
        pass


cmdclass[""clean""] = CleanCommand
cmdclass[""build_ext""] = CheckingBuildExt

if _CYTHON_INSTALLED:
    suffix = "".pyx""
    cmdclass[""cython""] = CythonCommand
else:
    suffix = "".c""
    cmdclass[""build_src""] = DummyBuildSrc

# ----------------------------------------------------------------------
# Preparation of compiler arguments

debugging_symbols_requested = ""--with-debugging-symbols"" in sys.argv
if debugging_symbols_requested:
    sys.argv.remove(""--with-debugging-symbols"")


if sys.byteorder == ""big"":
    endian_macro = [(""__BIG_ENDIAN__"", ""1"")]
else:
    endian_macro = [(""__LITTLE_ENDIAN__"", ""1"")]


extra_compile_args = []
extra_link_args = []
if is_platform_windows():
    if debugging_symbols_requested:
        extra_compile_args.append(""/Z7"")
        extra_link_args.append(""/DEBUG"")
else:
    # PANDAS_CI=1 is set in CI
    if os.environ.get(""PANDAS_CI"", ""0"") == ""1"":
        extra_compile_args.append(""-Werror"")
    if debugging_symbols_requested:
        extra_compile_args.append(""-g3"")
        extra_compile_args.append(""-UNDEBUG"")
        extra_compile_args.append(""-O0"")

# Build for at least macOS 10.9 when compiling on a 10.9 system or above,
# overriding CPython distuitls behaviour which is to target the version that
# python was built for. This may be overridden by setting
# MACOSX_DEPLOYMENT_TARGET before calling setup.py
if is_platform_mac():
    if ""MACOSX_DEPLOYMENT_TARGET"" not in os.environ:
        current_system = platform.mac_ver()[0]
        python_target = get_config_vars().get(
            ""MACOSX_DEPLOYMENT_TARGET"", current_system
        )
        target_macos_version = ""10.9""
        parsed_macos_version = parse_version(target_macos_version)
        if (
            parse_version(str(python_target))
            < parsed_macos_version
            <= parse_version(current_system)
        ):
            os.environ[""MACOSX_DEPLOYMENT_TARGET""] = target_macos_version

    if sys.version_info[:2] == (3, 8):  # GH 33239
        extra_compile_args.append(""-Wno-error=deprecated-declarations"")

    # https://github.com/pandas-dev/pandas/issues/35559
    extra_compile_args.append(""-Wno-error=unreachable-code"")

# enable coverage by building cython files by setting the environment variable
# ""PANDAS_CYTHON_COVERAGE"" (with a Truthy value) or by running build_ext
# with `--with-cython-coverage`enabled
linetrace = os.environ.get(""PANDAS_CYTHON_COVERAGE"", False)
if ""--with-cython-coverage"" in sys.argv:
    linetrace = True
    sys.argv.remove(""--with-cython-coverage"")

# Note: if not using `cythonize`, coverage can be enabled by
# pinning `ext.cython_directives = directives` to each ext in extensions.
# github.com/cython/cython/wiki/enhancements-compilerdirectives#in-setuppy
directives = {""linetrace"": False, ""language_level"": 3, ""always_allow_keywords"": True}
macros = []
if linetrace:
    # https://pypkg.com/pypi/pytest-cython/f/tests/example-project/setup.py
    directives[""linetrace""] = True
    macros = [(""CYTHON_TRACE"", ""1""), (""CYTHON_TRACE_NOGIL"", ""1"")]

# silence build warnings about deprecated API usage
# we can't do anything about these warnings because they stem from
# cython+numpy version mismatches.
macros.append((""NPY_NO_DEPRECATED_API"", ""0""))


# ----------------------------------------------------------------------
# Specification of Dependencies


# TODO(cython#4518): Need to check to see if e.g. `linetrace` has changed and
#  possibly re-compile.
def maybe_cythonize(extensions, *args, **kwargs):
    """"""
    Render tempita templates before calling cythonize. This is skipped for

    * clean
    * sdist
    """"""
    if ""clean"" in sys.argv or ""sdist"" in sys.argv:
        # See https://github.com/cython/cython/issues/1495
        return extensions

    elif not _CYTHON_INSTALLED:
        # GH#28836 raise a helfpul error message
        if _CYTHON_VERSION:
            raise RuntimeError(
                f""Cannot cythonize with old Cython version ({_CYTHON_VERSION} ""
                f""installed, needs {min_cython_ver})""
            )
        raise RuntimeError(""Cannot cythonize without Cython installed."")

    # reuse any parallel arguments provided for compilation to cythonize
    parser = argparse.ArgumentParser()
    parser.add_argument(""--parallel"", ""-j"", type=int, default=1)
    parsed, _ = parser.parse_known_args()

    kwargs[""nthreads""] = parsed.parallel
    build_ext.render_templates(_pxifiles)
    if debugging_symbols_requested:
        kwargs[""gdb_debug""] = True

    return cythonize(extensions, *args, **kwargs)


def srcpath(name=None, suffix="".pyx"", subdir=""src""):
    return pjoin(""pandas"", subdir, name + suffix)


lib_depends = [""pandas/_libs/include/pandas/parse_helper.h""]

tseries_depends = [
    ""pandas/_libs/include/pandas/datetime/pd_datetime.h"",
]

ext_data = {
    ""_libs.algos"": {
        ""pyxfile"": ""_libs/algos"",
        ""depends"": _pxi_dep[""algos""],
    },
    ""_libs.arrays"": {""pyxfile"": ""_libs/arrays""},
    ""_libs.groupby"": {""pyxfile"": ""_libs/groupby""},
    ""_libs.hashing"": {""pyxfile"": ""_libs/hashing"", ""depends"": []},
    ""_libs.hashtable"": {
        ""pyxfile"": ""_libs/hashtable"",
        ""depends"": (
            [
                ""pandas/_libs/include/pandas/vendored/klib/khash_python.h"",
                ""pandas/_libs/include/pandas/vendored/klib/khash.h"",
            ]
            + _pxi_dep[""hashtable""]
        ),
    },
    ""_libs.index"": {
        ""pyxfile"": ""_libs/index"",
        ""depends"": _pxi_dep[""index""],
    },
    ""_libs.indexing"": {""pyxfile"": ""_libs/indexing""},
    ""_libs.internals"": {""pyxfile"": ""_libs/internals""},
    ""_libs.interval"": {
        ""pyxfile"": ""_libs/interval"",
        ""depends"": _pxi_dep[""interval""],
    },
    ""_libs.join"": {""pyxfile"": ""_libs/join""},
    ""_libs.lib"": {
        ""pyxfile"": ""_libs/lib"",
        ""depends"": lib_depends + tseries_depends,
    },
    ""_libs.missing"": {""pyxfile"": ""_libs/missing"", ""depends"": tseries_depends},
    ""_libs.parsers"": {
        ""pyxfile"": ""_libs/parsers"",
        ""depends"": [
            ""pandas/_libs/src/parser/tokenizer.h"",
            ""pandas/_libs/src/parser/io.h"",
            ""pandas/_libs/src/pd_parser.h"",
        ],
    },
    ""_libs.ops"": {""pyxfile"": ""_libs/ops""},
    ""_libs.ops_dispatch"": {""pyxfile"": ""_libs/ops_dispatch""},
    ""_libs.properties"": {""pyxfile"": ""_libs/properties""},
    ""_libs.reshape"": {""pyxfile"": ""_libs/reshape"", ""depends"": []},
    ""_libs.sparse"": {""pyxfile"": ""_libs/sparse"", ""depends"": _pxi_dep[""sparse""]},
    ""_libs.tslib"": {
        ""pyxfile"": ""_libs/tslib"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.base"": {""pyxfile"": ""_libs/tslibs/base""},
    ""_libs.tslibs.ccalendar"": {""pyxfile"": ""_libs/tslibs/ccalendar""},
    ""_libs.tslibs.dtypes"": {""pyxfile"": ""_libs/tslibs/dtypes""},
    ""_libs.tslibs.conversion"": {
        ""pyxfile"": ""_libs/tslibs/conversion"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.fields"": {
        ""pyxfile"": ""_libs/tslibs/fields"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.nattype"": {""pyxfile"": ""_libs/tslibs/nattype""},
    ""_libs.tslibs.np_datetime"": {
        ""pyxfile"": ""_libs/tslibs/np_datetime"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.offsets"": {
        ""pyxfile"": ""_libs/tslibs/offsets"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.parsing"": {
        ""pyxfile"": ""_libs/tslibs/parsing"",
        ""sources"": [""pandas/_libs/src/parser/tokenizer.c""],
    },
    ""_libs.tslibs.period"": {
        ""pyxfile"": ""_libs/tslibs/period"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.strptime"": {
        ""pyxfile"": ""_libs/tslibs/strptime"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.timedeltas"": {
        ""pyxfile"": ""_libs/tslibs/timedeltas"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.timestamps"": {
        ""pyxfile"": ""_libs/tslibs/timestamps"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.timezones"": {""pyxfile"": ""_libs/tslibs/timezones""},
    ""_libs.tslibs.tzconversion"": {
        ""pyxfile"": ""_libs/tslibs/tzconversion"",
        ""depends"": tseries_depends,
    },
    ""_libs.tslibs.vectorized"": {
        ""pyxfile"": ""_libs/tslibs/vectorized"",
        ""depends"": tseries_depends,
    },
    ""_libs.testing"": {""pyxfile"": ""_libs/testing""},
    ""_libs.window.aggregations"": {
        ""pyxfile"": ""_libs/window/aggregations"",
        ""language"": ""c++"",
        ""suffix"": "".cpp"",
        ""depends"": [""pandas/_libs/include/pandas/skiplist.h""],
    },
    ""_libs.window.indexers"": {""pyxfile"": ""_libs/window/indexers""},
    ""_libs.writers"": {""pyxfile"": ""_libs/writers""},
    ""_libs.sas"": {""pyxfile"": ""_libs/sas""},
    ""_libs.byteswap"": {""pyxfile"": ""_libs/byteswap""},
}

extensions = []

for name, data in ext_data.items():
    source_suffix = suffix if suffix == "".pyx"" else data.get(""suffix"", "".c"")

    sources = [srcpath(data[""pyxfile""], suffix=source_suffix, subdir="""")]

    sources.extend(data.get(""sources"", []))

    include = [""pandas/_libs/include"", numpy.get_include()]

    undef_macros = []

    if (
        sys.platform == ""zos""
        and data.get(""language"") == ""c++""
        and os.path.basename(os.environ.get(""CXX"", ""/bin/xlc++"")) in (""xlc"", ""xlc++"")
    ):
        data.get(""macros"", macros).append((""__s390__"", ""1""))
        extra_compile_args.append(""-qlanglvl=extended0x:nolibext"")
        undef_macros.append(""_POSIX_THREADS"")

    obj = Extension(
        f""pandas.{name}"",
        sources=sources,
        depends=data.get(""depends"", []),
        include_dirs=include,
        language=data.get(""language"", ""c""),
        define_macros=data.get(""macros"", macros),
        extra_compile_args=extra_compile_args,
        extra_link_args=extra_link_args,
        undef_macros=undef_macros,
    )

    extensions.append(obj)

# ----------------------------------------------------------------------
# ujson

if suffix == "".pyx"":
    # undo dumb setuptools bug clobbering .pyx sources back to .c
    for ext in extensions:
        if ext.sources[0].endswith(("".c"", "".cpp"")):
            root, _ = os.path.splitext(ext.sources[0])
            ext.sources[0] = root + suffix

ujson_ext = Extension(
    ""pandas._libs.json"",
    depends=[
        ""pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h"",
        ""pandas/_libs/include/pandas/datetime/pd_datetime.h"",
    ],
    sources=(
        [
            ""pandas/_libs/src/vendored/ujson/python/ujson.c"",
            ""pandas/_libs/src/vendored/ujson/python/objToJSON.c"",
            ""pandas/_libs/src/vendored/ujson/python/JSONtoObj.c"",
            ""pandas/_libs/src/vendored/ujson/lib/ultrajsonenc.c"",
            ""pandas/_libs/src/vendored/ujson/lib/ultrajsondec.c"",
        ]
    ),
    include_dirs=[
        ""pandas/_libs/include"",
        numpy.get_include(),
    ],
    extra_compile_args=(extra_compile_args),
    extra_link_args=extra_link_args,
    define_macros=macros,
)


extensions.append(ujson_ext)

# ----------------------------------------------------------------------

# ----------------------------------------------------------------------
# pd_datetime
pd_dt_ext = Extension(
    ""pandas._libs.pandas_datetime"",
    depends=[""pandas/_libs/tslibs/datetime/pd_datetime.h""],
    sources=(
        [
            ""pandas/_libs/src/vendored/numpy/datetime/np_datetime.c"",
            ""pandas/_libs/src/vendored/numpy/datetime/np_datetime_strings.c"",
            ""pandas/_libs/src/datetime/date_conversions.c"",
            ""pandas/_libs/src/datetime/pd_datetime.c"",
        ]
    ),
    include_dirs=[
        ""pandas/_libs/include"",
        numpy.get_include(),
    ],
    extra_compile_args=(extra_compile_args),
    extra_link_args=extra_link_args,
    define_macros=macros,
)


extensions.append(pd_dt_ext)

# ----------------------------------------------------------------------

# ----------------------------------------------------------------------
# pd_datetime
pd_parser_ext = Extension(
    ""pandas._libs.pandas_parser"",
    depends=[""pandas/_libs/include/pandas/parser/pd_parser.h""],
    sources=(
        [
            ""pandas/_libs/src/parser/tokenizer.c"",
            ""pandas/_libs/src/parser/io.c"",
            ""pandas/_libs/src/parser/pd_parser.c"",
        ]
    ),
    include_dirs=[
        ""pandas/_libs/include"",
    ],
    extra_compile_args=(extra_compile_args),
    extra_link_args=extra_link_args,
    define_macros=macros,
)


extensions.append(pd_parser_ext)


# ----------------------------------------------------------------------


if __name__ == ""__main__"":
    # Freeze to support parallel compilation when using spawn instead of fork
    multiprocessing.freeze_support()
    setup(
        version=versioneer.get_version(),
        ext_modules=maybe_cythonize(extensions, compiler_directives=directives),
        cmdclass=cmdclass,
    )
"
karpathy_nanoGPT,karpathy_nanoGPT_model.py,16345,https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py,"""""""
Full definition of a GPT Language Model, all of it in this single file.
References:
1) the official GPT-2 TensorFlow implementation released by OpenAI:
https://github.com/openai/gpt-2/blob/master/src/model.py
2) huggingface/transformers PyTorch implementation:
https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py
""""""

import math
import inspect
from dataclasses import dataclass

import torch
import torch.nn as nn
from torch.nn import functional as F

class LayerNorm(nn.Module):
    """""" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """"""

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print(""WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0"")
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(""bias"", torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # with weight tying when using torch.compile() some warnings get generated:
        # ""UserWarning: functional_call was passed multiple values for tied weights.
        # This behavior is deprecated and will be an error in future versions""
        # not 100% sure what this is, so far seems to be harmless. TODO investigate
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

        # init all weights
        self.apply(self._init_weights)
        # apply special scaled init to the residual projections, per GPT-2 paper
        for pn, p in self.named_parameters():
            if pn.endswith('c_proj.weight'):
                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))

        # report number of parameters
        print(""number of parameters: %.2fM"" % (self.get_num_params()/1e6,))

    def get_num_params(self, non_embedding=True):
        """"""
        Return the number of parameters in the model.
        For non-embedding count (default), the position embeddings get subtracted.
        The token embeddings would too, except due to the parameter sharing these
        params are actually used as weights in the final layer, so we include them.
        """"""
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            n_params -= self.transformer.wpe.weight.numel()
        return n_params

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        assert t <= self.config.block_size, f""Cannot forward sequence of length {t}, block size is only {self.config.block_size}""
        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)

        # forward the GPT model itself
        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
        x = self.transformer.drop(tok_emb + pos_emb)
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            loss = None

        return logits, loss

    def crop_block_size(self, block_size):
        # model surgery to decrease the block size if necessary
        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
        # but want to use a smaller block size for some smaller, simpler model
        assert block_size <= self.config.block_size
        self.config.block_size = block_size
        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
        for block in self.transformer.h:
            if hasattr(block.attn, 'bias'):
                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]

    @classmethod
    def from_pretrained(cls, model_type, override_args=None):
        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}
        override_args = override_args or {} # default to empty dict
        # only dropout can be overridden see more notes below
        assert all(k == 'dropout' for k in override_args)
        from transformers import GPT2LMHeadModel
        print(""loading weights from pretrained gpt: %s"" % model_type)

        # n_layer, n_head and n_embd are determined from model_type
        config_args = {
            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
        }[model_type]
        print(""forcing vocab_size=50257, block_size=1024, bias=True"")
        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints
        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints
        config_args['bias'] = True # always True for GPT model checkpoints
        # we can override the dropout rate, if desired
        if 'dropout' in override_args:
            print(f""overriding dropout rate to {override_args['dropout']}"")
            config_args['dropout'] = override_args['dropout']
        # create a from-scratch initialized minGPT model
        config = GPTConfig(**config_args)
        model = GPT(config)
        sd = model.state_dict()
        sd_keys = sd.keys()
        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param

        # init a huggingface/transformers model
        model_hf = GPT2LMHeadModel.from_pretrained(model_type)
        sd_hf = model_hf.state_dict()

        # copy while ensuring all of the parameters are aligned and match in names and shapes
        sd_keys_hf = sd_hf.keys()
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)
        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']
        # basically the openai checkpoints use a ""Conv1D"" module, but we only want to use a vanilla Linear
        # this means that we have to transpose these weights when we import them
        assert len(sd_keys_hf) == len(sd_keys), f""mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}""
        for k in sd_keys_hf:
            if any(k.endswith(w) for w in transposed):
                # special treatment for the Conv1D weights we need to transpose
                assert sd_hf[k].shape[::-1] == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k].t())
            else:
                # vanilla copy over the other parameters
                assert sd_hf[k].shape == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k])

        return model

    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
        # start with all of the candidate parameters
        param_dict = {pn: p for pn, p in self.named_parameters()}
        # filter out those that do not require grad
        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
        optim_groups = [
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f""num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters"")
        print(f""num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters"")
        # Create AdamW optimizer and use the fused version if it is available
        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
        use_fused = fused_available and device_type == 'cuda'
        extra_args = dict(fused=True) if use_fused else dict()
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)
        print(f""using fused AdamW: {use_fused}"")

        return optimizer

    def estimate_mfu(self, fwdbwd_per_iter, dt):
        """""" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """"""
        # first estimate the number of flops we do per iteration.
        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
        N = self.get_num_params()
        cfg = self.config
        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
        flops_per_token = 6*N + 12*L*H*Q*T
        flops_per_fwdbwd = flops_per_token * T
        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
        # express our flops throughput as ratio of A100 bfloat16 peak flops
        flops_achieved = flops_per_iter * (1.0/dt) # per second
        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
        mfu = flops_achieved / flops_promised
        return mfu

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """"""
        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you'll want to make sure to be in model.eval() mode of operation for this.
        """"""
        for _ in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
            # forward the model to get the logits for the index in the sequence
            logits, _ = self(idx_cond)
            # pluck the logits at the final step and scale by desired temperature
            logits = logits[:, -1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)

        return idx
"
vllm-project_vllm,vllm-project_vllm_collect_env.py,26257,https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py,"# SPDX-License-Identifier: Apache-2.0

# ruff: noqa
# code borrowed from https://github.com/pytorch/pytorch/blob/main/torch/utils/collect_env.py

import datetime
import locale
import os
import re
import subprocess
import sys
# Unlike the rest of the PyTorch this file must be python2 compliant.
# This script outputs relevant system environment info
# Run it with `python collect_env.py` or `python -m torch.utils.collect_env`
from collections import namedtuple

from vllm.envs import environment_variables

try:
    import torch
    TORCH_AVAILABLE = True
except (ImportError, NameError, AttributeError, OSError):
    TORCH_AVAILABLE = False

# System Environment Information
SystemEnv = namedtuple(
    'SystemEnv',
    [
        'torch_version',
        'is_debug_build',
        'cuda_compiled_version',
        'gcc_version',
        'clang_version',
        'cmake_version',
        'os',
        'libc_version',
        'python_version',
        'python_platform',
        'is_cuda_available',
        'cuda_runtime_version',
        'cuda_module_loading',
        'nvidia_driver_version',
        'nvidia_gpu_models',
        'cudnn_version',
        'pip_version',  # 'pip' or 'pip3'
        'pip_packages',
        'conda_packages',
        'hip_compiled_version',
        'hip_runtime_version',
        'miopen_runtime_version',
        'caching_allocator_config',
        'is_xnnpack_available',
        'cpu_info',
        'rocm_version',  # vllm specific field
        'neuron_sdk_version',  # vllm specific field
        'vllm_version',  # vllm specific field
        'vllm_build_flags',  # vllm specific field
        'gpu_topo',  # vllm specific field
        'env_vars',
    ])

DEFAULT_CONDA_PATTERNS = {
    ""torch"",
    ""numpy"",
    ""cudatoolkit"",
    ""soumith"",
    ""mkl"",
    ""magma"",
    ""triton"",
    ""optree"",
    ""nccl"",
    ""transformers"",
    ""zmq"",
    ""nvidia"",
    ""pynvml"",
}

DEFAULT_PIP_PATTERNS = {
    ""torch"",
    ""numpy"",
    ""mypy"",
    ""flake8"",
    ""triton"",
    ""optree"",
    ""onnx"",
    ""nccl"",
    ""transformers"",
    ""zmq"",
    ""nvidia"",
    ""pynvml"",
}


def run(command):
    """"""Return (return-code, stdout, stderr).""""""
    shell = True if type(command) is str else False
    p = subprocess.Popen(command,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE,
                         shell=shell)
    raw_output, raw_err = p.communicate()
    rc = p.returncode
    if get_platform() == 'win32':
        enc = 'oem'
    else:
        enc = locale.getpreferredencoding()
    output = raw_output.decode(enc)
    err = raw_err.decode(enc)
    return rc, output.strip(), err.strip()


def run_and_read_all(run_lambda, command):
    """"""Run command using run_lambda; reads and returns entire output if rc is 0.""""""
    rc, out, _ = run_lambda(command)
    if rc != 0:
        return None
    return out


def run_and_parse_first_match(run_lambda, command, regex):
    """"""Run command using run_lambda, returns the first regex match if it exists.""""""
    rc, out, _ = run_lambda(command)
    if rc != 0:
        return None
    match = re.search(regex, out)
    if match is None:
        return None
    return match.group(1)


def run_and_return_first_line(run_lambda, command):
    """"""Run command using run_lambda and returns first line if output is not empty.""""""
    rc, out, _ = run_lambda(command)
    if rc != 0:
        return None
    return out.split('\n')[0]


def get_conda_packages(run_lambda, patterns=None):
    if patterns is None:
        patterns = DEFAULT_CONDA_PATTERNS
    conda = os.environ.get('CONDA_EXE', 'conda')
    out = run_and_read_all(run_lambda, ""{} list"".format(conda))
    if out is None:
        return out

    return ""\n"".join(line for line in out.splitlines()
                     if not line.startswith(""#"") and any(name in line
                                                         for name in patterns))


def get_gcc_version(run_lambda):
    return run_and_parse_first_match(run_lambda, 'gcc --version', r'gcc (.*)')


def get_clang_version(run_lambda):
    return run_and_parse_first_match(run_lambda, 'clang --version',
                                     r'clang version (.*)')


def get_cmake_version(run_lambda):
    return run_and_parse_first_match(run_lambda, 'cmake --version',
                                     r'cmake (.*)')


def get_nvidia_driver_version(run_lambda):
    if get_platform() == 'darwin':
        cmd = 'kextstat | grep -i cuda'
        return run_and_parse_first_match(run_lambda, cmd,
                                         r'com[.]nvidia[.]CUDA [(](.*?)[)]')
    smi = get_nvidia_smi()
    return run_and_parse_first_match(run_lambda, smi,
                                     r'Driver Version: (.*?) ')


def get_gpu_info(run_lambda):
    if get_platform() == 'darwin' or (TORCH_AVAILABLE and hasattr(
            torch.version, 'hip') and torch.version.hip is not None):
        if TORCH_AVAILABLE and torch.cuda.is_available():
            if torch.version.hip is not None:
                prop = torch.cuda.get_device_properties(0)
                if hasattr(prop, ""gcnArchName""):
                    gcnArch = "" ({})"".format(prop.gcnArchName)
                else:
                    gcnArch = ""NoGCNArchNameOnOldPyTorch""
            else:
                gcnArch = """"
            return torch.cuda.get_device_name(None) + gcnArch
        return None
    smi = get_nvidia_smi()
    uuid_regex = re.compile(r' \(UUID: .+?\)')
    rc, out, _ = run_lambda(smi + ' -L')
    if rc != 0:
        return None
    # Anonymize GPUs by removing their UUID
    return re.sub(uuid_regex, '', out)


def get_running_cuda_version(run_lambda):
    return run_and_parse_first_match(run_lambda, 'nvcc --version',
                                     r'release .+ V(.*)')


def get_cudnn_version(run_lambda):
    """"""Return a list of libcudnn.so; it's hard to tell which one is being used.""""""
    if get_platform() == 'win32':
        system_root = os.environ.get('SYSTEMROOT', 'C:\\Windows')
        cuda_path = os.environ.get('CUDA_PATH', ""%CUDA_PATH%"")
        where_cmd = os.path.join(system_root, 'System32', 'where')
        cudnn_cmd = '{} /R ""{}\\bin"" cudnn*.dll'.format(where_cmd, cuda_path)
    elif get_platform() == 'darwin':
        # CUDA libraries and drivers can be found in /usr/local/cuda/. See
        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install
        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac
        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.
        cudnn_cmd = 'ls /usr/local/cuda/lib/libcudnn*'
    else:
        cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d"" "" -f1 | rev'
    rc, out, _ = run_lambda(cudnn_cmd)
    # find will return 1 if there are permission errors or if not found
    if len(out) == 0 or (rc != 1 and rc != 0):
        l = os.environ.get('CUDNN_LIBRARY')
        if l is not None and os.path.isfile(l):
            return os.path.realpath(l)
        return None
    files_set = set()
    for fn in out.split('\n'):
        fn = os.path.realpath(fn)  # eliminate symbolic links
        if os.path.isfile(fn):
            files_set.add(fn)
    if not files_set:
        return None
    # Alphabetize the result because the order is non-deterministic otherwise
    files = sorted(files_set)
    if len(files) == 1:
        return files[0]
    result = '\n'.join(files)
    return 'Probably one of the following:\n{}'.format(result)


def get_nvidia_smi():
    # Note: nvidia-smi is currently available only on Windows and Linux
    smi = 'nvidia-smi'
    if get_platform() == 'win32':
        system_root = os.environ.get('SYSTEMROOT', 'C:\\Windows')
        program_files_root = os.environ.get('PROGRAMFILES',
                                            'C:\\Program Files')
        legacy_path = os.path.join(program_files_root, 'NVIDIA Corporation',
                                   'NVSMI', smi)
        new_path = os.path.join(system_root, 'System32', smi)
        smis = [new_path, legacy_path]
        for candidate_smi in smis:
            if os.path.exists(candidate_smi):
                smi = '""{}""'.format(candidate_smi)
                break
    return smi


def get_rocm_version(run_lambda):
    """"""Returns the ROCm version if available, otherwise 'N/A'.""""""
    return run_and_parse_first_match(run_lambda, 'hipcc --version',
                                     r'HIP version: (\S+)')


def get_neuron_sdk_version(run_lambda):
    # Adapted from your install script
    try:
        result = run_lambda([""neuron-ls""])
        return result if result[0] == 0 else 'N/A'
    except Exception:
        return 'N/A'


def get_vllm_version():
    from vllm import __version__, __version_tuple__

    if __version__ == ""dev"":
        return ""N/A (dev)""

    if len(__version_tuple__) == 4: # dev build
        git_sha = __version_tuple__[-1][1:] # type: ignore
        return f""{__version__} (git sha: {git_sha}""

    return __version__

def summarize_vllm_build_flags():
    # This could be a static method if the flags are constant, or dynamic if you need to check environment variables, etc.
    return 'CUDA Archs: {}; ROCm: {}; Neuron: {}'.format(
        os.environ.get('TORCH_CUDA_ARCH_LIST', 'Not Set'),
        'Enabled' if os.environ.get('ROCM_HOME') else 'Disabled',
        'Enabled' if os.environ.get('NEURON_CORES') else 'Disabled',
    )


def get_gpu_topo(run_lambda):
    output = None

    if get_platform() == 'linux':
        output = run_and_read_all(run_lambda, 'nvidia-smi topo -m')
        if output is None:
            output = run_and_read_all(run_lambda, 'rocm-smi --showtopo')

    return output


# example outputs of CPU infos
#  * linux
#    Architecture:            x86_64
#      CPU op-mode(s):        32-bit, 64-bit
#      Address sizes:         46 bits physical, 48 bits virtual
#      Byte Order:            Little Endian
#    CPU(s):                  128
#      On-line CPU(s) list:   0-127
#    Vendor ID:               GenuineIntel
#      Model name:            Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz
#        CPU family:          6
#        Model:               106
#        Thread(s) per core:  2
#        Core(s) per socket:  32
#        Socket(s):           2
#        Stepping:            6
#        BogoMIPS:            5799.78
#        Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr
#                             sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl
#                             xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16
#                             pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand
#                             hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced
#                             fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap
#                             avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1
#                             xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq
#                             avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities
#    Virtualization features:
#      Hypervisor vendor:     KVM
#      Virtualization type:   full
#    Caches (sum of all):
#      L1d:                   3 MiB (64 instances)
#      L1i:                   2 MiB (64 instances)
#      L2:                    80 MiB (64 instances)
#      L3:                    108 MiB (2 instances)
#    NUMA:
#      NUMA node(s):          2
#      NUMA node0 CPU(s):     0-31,64-95
#      NUMA node1 CPU(s):     32-63,96-127
#    Vulnerabilities:
#      Itlb multihit:         Not affected
#      L1tf:                  Not affected
#      Mds:                   Not affected
#      Meltdown:              Not affected
#      Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
#      Retbleed:              Not affected
#      Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp
#      Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization
#      Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
#      Srbds:                 Not affected
#      Tsx async abort:       Not affected
#  * win32
#    Architecture=9
#    CurrentClockSpeed=2900
#    DeviceID=CPU0
#    Family=179
#    L2CacheSize=40960
#    L2CacheSpeed=
#    Manufacturer=GenuineIntel
#    MaxClockSpeed=2900
#    Name=Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz
#    ProcessorType=3
#    Revision=27142
#
#    Architecture=9
#    CurrentClockSpeed=2900
#    DeviceID=CPU1
#    Family=179
#    L2CacheSize=40960
#    L2CacheSpeed=
#    Manufacturer=GenuineIntel
#    MaxClockSpeed=2900
#    Name=Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz
#    ProcessorType=3
#    Revision=27142


def get_cpu_info(run_lambda):
    rc, out, err = 0, '', ''
    if get_platform() == 'linux':
        rc, out, err = run_lambda('lscpu')
    elif get_platform() == 'win32':
        rc, out, err = run_lambda(
            'wmic cpu get Name,Manufacturer,Family,Architecture,ProcessorType,DeviceID, \
        CurrentClockSpeed,MaxClockSpeed,L2CacheSize,L2CacheSpeed,Revision /VALUE'
        )
    elif get_platform() == 'darwin':
        rc, out, err = run_lambda(""sysctl -n machdep.cpu.brand_string"")
    cpu_info = 'None'
    if rc == 0:
        cpu_info = out
    else:
        cpu_info = err
    return cpu_info


def get_platform():
    if sys.platform.startswith('linux'):
        return 'linux'
    elif sys.platform.startswith('win32'):
        return 'win32'
    elif sys.platform.startswith('cygwin'):
        return 'cygwin'
    elif sys.platform.startswith('darwin'):
        return 'darwin'
    else:
        return sys.platform


def get_mac_version(run_lambda):
    return run_and_parse_first_match(run_lambda, 'sw_vers -productVersion',
                                     r'(.*)')


def get_windows_version(run_lambda):
    system_root = os.environ.get('SYSTEMROOT', 'C:\\Windows')
    wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')
    findstr_cmd = os.path.join(system_root, 'System32', 'findstr')
    return run_and_read_all(
        run_lambda,
        '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))


def get_lsb_version(run_lambda):
    return run_and_parse_first_match(run_lambda, 'lsb_release -a',
                                     r'Description:\t(.*)')


def check_release_file(run_lambda):
    return run_and_parse_first_match(run_lambda, 'cat /etc/*-release',
                                     r'PRETTY_NAME=""(.*)""')


def get_os(run_lambda):
    from platform import machine
    platform = get_platform()

    if platform == 'win32' or platform == 'cygwin':
        return get_windows_version(run_lambda)

    if platform == 'darwin':
        version = get_mac_version(run_lambda)
        if version is None:
            return None
        return 'macOS {} ({})'.format(version, machine())

    if platform == 'linux':
        # Ubuntu/Debian based
        desc = get_lsb_version(run_lambda)
        if desc is not None:
            return '{} ({})'.format(desc, machine())

        # Try reading /etc/*-release
        desc = check_release_file(run_lambda)
        if desc is not None:
            return '{} ({})'.format(desc, machine())

        return '{} ({})'.format(platform, machine())

    # Unknown platform
    return platform


def get_python_platform():
    import platform
    return platform.platform()


def get_libc_version():
    import platform
    if get_platform() != 'linux':
        return 'N/A'
    return '-'.join(platform.libc_ver())


def get_pip_packages(run_lambda, patterns=None):
    """"""Return `pip list` output. Note: will also find conda-installed pytorch and numpy packages.""""""
    if patterns is None:
        patterns = DEFAULT_PIP_PATTERNS

    # People generally have `pip` as `pip` or `pip3`
    # But here it is invoked as `python -mpip`
    def run_with_pip(pip):
        out = run_and_read_all(run_lambda, pip + [""list"", ""--format=freeze""])
        return ""\n"".join(line for line in out.splitlines()
                         if any(name in line for name in patterns))

    pip_version = 'pip3' if sys.version[0] == '3' else 'pip'
    out = run_with_pip([sys.executable, '-mpip'])

    return pip_version, out


def get_cachingallocator_config():
    ca_config = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')
    return ca_config


def get_cuda_module_loading_config():
    if TORCH_AVAILABLE and torch.cuda.is_available():
        torch.cuda.init()
        config = os.environ.get('CUDA_MODULE_LOADING', '')
        return config
    else:
        return ""N/A""


def is_xnnpack_available():
    if TORCH_AVAILABLE:
        import torch.backends.xnnpack
        return str(
            torch.backends.xnnpack.enabled)  # type: ignore[attr-defined]
    else:
        return ""N/A""

def get_env_vars():
    env_vars = ''
    secret_terms=('secret', 'token', 'api', 'access', 'password')
    report_prefix = (""TORCH"", ""NCCL"", ""PYTORCH"",
                     ""CUDA"", ""CUBLAS"", ""CUDNN"",
                     ""OMP_"", ""MKL_"",
                     ""NVIDIA"")
    for k, v in os.environ.items():
        if any(term in k.lower() for term in secret_terms):
            continue
        if k in environment_variables:
            env_vars = env_vars + ""{}={}"".format(k, v) + ""\n""
        if k.startswith(report_prefix):
            env_vars = env_vars + ""{}={}"".format(k, v) + ""\n""

    return env_vars

def get_env_info():
    run_lambda = run
    pip_version, pip_list_output = get_pip_packages(run_lambda)

    if TORCH_AVAILABLE:
        version_str = torch.__version__
        debug_mode_str = str(torch.version.debug)
        cuda_available_str = str(torch.cuda.is_available())
        cuda_version_str = torch.version.cuda
        if not hasattr(torch.version,
                       'hip') or torch.version.hip is None:  # cuda version
            hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'
        else:  # HIP version

            def get_version_or_na(cfg, prefix):
                _lst = [s.rsplit(None, 1)[-1] for s in cfg if prefix in s]
                return _lst[0] if _lst else 'N/A'

            cfg = torch._C._show_config().split('\n')
            hip_runtime_version = get_version_or_na(cfg, 'HIP Runtime')
            miopen_runtime_version = get_version_or_na(cfg, 'MIOpen')
            cuda_version_str = 'N/A'
            hip_compiled_version = torch.version.hip
    else:
        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'
        hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'

    sys_version = sys.version.replace(""\n"", "" "")

    conda_packages = get_conda_packages(run_lambda)

    rocm_version = get_rocm_version(run_lambda)
    neuron_sdk_version = get_neuron_sdk_version(run_lambda)
    vllm_version = get_vllm_version()
    vllm_build_flags = summarize_vllm_build_flags()
    gpu_topo = get_gpu_topo(run_lambda)

    return SystemEnv(
        torch_version=version_str,
        is_debug_build=debug_mode_str,
        python_version='{} ({}-bit runtime)'.format(
            sys_version,
            sys.maxsize.bit_length() + 1),
        python_platform=get_python_platform(),
        is_cuda_available=cuda_available_str,
        cuda_compiled_version=cuda_version_str,
        cuda_runtime_version=get_running_cuda_version(run_lambda),
        cuda_module_loading=get_cuda_module_loading_config(),
        nvidia_gpu_models=get_gpu_info(run_lambda),
        nvidia_driver_version=get_nvidia_driver_version(run_lambda),
        cudnn_version=get_cudnn_version(run_lambda),
        hip_compiled_version=hip_compiled_version,
        hip_runtime_version=hip_runtime_version,
        miopen_runtime_version=miopen_runtime_version,
        pip_version=pip_version,
        pip_packages=pip_list_output,
        conda_packages=conda_packages,
        os=get_os(run_lambda),
        libc_version=get_libc_version(),
        gcc_version=get_gcc_version(run_lambda),
        clang_version=get_clang_version(run_lambda),
        cmake_version=get_cmake_version(run_lambda),
        caching_allocator_config=get_cachingallocator_config(),
        is_xnnpack_available=is_xnnpack_available(),
        cpu_info=get_cpu_info(run_lambda),
        rocm_version=rocm_version,
        neuron_sdk_version=neuron_sdk_version,
        vllm_version=vllm_version,
        vllm_build_flags=vllm_build_flags,
        gpu_topo=gpu_topo,
        env_vars=get_env_vars(),
    )


env_info_fmt = """"""
PyTorch version: {torch_version}
Is debug build: {is_debug_build}
CUDA used to build PyTorch: {cuda_compiled_version}
ROCM used to build PyTorch: {hip_compiled_version}

OS: {os}
GCC version: {gcc_version}
Clang version: {clang_version}
CMake version: {cmake_version}
Libc version: {libc_version}

Python version: {python_version}
Python platform: {python_platform}
Is CUDA available: {is_cuda_available}
CUDA runtime version: {cuda_runtime_version}
CUDA_MODULE_LOADING set to: {cuda_module_loading}
GPU models and configuration: {nvidia_gpu_models}
Nvidia driver version: {nvidia_driver_version}
cuDNN version: {cudnn_version}
HIP runtime version: {hip_runtime_version}
MIOpen runtime version: {miopen_runtime_version}
Is XNNPACK available: {is_xnnpack_available}

CPU:
{cpu_info}

Versions of relevant libraries:
{pip_packages}
{conda_packages}
"""""".strip()

# both the above code and the following code use `strip()` to
# remove leading/trailing whitespaces, so we need to add a newline
# in between to separate the two sections
env_info_fmt += ""\n""

env_info_fmt += """"""
ROCM Version: {rocm_version}
Neuron SDK Version: {neuron_sdk_version}
vLLM Version: {vllm_version}
vLLM Build Flags:
{vllm_build_flags}
GPU Topology:
{gpu_topo}

{env_vars}
"""""".strip()


def pretty_str(envinfo):

    def replace_nones(dct, replacement='Could not collect'):
        for key in dct.keys():
            if dct[key] is not None:
                continue
            dct[key] = replacement
        return dct

    def replace_bools(dct, true='Yes', false='No'):
        for key in dct.keys():
            if dct[key] is True:
                dct[key] = true
            elif dct[key] is False:
                dct[key] = false
        return dct

    def prepend(text, tag='[prepend]'):
        lines = text.split('\n')
        updated_lines = [tag + line for line in lines]
        return '\n'.join(updated_lines)

    def replace_if_empty(text, replacement='No relevant packages'):
        if text is not None and len(text) == 0:
            return replacement
        return text

    def maybe_start_on_next_line(string):
        # If `string` is multiline, prepend a \n to it.
        if string is not None and len(string.split('\n')) > 1:
            return '\n{}\n'.format(string)
        return string

    mutable_dict = envinfo._asdict()

    # If nvidia_gpu_models is multiline, start on the next line
    mutable_dict['nvidia_gpu_models'] = \
        maybe_start_on_next_line(envinfo.nvidia_gpu_models)

    # If the machine doesn't have CUDA, report some fields as 'No CUDA'
    dynamic_cuda_fields = [
        'cuda_runtime_version',
        'nvidia_gpu_models',
        'nvidia_driver_version',
    ]
    all_cuda_fields = dynamic_cuda_fields + ['cudnn_version']
    all_dynamic_cuda_fields_missing = all(mutable_dict[field] is None
                                          for field in dynamic_cuda_fields)
    if TORCH_AVAILABLE and not torch.cuda.is_available(
    ) and all_dynamic_cuda_fields_missing:
        for field in all_cuda_fields:
            mutable_dict[field] = 'No CUDA'
        if envinfo.cuda_compiled_version is None:
            mutable_dict['cuda_compiled_version'] = 'None'

    # Replace True with Yes, False with No
    mutable_dict = replace_bools(mutable_dict)

    # Replace all None objects with 'Could not collect'
    mutable_dict = replace_nones(mutable_dict)

    # If either of these are '', replace with 'No relevant packages'
    mutable_dict['pip_packages'] = replace_if_empty(
        mutable_dict['pip_packages'])
    mutable_dict['conda_packages'] = replace_if_empty(
        mutable_dict['conda_packages'])

    # Tag conda and pip packages with a prefix
    # If they were previously None, they'll show up as ie '[conda] Could not collect'
    if mutable_dict['pip_packages']:
        mutable_dict['pip_packages'] = prepend(
            mutable_dict['pip_packages'], '[{}] '.format(envinfo.pip_version))
    if mutable_dict['conda_packages']:
        mutable_dict['conda_packages'] = prepend(
            mutable_dict['conda_packages'], '[conda] ')
    mutable_dict['cpu_info'] = envinfo.cpu_info
    return env_info_fmt.format(**mutable_dict)


def get_pretty_env_info():
    return pretty_str(get_env_info())


def main():
    print(""Collecting environment information..."")
    output = get_pretty_env_info()
    print(output)

    if TORCH_AVAILABLE and hasattr(torch, 'utils') and hasattr(
            torch.utils, '_crash_handler'):
        minidump_dir = torch.utils._crash_handler.DEFAULT_MINIDUMP_DIR
        if sys.platform == ""linux"" and os.path.exists(minidump_dir):
            dumps = [
                os.path.join(minidump_dir, dump)
                for dump in os.listdir(minidump_dir)
            ]
            latest = max(dumps, key=os.path.getctime)
            ctime = os.path.getctime(latest)
            creation_time = datetime.datetime.fromtimestamp(ctime).strftime(
                '%Y-%m-%d %H:%M:%S')
            msg = ""\n*** Detected a minidump at {} created on {}, "".format(latest, creation_time) + \
                  ""if this is related to your bug please include it when you file a report ***""
            print(msg, file=sys.stderr)


if __name__ == '__main__':
    main()
"
vllm-project_vllm,vllm-project_vllm_setup.py,23396,https://raw.githubusercontent.com/vllm-project/vllm/main/setup.py,"# SPDX-License-Identifier: Apache-2.0

import ctypes
import importlib.util
import logging
import os
import re
import subprocess
import sys
from pathlib import Path
from shutil import which
from typing import Dict, List

import torch
from packaging.version import Version, parse
from setuptools import Extension, setup
from setuptools.command.build_ext import build_ext
from setuptools_scm import get_version
from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME


def load_module_from_path(module_name, path):
    spec = importlib.util.spec_from_file_location(module_name, path)
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)
    return module


ROOT_DIR = os.path.dirname(__file__)
logger = logging.getLogger(__name__)

# cannot import envs directly because it depends on vllm,
#  which is not installed yet
envs = load_module_from_path('envs', os.path.join(ROOT_DIR, 'vllm', 'envs.py'))

VLLM_TARGET_DEVICE = envs.VLLM_TARGET_DEVICE

if sys.platform.startswith(""darwin"") and VLLM_TARGET_DEVICE != ""cpu"":
    logger.warning(
        ""VLLM_TARGET_DEVICE automatically set to `cpu` due to macOS"")
    VLLM_TARGET_DEVICE = ""cpu""
elif not (sys.platform.startswith(""linux"")
          or sys.platform.startswith(""darwin"")):
    logger.warning(
        ""vLLM only supports Linux platform (including WSL) and MacOS.""
        ""Building on %s, ""
        ""so vLLM may not be able to run correctly"", sys.platform)
    VLLM_TARGET_DEVICE = ""empty""
elif (sys.platform.startswith(""linux"") and torch.version.cuda is None
      and os.getenv(""VLLM_TARGET_DEVICE"") is None
      and torch.version.hip is None):
    # if cuda or hip is not available and VLLM_TARGET_DEVICE is not set,
    # fallback to cpu
    VLLM_TARGET_DEVICE = ""cpu""

MAIN_CUDA_VERSION = ""12.1""


def is_sccache_available() -> bool:
    return which(""sccache"") is not None


def is_ccache_available() -> bool:
    return which(""ccache"") is not None


def is_ninja_available() -> bool:
    return which(""ninja"") is not None


class CMakeExtension(Extension):

    def __init__(self, name: str, cmake_lists_dir: str = '.', **kwa) -> None:
        super().__init__(name, sources=[], py_limited_api=True, **kwa)
        self.cmake_lists_dir = os.path.abspath(cmake_lists_dir)


class cmake_build_ext(build_ext):
    # A dict of extension directories that have been configured.
    did_config: Dict[str, bool] = {}

    #
    # Determine number of compilation jobs and optionally nvcc compile threads.
    #
    def compute_num_jobs(self):
        # `num_jobs` is either the value of the MAX_JOBS environment variable
        # (if defined) or the number of CPUs available.
        num_jobs = envs.MAX_JOBS
        if num_jobs is not None:
            num_jobs = int(num_jobs)
            logger.info(""Using MAX_JOBS=%d as the number of jobs."", num_jobs)
        else:
            try:
                # os.sched_getaffinity() isn't universally available, so fall
                #  back to os.cpu_count() if we get an error here.
                num_jobs = len(os.sched_getaffinity(0))
            except AttributeError:
                num_jobs = os.cpu_count()

        nvcc_threads = None
        if _is_cuda() and get_nvcc_cuda_version() >= Version(""11.2""):
            # `nvcc_threads` is either the value of the NVCC_THREADS
            # environment variable (if defined) or 1.
            # when it is set, we reduce `num_jobs` to avoid
            # overloading the system.
            nvcc_threads = envs.NVCC_THREADS
            if nvcc_threads is not None:
                nvcc_threads = int(nvcc_threads)
                logger.info(
                    ""Using NVCC_THREADS=%d as the number of nvcc threads."",
                    nvcc_threads)
            else:
                nvcc_threads = 1
            num_jobs = max(1, num_jobs // nvcc_threads)

        return num_jobs, nvcc_threads

    #
    # Perform cmake configuration for a single extension.
    #
    def configure(self, ext: CMakeExtension) -> None:
        # If we've already configured using the CMakeLists.txt for
        # this extension, exit early.
        if ext.cmake_lists_dir in cmake_build_ext.did_config:
            return

        cmake_build_ext.did_config[ext.cmake_lists_dir] = True

        # Select the build type.
        # Note: optimization level + debug info are set by the build type
        default_cfg = ""Debug"" if self.debug else ""RelWithDebInfo""
        cfg = envs.CMAKE_BUILD_TYPE or default_cfg

        cmake_args = [
            '-DCMAKE_BUILD_TYPE={}'.format(cfg),
            '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
        ]

        verbose = envs.VERBOSE
        if verbose:
            cmake_args += ['-DCMAKE_VERBOSE_MAKEFILE=ON']

        if is_sccache_available():
            cmake_args += [
                '-DCMAKE_C_COMPILER_LAUNCHER=sccache',
                '-DCMAKE_CXX_COMPILER_LAUNCHER=sccache',
                '-DCMAKE_CUDA_COMPILER_LAUNCHER=sccache',
                '-DCMAKE_HIP_COMPILER_LAUNCHER=sccache',
            ]
        elif is_ccache_available():
            cmake_args += [
                '-DCMAKE_C_COMPILER_LAUNCHER=ccache',
                '-DCMAKE_CXX_COMPILER_LAUNCHER=ccache',
                '-DCMAKE_CUDA_COMPILER_LAUNCHER=ccache',
                '-DCMAKE_HIP_COMPILER_LAUNCHER=ccache',
            ]

        # Pass the python executable to cmake so it can find an exact
        # match.
        cmake_args += ['-DVLLM_PYTHON_EXECUTABLE={}'.format(sys.executable)]

        # Pass the python path to cmake so it can reuse the build dependencies
        # on subsequent calls to python.
        cmake_args += ['-DVLLM_PYTHON_PATH={}'.format("":"".join(sys.path))]

        # Override the base directory for FetchContent downloads to $ROOT/.deps
        # This allows sharing dependencies between profiles,
        # and plays more nicely with sccache.
        # To override this, set the FETCHCONTENT_BASE_DIR environment variable.
        fc_base_dir = os.path.join(ROOT_DIR, "".deps"")
        fc_base_dir = os.environ.get(""FETCHCONTENT_BASE_DIR"", fc_base_dir)
        cmake_args += ['-DFETCHCONTENT_BASE_DIR={}'.format(fc_base_dir)]

        #
        # Setup parallelism and build tool
        #
        num_jobs, nvcc_threads = self.compute_num_jobs()

        if nvcc_threads:
            cmake_args += ['-DNVCC_THREADS={}'.format(nvcc_threads)]

        if is_ninja_available():
            build_tool = ['-G', 'Ninja']
            cmake_args += [
                '-DCMAKE_JOB_POOL_COMPILE:STRING=compile',
                '-DCMAKE_JOB_POOLS:STRING=compile={}'.format(num_jobs),
            ]
        else:
            # Default build tool to whatever cmake picks.
            build_tool = []
        subprocess.check_call(
            ['cmake', ext.cmake_lists_dir, *build_tool, *cmake_args],
            cwd=self.build_temp)

    def build_extensions(self) -> None:
        # Ensure that CMake is present and working
        try:
            subprocess.check_output(['cmake', '--version'])
        except OSError as e:
            raise RuntimeError('Cannot find CMake executable') from e

        # Create build directory if it does not exist.
        if not os.path.exists(self.build_temp):
            os.makedirs(self.build_temp)

        targets = []

        def target_name(s: str) -> str:
            return s.removeprefix(""vllm."").removeprefix(""vllm_flash_attn."")

        # Build all the extensions
        for ext in self.extensions:
            self.configure(ext)
            targets.append(target_name(ext.name))

        num_jobs, _ = self.compute_num_jobs()

        build_args = [
            ""--build"",
            ""."",
            f""-j={num_jobs}"",
            *[f""--target={name}"" for name in targets],
        ]

        subprocess.check_call([""cmake"", *build_args], cwd=self.build_temp)

        # Install the libraries
        for ext in self.extensions:
            # Install the extension into the proper location
            outdir = Path(self.get_ext_fullpath(ext.name)).parent.absolute()

            # Skip if the install directory is the same as the build directory
            if outdir == self.build_temp:
                continue

            # CMake appends the extension prefix to the install path,
            # and outdir already contains that prefix, so we need to remove it.
            # We assume only the final component of extension prefix is added by
            # CMake, this is currently true for current extensions but may not
            # always be the case.
            prefix = outdir
            if '.' in ext.name:
                prefix = prefix.parent

            # prefix here should actually be the same for all components
            install_args = [
                ""cmake"", ""--install"", ""."", ""--prefix"", prefix, ""--component"",
                target_name(ext.name)
            ]
            subprocess.check_call(install_args, cwd=self.build_temp)

    def run(self):
        # First, run the standard build_ext command to compile the extensions
        super().run()

        # copy vllm/vllm_flash_attn/*.py from self.build_lib to current
        # directory so that they can be included in the editable build
        import glob
        files = glob.glob(
            os.path.join(self.build_lib, ""vllm"", ""vllm_flash_attn"", ""*.py""))
        for file in files:
            dst_file = os.path.join(""vllm/vllm_flash_attn"",
                                    os.path.basename(file))
            print(f""Copying {file} to {dst_file}"")
            self.copy_file(file, dst_file)


class repackage_wheel(build_ext):
    """"""Extracts libraries and other files from an existing wheel.""""""

    def get_base_commit_in_main_branch(self) -> str:
        import subprocess

        try:
            current_branch = subprocess.check_output(
                [""git"", ""branch"", ""--show-current""]).decode(""utf-8"").strip()

            base_commit = subprocess.check_output(
                [""git"", ""merge-base"", ""main"",
                 current_branch]).decode(""utf-8"").strip()
            return base_commit
        except Exception as err:
            logger.warning(
                ""Failed to get the base commit in the main branch. ""
                ""Using the nightly wheel. The libraries in this ""
                ""wheel may not be compatible with your dev branch: %s"", err)
            return ""nightly""

    def run(self) -> None:
        assert _is_cuda(
        ), ""VLLM_USE_PRECOMPILED is only supported for CUDA builds""

        wheel_location = os.getenv(""VLLM_PRECOMPILED_WHEEL_LOCATION"", None)
        if wheel_location is None:
            base_commit = self.get_base_commit_in_main_branch()
            wheel_location = f""https://wheels.vllm.ai/{base_commit}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl""

        import zipfile

        if os.path.isfile(wheel_location):
            wheel_path = wheel_location
            print(f""Using existing wheel={wheel_path}"")
        else:
            # Download the wheel from a given URL, assume
            # the filename is the last part of the URL
            wheel_filename = wheel_location.split(""/"")[-1]

            import tempfile

            # create a temporary directory to store the wheel
            temp_dir = tempfile.mkdtemp(prefix=""vllm-wheels"")
            wheel_path = os.path.join(temp_dir, wheel_filename)

            print(f""Downloading wheel from {wheel_location} to {wheel_path}"")

            from urllib.request import urlretrieve

            try:
                urlretrieve(wheel_location, filename=wheel_path)
            except Exception as e:
                from setuptools.errors import SetupError

                raise SetupError(
                    f""Failed to get vLLM wheel from {wheel_location}"") from e

        with zipfile.ZipFile(wheel_path) as wheel:
            files_to_copy = [
                ""vllm/_C.abi3.so"",
                ""vllm/_moe_C.abi3.so"",
                ""vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so"",
                ""vllm/vllm_flash_attn/_vllm_fa3_C.abi3.so"",
                ""vllm/vllm_flash_attn/flash_attn_interface.py"",
                ""vllm/vllm_flash_attn/__init__.py"",
                ""vllm/cumem_allocator.abi3.so"",
                # ""vllm/_version.py"", # not available in nightly wheels yet
            ]
            file_members = filter(lambda x: x.filename in files_to_copy,
                                  wheel.filelist)

            for file in file_members:
                print(f""Extracting and including {file.filename} ""
                      ""from existing wheel"")
                package_name = os.path.dirname(file.filename).replace(""/"", ""."")
                file_name = os.path.basename(file.filename)

                if package_name not in package_data:
                    package_data[package_name] = []

                wheel.extract(file)
                if file_name.endswith("".py""):
                    # python files shouldn't be added to package_data
                    continue

                package_data[package_name].append(file_name)


def _is_hpu() -> bool:
    # if VLLM_TARGET_DEVICE env var was set explicitly, skip HPU autodetection
    if os.getenv(""VLLM_TARGET_DEVICE"", None) == VLLM_TARGET_DEVICE:
        return VLLM_TARGET_DEVICE == ""hpu""

    # if VLLM_TARGET_DEVICE was not set explicitly, check if hl-smi succeeds,
    # and if it doesn't, check if habanalabs driver is loaded
    is_hpu_available = False
    try:
        out = subprocess.run([""hl-smi""], capture_output=True, check=True)
        is_hpu_available = out.returncode == 0
    except (FileNotFoundError, PermissionError, subprocess.CalledProcessError):
        if sys.platform.startswith(""linux""):
            try:
                output = subprocess.check_output(
                    'lsmod | grep habanalabs | wc -l', shell=True)
                is_hpu_available = int(output) > 0
            except (ValueError, FileNotFoundError, PermissionError,
                    subprocess.CalledProcessError):
                pass
    return is_hpu_available


def _no_device() -> bool:
    return VLLM_TARGET_DEVICE == ""empty""


def _is_cuda() -> bool:
    has_cuda = torch.version.cuda is not None
    return (VLLM_TARGET_DEVICE == ""cuda"" and has_cuda
            and not (_is_neuron() or _is_tpu() or _is_hpu()))


def _is_hip() -> bool:
    return (VLLM_TARGET_DEVICE == ""cuda""
            or VLLM_TARGET_DEVICE == ""rocm"") and torch.version.hip is not None


def _is_neuron() -> bool:
    return VLLM_TARGET_DEVICE == ""neuron""


def _is_tpu() -> bool:
    return VLLM_TARGET_DEVICE == ""tpu""


def _is_cpu() -> bool:
    return VLLM_TARGET_DEVICE == ""cpu""


def _is_openvino() -> bool:
    return VLLM_TARGET_DEVICE == ""openvino""


def _is_xpu() -> bool:
    return VLLM_TARGET_DEVICE == ""xpu""


def _build_custom_ops() -> bool:
    return _is_cuda() or _is_hip() or _is_cpu()


def get_rocm_version():
    # Get the Rocm version from the ROCM_HOME/bin/librocm-core.so
    # see https://github.com/ROCm/rocm-core/blob/d11f5c20d500f729c393680a01fa902ebf92094b/rocm_version.cpp#L21
    try:
        librocm_core_file = Path(ROCM_HOME) / ""lib"" / ""librocm-core.so""
        if not librocm_core_file.is_file():
            return None
        librocm_core = ctypes.CDLL(librocm_core_file)
        VerErrors = ctypes.c_uint32
        get_rocm_core_version = librocm_core.getROCmVersion
        get_rocm_core_version.restype = VerErrors
        get_rocm_core_version.argtypes = [
            ctypes.POINTER(ctypes.c_uint32),
            ctypes.POINTER(ctypes.c_uint32),
            ctypes.POINTER(ctypes.c_uint32),
        ]
        major = ctypes.c_uint32()
        minor = ctypes.c_uint32()
        patch = ctypes.c_uint32()

        if (get_rocm_core_version(ctypes.byref(major), ctypes.byref(minor),
                                  ctypes.byref(patch)) == 0):
            return f""{major.value}.{minor.value}.{patch.value}""
        return None
    except Exception:
        return None


def get_neuronxcc_version():
    import sysconfig
    site_dir = sysconfig.get_paths()[""purelib""]
    version_file = os.path.join(site_dir, ""neuronxcc"", ""version"",
                                ""__init__.py"")

    # Check if the command was executed successfully
    with open(version_file) as fp:
        content = fp.read()

    # Extract the version using a regular expression
    match = re.search(r""__version__ = '(\S+)'"", content)
    if match:
        # Return the version string
        return match.group(1)
    else:
        raise RuntimeError(""Could not find Neuron version in the output"")


def get_nvcc_cuda_version() -> Version:
    """"""Get the CUDA version from nvcc.

    Adapted from https://github.com/NVIDIA/apex/blob/8b7a1ff183741dd8f9b87e7bafd04cfde99cea28/setup.py
    """"""
    assert CUDA_HOME is not None, ""CUDA_HOME is not set""
    nvcc_output = subprocess.check_output([CUDA_HOME + ""/bin/nvcc"", ""-V""],
                                          universal_newlines=True)
    output = nvcc_output.split()
    release_idx = output.index(""release"") + 1
    nvcc_cuda_version = parse(output[release_idx].split("","")[0])
    return nvcc_cuda_version


def get_path(*filepath) -> str:
    return os.path.join(ROOT_DIR, *filepath)


def get_gaudi_sw_version():
    """"""
    Returns the driver version.
    """"""
    # Enable console printing for `hl-smi` check
    output = subprocess.run(""hl-smi"",
                            shell=True,
                            text=True,
                            capture_output=True,
                            env={""ENABLE_CONSOLE"": ""true""})
    if output.returncode == 0 and output.stdout:
        return output.stdout.split(""\n"")[2].replace(
            "" "", """").split("":"")[1][:-1].split(""-"")[0]
    return ""0.0.0""  # when hl-smi is not available


def get_vllm_version() -> str:
    version = get_version(write_to=""vllm/_version.py"")
    sep = ""+"" if ""+"" not in version else "".""  # dev versions might contain +

    if _no_device():
        if envs.VLLM_TARGET_DEVICE == ""empty"":
            version += f""{sep}empty""
    elif _is_cuda():
        if envs.VLLM_USE_PRECOMPILED:
            version += f""{sep}precompiled""
        else:
            cuda_version = str(get_nvcc_cuda_version())
            if cuda_version != MAIN_CUDA_VERSION:
                cuda_version_str = cuda_version.replace(""."", """")[:3]
                # skip this for source tarball, required for pypi
                if ""sdist"" not in sys.argv:
                    version += f""{sep}cu{cuda_version_str}""
    elif _is_hip():
        # Get the Rocm Version
        rocm_version = get_rocm_version() or torch.version.hip
        if rocm_version and rocm_version != MAIN_CUDA_VERSION:
            version += f""{sep}rocm{rocm_version.replace('.', '')[:3]}""
    elif _is_neuron():
        # Get the Neuron version
        neuron_version = str(get_neuronxcc_version())
        if neuron_version != MAIN_CUDA_VERSION:
            neuron_version_str = neuron_version.replace(""."", """")[:3]
            version += f""{sep}neuron{neuron_version_str}""
    elif _is_hpu():
        # Get the Intel Gaudi Software Suite version
        gaudi_sw_version = str(get_gaudi_sw_version())
        if gaudi_sw_version != MAIN_CUDA_VERSION:
            gaudi_sw_version = gaudi_sw_version.replace(""."", """")[:3]
            version += f""{sep}gaudi{gaudi_sw_version}""
    elif _is_openvino():
        version += f""{sep}openvino""
    elif _is_tpu():
        version += f""{sep}tpu""
    elif _is_cpu():
        if envs.VLLM_TARGET_DEVICE == ""cpu"":
            version += f""{sep}cpu""
    elif _is_xpu():
        version += f""{sep}xpu""
    else:
        raise RuntimeError(""Unknown runtime environment"")

    return version


def get_requirements() -> List[str]:
    """"""Get Python package dependencies from requirements.txt.""""""

    def _read_requirements(filename: str) -> List[str]:
        with open(get_path(filename)) as f:
            requirements = f.read().strip().split(""\n"")
        resolved_requirements = []
        for line in requirements:
            if line.startswith(""-r ""):
                resolved_requirements += _read_requirements(line.split()[1])
            elif line.startswith(""--""):
                continue
            else:
                resolved_requirements.append(line)
        return resolved_requirements

    if _no_device():
        requirements = _read_requirements(""requirements-common.txt"")
    elif _is_cuda():
        requirements = _read_requirements(""requirements-cuda.txt"")
        cuda_major, cuda_minor = torch.version.cuda.split(""."")
        modified_requirements = []
        for req in requirements:
            if (""vllm-flash-attn"" in req
                    and not (cuda_major == ""12"" and cuda_minor == ""1"")):
                # vllm-flash-attn is built only for CUDA 12.1.
                # Skip for other versions.
                continue
            modified_requirements.append(req)
        requirements = modified_requirements
    elif _is_hip():
        requirements = _read_requirements(""requirements-rocm.txt"")
    elif _is_neuron():
        requirements = _read_requirements(""requirements-neuron.txt"")
    elif _is_hpu():
        requirements = _read_requirements(""requirements-hpu.txt"")
    elif _is_openvino():
        requirements = _read_requirements(""requirements-openvino.txt"")
    elif _is_tpu():
        requirements = _read_requirements(""requirements-tpu.txt"")
    elif _is_cpu():
        requirements = _read_requirements(""requirements-cpu.txt"")
    elif _is_xpu():
        requirements = _read_requirements(""requirements-xpu.txt"")
    else:
        raise ValueError(
            ""Unsupported platform, please use CUDA, ROCm, Neuron, HPU, ""
            ""OpenVINO, or CPU."")
    return requirements


ext_modules = []

if _is_cuda() or _is_hip():
    ext_modules.append(CMakeExtension(name=""vllm._moe_C""))

if _is_hip():
    ext_modules.append(CMakeExtension(name=""vllm._rocm_C""))

if _is_cuda():
    ext_modules.append(CMakeExtension(name=""vllm.vllm_flash_attn._vllm_fa2_C""))
    if envs.VLLM_USE_PRECOMPILED or get_nvcc_cuda_version() >= Version(""12.0""):
        # FA3 requires CUDA 12.0 or later
        ext_modules.append(
            CMakeExtension(name=""vllm.vllm_flash_attn._vllm_fa3_C""))
    ext_modules.append(CMakeExtension(name=""vllm.cumem_allocator""))

if _build_custom_ops():
    ext_modules.append(CMakeExtension(name=""vllm._C""))

package_data = {
    ""vllm"": [
        ""py.typed"",
        ""model_executor/layers/fused_moe/configs/*.json"",
        ""model_executor/layers/quantization/utils/configs/*.json"",
    ]
}

if _no_device():
    ext_modules = []

if not ext_modules:
    cmdclass = {}
else:
    cmdclass = {
        ""build_ext"":
        repackage_wheel if envs.VLLM_USE_PRECOMPILED else cmake_build_ext
    }

setup(
    # static metadata should rather go in pyproject.toml
    version=get_vllm_version(),
    ext_modules=ext_modules,
    install_requires=get_requirements(),
    extras_require={
        ""tensorizer"": [""tensorizer>=2.9.0""],
        ""runai"": [""runai-model-streamer"", ""runai-model-streamer-s3"", ""boto3""],
        ""audio"": [""librosa"", ""soundfile""],  # Required for audio processing
        ""video"": [""decord""]  # Required for video processing
    },
    cmdclass=cmdclass,
    package_data=package_data,
)
"
XingangPan_DragGAN,XingangPan_DragGAN_legacy.py,16561,https://raw.githubusercontent.com/XingangPan/DragGAN/main/legacy.py,"# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

""""""Converting legacy network pickle into the new format.""""""

import click
import pickle
import re
import copy
import numpy as np
import torch
import dnnlib
from torch_utils import misc

#----------------------------------------------------------------------------

def load_network_pkl(f, force_fp16=False):
    data = _LegacyUnpickler(f).load()

    # Legacy TensorFlow pickle => convert.
    if isinstance(data, tuple) and len(data) == 3 and all(isinstance(net, _TFNetworkStub) for net in data):
        tf_G, tf_D, tf_Gs = data
        G = convert_tf_generator(tf_G)
        D = convert_tf_discriminator(tf_D)
        G_ema = convert_tf_generator(tf_Gs)
        data = dict(G=G, D=D, G_ema=G_ema)

    # Add missing fields.
    if 'training_set_kwargs' not in data:
        data['training_set_kwargs'] = None
    if 'augment_pipe' not in data:
        data['augment_pipe'] = None

    # Validate contents.
    assert isinstance(data['G'], torch.nn.Module)
    assert isinstance(data['D'], torch.nn.Module)
    assert isinstance(data['G_ema'], torch.nn.Module)
    assert isinstance(data['training_set_kwargs'], (dict, type(None)))
    assert isinstance(data['augment_pipe'], (torch.nn.Module, type(None)))

    # Force FP16.
    if force_fp16:
        for key in ['G', 'D', 'G_ema']:
            old = data[key]
            kwargs = copy.deepcopy(old.init_kwargs)
            fp16_kwargs = kwargs.get('synthesis_kwargs', kwargs)
            fp16_kwargs.num_fp16_res = 4
            fp16_kwargs.conv_clamp = 256
            if kwargs != old.init_kwargs:
                new = type(old)(**kwargs).eval().requires_grad_(False)
                misc.copy_params_and_buffers(old, new, require_all=True)
                data[key] = new
    return data

#----------------------------------------------------------------------------

class _TFNetworkStub(dnnlib.EasyDict):
    pass

class _LegacyUnpickler(pickle.Unpickler):
    def find_class(self, module, name):
        if module == 'dnnlib.tflib.network' and name == 'Network':
            return _TFNetworkStub
        return super().find_class(module, name)

#----------------------------------------------------------------------------

def _collect_tf_params(tf_net):
    # pylint: disable=protected-access
    tf_params = dict()
    def recurse(prefix, tf_net):
        for name, value in tf_net.variables:
            tf_params[prefix + name] = value
        for name, comp in tf_net.components.items():
            recurse(prefix + name + '/', comp)
    recurse('', tf_net)
    return tf_params

#----------------------------------------------------------------------------

def _populate_module_params(module, *patterns):
    for name, tensor in misc.named_params_and_buffers(module):
        found = False
        value = None
        for pattern, value_fn in zip(patterns[0::2], patterns[1::2]):
            match = re.fullmatch(pattern, name)
            if match:
                found = True
                if value_fn is not None:
                    value = value_fn(*match.groups())
                break
        try:
            assert found
            if value is not None:
                tensor.copy_(torch.from_numpy(np.array(value)))
        except:
            print(name, list(tensor.shape))
            raise

#----------------------------------------------------------------------------

def convert_tf_generator(tf_G):
    if tf_G.version < 4:
        raise ValueError('TensorFlow pickle version too low')

    # Collect kwargs.
    tf_kwargs = tf_G.static_kwargs
    known_kwargs = set()
    def kwarg(tf_name, default=None, none=None):
        known_kwargs.add(tf_name)
        val = tf_kwargs.get(tf_name, default)
        return val if val is not None else none

    # Convert kwargs.
    from training import networks_stylegan2
    network_class = networks_stylegan2.Generator
    kwargs = dnnlib.EasyDict(
        z_dim               = kwarg('latent_size',          512),
        c_dim               = kwarg('label_size',           0),
        w_dim               = kwarg('dlatent_size',         512),
        img_resolution      = kwarg('resolution',           1024),
        img_channels        = kwarg('num_channels',         3),
        channel_base        = kwarg('fmap_base',            16384) * 2,
        channel_max         = kwarg('fmap_max',             512),
        num_fp16_res        = kwarg('num_fp16_res',         0),
        conv_clamp          = kwarg('conv_clamp',           None),
        architecture        = kwarg('architecture',         'skip'),
        resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),
        use_noise           = kwarg('use_noise',            True),
        activation          = kwarg('nonlinearity',         'lrelu'),
        mapping_kwargs      = dnnlib.EasyDict(
            num_layers      = kwarg('mapping_layers',       8),
            embed_features  = kwarg('label_fmaps',          None),
            layer_features  = kwarg('mapping_fmaps',        None),
            activation      = kwarg('mapping_nonlinearity', 'lrelu'),
            lr_multiplier   = kwarg('mapping_lrmul',        0.01),
            w_avg_beta      = kwarg('w_avg_beta',           0.995,  none=1),
        ),
    )

    # Check for unknown kwargs.
    kwarg('truncation_psi')
    kwarg('truncation_cutoff')
    kwarg('style_mixing_prob')
    kwarg('structure')
    kwarg('conditioning')
    kwarg('fused_modconv')
    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)
    if len(unknown_kwargs) > 0:
        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])

    # Collect params.
    tf_params = _collect_tf_params(tf_G)
    for name, value in list(tf_params.items()):
        match = re.fullmatch(r'ToRGB_lod(\d+)/(.*)', name)
        if match:
            r = kwargs.img_resolution // (2 ** int(match.group(1)))
            tf_params[f'{r}x{r}/ToRGB/{match.group(2)}'] = value
            kwargs.synthesis.kwargs.architecture = 'orig'
    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')

    # Convert params.
    G = network_class(**kwargs).eval().requires_grad_(False)
    # pylint: disable=unnecessary-lambda
    # pylint: disable=f-string-without-interpolation
    _populate_module_params(G,
        r'mapping\.w_avg',                                  lambda:     tf_params[f'dlatent_avg'],
        r'mapping\.embed\.weight',                          lambda:     tf_params[f'mapping/LabelEmbed/weight'].transpose(),
        r'mapping\.embed\.bias',                            lambda:     tf_params[f'mapping/LabelEmbed/bias'],
        r'mapping\.fc(\d+)\.weight',                        lambda i:   tf_params[f'mapping/Dense{i}/weight'].transpose(),
        r'mapping\.fc(\d+)\.bias',                          lambda i:   tf_params[f'mapping/Dense{i}/bias'],
        r'synthesis\.b4\.const',                            lambda:     tf_params[f'synthesis/4x4/Const/const'][0],
        r'synthesis\.b4\.conv1\.weight',                    lambda:     tf_params[f'synthesis/4x4/Conv/weight'].transpose(3, 2, 0, 1),
        r'synthesis\.b4\.conv1\.bias',                      lambda:     tf_params[f'synthesis/4x4/Conv/bias'],
        r'synthesis\.b4\.conv1\.noise_const',               lambda:     tf_params[f'synthesis/noise0'][0, 0],
        r'synthesis\.b4\.conv1\.noise_strength',            lambda:     tf_params[f'synthesis/4x4/Conv/noise_strength'],
        r'synthesis\.b4\.conv1\.affine\.weight',            lambda:     tf_params[f'synthesis/4x4/Conv/mod_weight'].transpose(),
        r'synthesis\.b4\.conv1\.affine\.bias',              lambda:     tf_params[f'synthesis/4x4/Conv/mod_bias'] + 1,
        r'synthesis\.b(\d+)\.conv0\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/weight'][::-1, ::-1].transpose(3, 2, 0, 1),
        r'synthesis\.b(\d+)\.conv0\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/bias'],
        r'synthesis\.b(\d+)\.conv0\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-5}'][0, 0],
        r'synthesis\.b(\d+)\.conv0\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/noise_strength'],
        r'synthesis\.b(\d+)\.conv0\.affine\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_weight'].transpose(),
        r'synthesis\.b(\d+)\.conv0\.affine\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_bias'] + 1,
        r'synthesis\.b(\d+)\.conv1\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/weight'].transpose(3, 2, 0, 1),
        r'synthesis\.b(\d+)\.conv1\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/bias'],
        r'synthesis\.b(\d+)\.conv1\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-4}'][0, 0],
        r'synthesis\.b(\d+)\.conv1\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/noise_strength'],
        r'synthesis\.b(\d+)\.conv1\.affine\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_weight'].transpose(),
        r'synthesis\.b(\d+)\.conv1\.affine\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_bias'] + 1,
        r'synthesis\.b(\d+)\.torgb\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/weight'].transpose(3, 2, 0, 1),
        r'synthesis\.b(\d+)\.torgb\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/bias'],
        r'synthesis\.b(\d+)\.torgb\.affine\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_weight'].transpose(),
        r'synthesis\.b(\d+)\.torgb\.affine\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_bias'] + 1,
        r'synthesis\.b(\d+)\.skip\.weight',                 lambda r:   tf_params[f'synthesis/{r}x{r}/Skip/weight'][::-1, ::-1].transpose(3, 2, 0, 1),
        r'.*\.resample_filter',                             None,
        r'.*\.act_filter',                                  None,
    )
    return G

#----------------------------------------------------------------------------

def convert_tf_discriminator(tf_D):
    if tf_D.version < 4:
        raise ValueError('TensorFlow pickle version too low')

    # Collect kwargs.
    tf_kwargs = tf_D.static_kwargs
    known_kwargs = set()
    def kwarg(tf_name, default=None):
        known_kwargs.add(tf_name)
        return tf_kwargs.get(tf_name, default)

    # Convert kwargs.
    kwargs = dnnlib.EasyDict(
        c_dim                   = kwarg('label_size',           0),
        img_resolution          = kwarg('resolution',           1024),
        img_channels            = kwarg('num_channels',         3),
        architecture            = kwarg('architecture',         'resnet'),
        channel_base            = kwarg('fmap_base',            16384) * 2,
        channel_max             = kwarg('fmap_max',             512),
        num_fp16_res            = kwarg('num_fp16_res',         0),
        conv_clamp              = kwarg('conv_clamp',           None),
        cmap_dim                = kwarg('mapping_fmaps',        None),
        block_kwargs = dnnlib.EasyDict(
            activation          = kwarg('nonlinearity',         'lrelu'),
            resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),
            freeze_layers       = kwarg('freeze_layers',        0),
        ),
        mapping_kwargs = dnnlib.EasyDict(
            num_layers          = kwarg('mapping_layers',       0),
            embed_features      = kwarg('mapping_fmaps',        None),
            layer_features      = kwarg('mapping_fmaps',        None),
            activation          = kwarg('nonlinearity',         'lrelu'),
            lr_multiplier       = kwarg('mapping_lrmul',        0.1),
        ),
        epilogue_kwargs = dnnlib.EasyDict(
            mbstd_group_size    = kwarg('mbstd_group_size',     None),
            mbstd_num_channels  = kwarg('mbstd_num_features',   1),
            activation          = kwarg('nonlinearity',         'lrelu'),
        ),
    )

    # Check for unknown kwargs.
    kwarg('structure')
    kwarg('conditioning')
    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)
    if len(unknown_kwargs) > 0:
        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])

    # Collect params.
    tf_params = _collect_tf_params(tf_D)
    for name, value in list(tf_params.items()):
        match = re.fullmatch(r'FromRGB_lod(\d+)/(.*)', name)
        if match:
            r = kwargs.img_resolution // (2 ** int(match.group(1)))
            tf_params[f'{r}x{r}/FromRGB/{match.group(2)}'] = value
            kwargs.architecture = 'orig'
    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')

    # Convert params.
    from training import networks_stylegan2
    D = networks_stylegan2.Discriminator(**kwargs).eval().requires_grad_(False)
    # pylint: disable=unnecessary-lambda
    # pylint: disable=f-string-without-interpolation
    _populate_module_params(D,
        r'b(\d+)\.fromrgb\.weight',     lambda r:       tf_params[f'{r}x{r}/FromRGB/weight'].transpose(3, 2, 0, 1),
        r'b(\d+)\.fromrgb\.bias',       lambda r:       tf_params[f'{r}x{r}/FromRGB/bias'],
        r'b(\d+)\.conv(\d+)\.weight',   lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{["""",""_down""][int(i)]}/weight'].transpose(3, 2, 0, 1),
        r'b(\d+)\.conv(\d+)\.bias',     lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{["""",""_down""][int(i)]}/bias'],
        r'b(\d+)\.skip\.weight',        lambda r:       tf_params[f'{r}x{r}/Skip/weight'].transpose(3, 2, 0, 1),
        r'mapping\.embed\.weight',      lambda:         tf_params[f'LabelEmbed/weight'].transpose(),
        r'mapping\.embed\.bias',        lambda:         tf_params[f'LabelEmbed/bias'],
        r'mapping\.fc(\d+)\.weight',    lambda i:       tf_params[f'Mapping{i}/weight'].transpose(),
        r'mapping\.fc(\d+)\.bias',      lambda i:       tf_params[f'Mapping{i}/bias'],
        r'b4\.conv\.weight',            lambda:         tf_params[f'4x4/Conv/weight'].transpose(3, 2, 0, 1),
        r'b4\.conv\.bias',              lambda:         tf_params[f'4x4/Conv/bias'],
        r'b4\.fc\.weight',              lambda:         tf_params[f'4x4/Dense0/weight'].transpose(),
        r'b4\.fc\.bias',                lambda:         tf_params[f'4x4/Dense0/bias'],
        r'b4\.out\.weight',             lambda:         tf_params[f'Output/weight'].transpose(),
        r'b4\.out\.bias',               lambda:         tf_params[f'Output/bias'],
        r'.*\.resample_filter',         None,
    )
    return D

#----------------------------------------------------------------------------

@click.command()
@click.option('--source', help='Input pickle', required=True, metavar='PATH')
@click.option('--dest', help='Output pickle', required=True, metavar='PATH')
@click.option('--force-fp16', help='Force the networks to use FP16', type=bool, default=False, metavar='BOOL', show_default=True)
def convert_network_pickle(source, dest, force_fp16):
    """"""Convert legacy network pickle into the native PyTorch format.

    The tool is able to load the main network configurations exported using the TensorFlow version of StyleGAN2 or StyleGAN2-ADA.
    It does not support e.g. StyleGAN2-ADA comparison methods, StyleGAN2 configs A-D, or StyleGAN1 networks.

    Example:

    \b
    python legacy.py \\
        --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \\
        --dest=stylegan2-cat-config-f.pkl
    """"""
    print(f'Loading ""{source}""...')
    with dnnlib.util.open_url(source) as f:
        data = load_network_pkl(f, force_fp16=force_fp16)
    print(f'Saving ""{dest}""...')
    with open(dest, 'wb') as f:
        pickle.dump(data, f)
    print('Done.')

#----------------------------------------------------------------------------

if __name__ == ""__main__"":
    convert_network_pickle() # pylint: disable=no-value-for-parameter

#----------------------------------------------------------------------------
"
zhayujie_chatgpt-on-wechat,zhayujie_chatgpt-on-wechat_config.py,16653,https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/config.py,"# encoding:utf-8

import json
import logging
import os
import pickle
import copy

from common.log import logger

# 将所有可用的配置项写在字典里, 请使用小写字母
# 此处的配置值无实际意义，程序不会读取此处的配置，仅用于提示格式，请将配置加入到config.json中
available_setting = {
    # openai api配置
    ""open_ai_api_key"": """",  # openai api key
    # openai apibase，当use_azure_chatgpt为true时，需要设置对应的api base
    ""open_ai_api_base"": ""https://api.openai.com/v1"",
    ""proxy"": """",  # openai使用的代理
    # chatgpt模型， 当use_azure_chatgpt为true时，其名称为Azure上model deployment名称
    ""model"": ""gpt-3.5-turbo"",  # 可选择: gpt-4o, pt-4o-mini, gpt-4-turbo, claude-3-sonnet, wenxin, moonshot, qwen-turbo, xunfei, glm-4, minimax, gemini等模型，全部可选模型详见common/const.py文件
    ""bot_type"": """",  # 可选配置，使用兼容openai格式的三方服务时候，需填""chatGPT""。bot具体名称详见common/const.py文件列出的bot_type，如不填根据model名称判断，
    ""use_azure_chatgpt"": False,  # 是否使用azure的chatgpt
    ""azure_deployment_id"": """",  # azure 模型部署名称
    ""azure_api_version"": """",  # azure api版本
    # Bot触发配置
    ""single_chat_prefix"": [""bot"", ""@bot""],  # 私聊时文本需要包含该前缀才能触发机器人回复
    ""single_chat_reply_prefix"": ""[bot] "",  # 私聊时自动回复的前缀，用于区分真人
    ""single_chat_reply_suffix"": """",  # 私聊时自动回复的后缀，\n 可以换行
    ""group_chat_prefix"": [""@bot""],  # 群聊时包含该前缀则会触发机器人回复
    ""no_need_at"": False,  # 群聊回复时是否不需要艾特
    ""group_chat_reply_prefix"": """",  # 群聊时自动回复的前缀
    ""group_chat_reply_suffix"": """",  # 群聊时自动回复的后缀，\n 可以换行
    ""group_chat_keyword"": [],  # 群聊时包含该关键词则会触发机器人回复
    ""group_at_off"": False,  # 是否关闭群聊时@bot的触发
    ""group_name_white_list"": [""ChatGPT测试群"", ""ChatGPT测试群2""],  # 开启自动回复的群名称列表
    ""group_name_keyword_white_list"": [],  # 开启自动回复的群名称关键词列表
    ""group_chat_in_one_session"": [""ChatGPT测试群""],  # 支持会话上下文共享的群名称
    ""nick_name_black_list"": [],  # 用户昵称黑名单
    ""group_welcome_msg"": """",  # 配置新人进群固定欢迎语，不配置则使用随机风格欢迎
    ""trigger_by_self"": False,  # 是否允许机器人触发
    ""text_to_image"": ""dall-e-2"",  # 图片生成模型，可选 dall-e-2, dall-e-3
    # Azure OpenAI dall-e-3 配置
    ""dalle3_image_style"": ""vivid"", # 图片生成dalle3的风格，可选有 vivid, natural
    ""dalle3_image_quality"": ""hd"", # 图片生成dalle3的质量，可选有 standard, hd
    # Azure OpenAI DALL-E API 配置, 当use_azure_chatgpt为true时,用于将文字回复的资源和Dall-E的资源分开.
    ""azure_openai_dalle_api_base"": """", # [可选] azure openai 用于回复图片的资源 endpoint，默认使用 open_ai_api_base
    ""azure_openai_dalle_api_key"": """", # [可选] azure openai 用于回复图片的资源 key，默认使用 open_ai_api_key
    ""azure_openai_dalle_deployment_id"":"""", # [可选] azure openai 用于回复图片的资源 deployment id，默认使用 text_to_image
    ""image_proxy"": True,  # 是否需要图片代理，国内访问LinkAI时需要
    ""image_create_prefix"": [""画"", ""看"", ""找""],  # 开启图片回复的前缀
    ""concurrency_in_session"": 1,  # 同一会话最多有多少条消息在处理中，大于1可能乱序
    ""image_create_size"": ""256x256"",  # 图片大小,可选有 256x256, 512x512, 1024x1024 (dall-e-3默认为1024x1024)
    ""group_chat_exit_group"": False,
    # chatgpt会话参数
    ""expires_in_seconds"": 3600,  # 无操作会话的过期时间
    # 人格描述
    ""character_desc"": ""你是ChatGPT, 一个由OpenAI训练的大型语言模型, 你旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。"",
    ""conversation_max_tokens"": 1000,  # 支持上下文记忆的最多字符数
    # chatgpt限流配置
    ""rate_limit_chatgpt"": 20,  # chatgpt的调用频率限制
    ""rate_limit_dalle"": 50,  # openai dalle的调用频率限制
    # chatgpt api参数 参考https://platform.openai.com/docs/api-reference/chat/create
    ""temperature"": 0.9,
    ""top_p"": 1,
    ""frequency_penalty"": 0,
    ""presence_penalty"": 0,
    ""request_timeout"": 180,  # chatgpt请求超时时间，openai接口默认设置为600，对于难问题一般需要较长时间
    ""timeout"": 120,  # chatgpt重试超时时间，在这个时间内，将会自动重试
    # Baidu 文心一言参数
    ""baidu_wenxin_model"": ""eb-instant"",  # 默认使用ERNIE-Bot-turbo模型
    ""baidu_wenxin_api_key"": """",  # Baidu api key
    ""baidu_wenxin_secret_key"": """",  # Baidu secret key
    ""baidu_wenxin_prompt_enabled"": False,  # Enable prompt if you are using ernie character model
    # 讯飞星火API
    ""xunfei_app_id"": """",  # 讯飞应用ID
    ""xunfei_api_key"": """",  # 讯飞 API key
    ""xunfei_api_secret"": """",  # 讯飞 API secret
    ""xunfei_domain"": """",  # 讯飞模型对应的domain参数，Spark4.0 Ultra为 4.0Ultra，其他模型详见: https://www.xfyun.cn/doc/spark/Web.html
    ""xunfei_spark_url"": """",  # 讯飞模型对应的请求地址，Spark4.0 Ultra为 wss://spark-api.xf-yun.com/v4.0/chat，其他模型参考详见: https://www.xfyun.cn/doc/spark/Web.html
    # claude 配置
    ""claude_api_cookie"": """",
    ""claude_uuid"": """",
    # claude api key
    ""claude_api_key"": """",
    # 通义千问API, 获取方式查看文档 https://help.aliyun.com/document_detail/2587494.html
    ""qwen_access_key_id"": """",
    ""qwen_access_key_secret"": """",
    ""qwen_agent_key"": """",
    ""qwen_app_id"": """",
    ""qwen_node_id"": """",  # 流程编排模型用到的id，如果没有用到qwen_node_id，请务必保持为空字符串
    # 阿里灵积(通义新版sdk)模型api key
    ""dashscope_api_key"": """",
    # Google Gemini Api Key
    ""gemini_api_key"": """",
    # wework的通用配置
    ""wework_smart"": True,  # 配置wework是否使用已登录的企业微信，False为多开
    # 语音设置
    ""speech_recognition"": True,  # 是否开启语音识别
    ""group_speech_recognition"": False,  # 是否开启群组语音识别
    ""voice_reply_voice"": False,  # 是否使用语音回复语音，需要设置对应语音合成引擎的api key
    ""always_reply_voice"": False,  # 是否一直使用语音回复
    ""voice_to_text"": ""openai"",  # 语音识别引擎，支持openai,baidu,google,azure,xunfei,ali
    ""text_to_voice"": ""openai"",  # 语音合成引擎，支持openai,baidu,google,azure,xunfei,ali,pytts(offline),elevenlabs,edge(online)
    ""text_to_voice_model"": ""tts-1"",
    ""tts_voice_id"": ""alloy"",
    # baidu 语音api配置， 使用百度语音识别和语音合成时需要
    ""baidu_app_id"": """",
    ""baidu_api_key"": """",
    ""baidu_secret_key"": """",
    # 1536普通话(支持简单的英文识别) 1737英语 1637粤语 1837四川话 1936普通话远场
    ""baidu_dev_pid"": 1536,
    # azure 语音api配置， 使用azure语音识别和语音合成时需要
    ""azure_voice_api_key"": """",
    ""azure_voice_region"": ""japaneast"",
    # elevenlabs 语音api配置
    ""xi_api_key"": """",  # 获取ap的方法可以参考https://docs.elevenlabs.io/api-reference/quick-start/authentication
    ""xi_voice_id"": """",  # ElevenLabs提供了9种英式、美式等英语发音id，分别是“Adam/Antoni/Arnold/Bella/Domi/Elli/Josh/Rachel/Sam”
    # 服务时间限制，目前支持itchat
    ""chat_time_module"": False,  # 是否开启服务时间限制
    ""chat_start_time"": ""00:00"",  # 服务开始时间
    ""chat_stop_time"": ""24:00"",  # 服务结束时间
    # 翻译api
    ""translate"": ""baidu"",  # 翻译api，支持baidu
    # baidu翻译api的配置
    ""baidu_translate_app_id"": """",  # 百度翻译api的appid
    ""baidu_translate_app_key"": """",  # 百度翻译api的秘钥
    # itchat的配置
    ""hot_reload"": False,  # 是否开启热重载
    # wechaty的配置
    ""wechaty_puppet_service_token"": """",  # wechaty的token
    # wechatmp的配置
    ""wechatmp_token"": """",  # 微信公众平台的Token
    ""wechatmp_port"": 8080,  # 微信公众平台的端口,需要端口转发到80或443
    ""wechatmp_app_id"": """",  # 微信公众平台的appID
    ""wechatmp_app_secret"": """",  # 微信公众平台的appsecret
    ""wechatmp_aes_key"": """",  # 微信公众平台的EncodingAESKey，加密模式需要
    # wechatcom的通用配置
    ""wechatcom_corp_id"": """",  # 企业微信公司的corpID
    # wechatcomapp的配置
    ""wechatcomapp_token"": """",  # 企业微信app的token
    ""wechatcomapp_port"": 9898,  # 企业微信app的服务端口,不需要端口转发
    ""wechatcomapp_secret"": """",  # 企业微信app的secret
    ""wechatcomapp_agent_id"": """",  # 企业微信app的agent_id
    ""wechatcomapp_aes_key"": """",  # 企业微信app的aes_key
    # 飞书配置
    ""feishu_port"": 80,  # 飞书bot监听端口
    ""feishu_app_id"": """",  # 飞书机器人应用APP Id
    ""feishu_app_secret"": """",  # 飞书机器人APP secret
    ""feishu_token"": """",  # 飞书 verification token
    ""feishu_bot_name"": """",  # 飞书机器人的名字
    # 钉钉配置
    ""dingtalk_client_id"": """",  # 钉钉机器人Client ID 
    ""dingtalk_client_secret"": """",  # 钉钉机器人Client Secret
    ""dingtalk_card_enabled"": False,
    
    # chatgpt指令自定义触发词
    ""clear_memory_commands"": [""#清除记忆""],  # 重置会话指令，必须以#开头
    # channel配置
    ""channel_type"": """",  # 通道类型，支持：{wx,wxy,terminal,wechatmp,wechatmp_service,wechatcom_app,dingtalk}
    ""subscribe_msg"": """",  # 订阅消息, 支持: wechatmp, wechatmp_service, wechatcom_app
    ""debug"": False,  # 是否开启debug模式，开启后会打印更多日志
    ""appdata_dir"": """",  # 数据目录
    # 插件配置
    ""plugin_trigger_prefix"": ""$"",  # 规范插件提供聊天相关指令的前缀，建议不要和管理员指令前缀""#""冲突
    # 是否使用全局插件配置
    ""use_global_plugin_config"": False,
    ""max_media_send_count"": 3,  # 单次最大发送媒体资源的个数
    ""media_send_interval"": 1,  # 发送图片的事件间隔，单位秒
    # 智谱AI 平台配置
    ""zhipu_ai_api_key"": """",
    ""zhipu_ai_api_base"": ""https://open.bigmodel.cn/api/paas/v4"",
    ""moonshot_api_key"": """",
    ""moonshot_base_url"": ""https://api.moonshot.cn/v1/chat/completions"",
    # LinkAI平台配置
    ""use_linkai"": False,
    ""linkai_api_key"": """",
    ""linkai_app_code"": """",
    ""linkai_api_base"": ""https://api.link-ai.tech"",  # linkAI服务地址
    ""Minimax_api_key"": """",
    ""Minimax_group_id"": """",
    ""Minimax_base_url"": """",
    ""web_port"": 9899,
}


class Config(dict):
    def __init__(self, d=None):
        super().__init__()
        if d is None:
            d = {}
        for k, v in d.items():
            self[k] = v
        # user_datas: 用户数据，key为用户名，value为用户数据，也是dict
        self.user_datas = {}

    def __getitem__(self, key):
        if key not in available_setting:
            raise Exception(""key {} not in available_setting"".format(key))
        return super().__getitem__(key)

    def __setitem__(self, key, value):
        if key not in available_setting:
            raise Exception(""key {} not in available_setting"".format(key))
        return super().__setitem__(key, value)

    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError as e:
            return default
        except Exception as e:
            raise e

    # Make sure to return a dictionary to ensure atomic
    def get_user_data(self, user) -> dict:
        if self.user_datas.get(user) is None:
            self.user_datas[user] = {}
        return self.user_datas[user]

    def load_user_datas(self):
        try:
            with open(os.path.join(get_appdata_dir(), ""user_datas.pkl""), ""rb"") as f:
                self.user_datas = pickle.load(f)
                logger.info(""[Config] User datas loaded."")
        except FileNotFoundError as e:
            logger.info(""[Config] User datas file not found, ignore."")
        except Exception as e:
            logger.info(""[Config] User datas error: {}"".format(e))
            self.user_datas = {}

    def save_user_datas(self):
        try:
            with open(os.path.join(get_appdata_dir(), ""user_datas.pkl""), ""wb"") as f:
                pickle.dump(self.user_datas, f)
                logger.info(""[Config] User datas saved."")
        except Exception as e:
            logger.info(""[Config] User datas error: {}"".format(e))


config = Config()


def drag_sensitive(config):
    try:
        if isinstance(config, str):
            conf_dict: dict = json.loads(config)
            conf_dict_copy = copy.deepcopy(conf_dict)
            for key in conf_dict_copy:
                if ""key"" in key or ""secret"" in key:
                    if isinstance(conf_dict_copy[key], str):
                        conf_dict_copy[key] = conf_dict_copy[key][0:3] + ""*"" * 5 + conf_dict_copy[key][-3:]
            return json.dumps(conf_dict_copy, indent=4)

        elif isinstance(config, dict):
            config_copy = copy.deepcopy(config)
            for key in config:
                if ""key"" in key or ""secret"" in key:
                    if isinstance(config_copy[key], str):
                        config_copy[key] = config_copy[key][0:3] + ""*"" * 5 + config_copy[key][-3:]
            return config_copy
    except Exception as e:
        logger.exception(e)
        return config
    return config


def load_config():
    global config
    config_path = ""./config.json""
    if not os.path.exists(config_path):
        logger.info(""配置文件不存在，将使用config-template.json模板"")
        config_path = ""./config-template.json""

    config_str = read_file(config_path)
    logger.debug(""[INIT] config str: {}"".format(drag_sensitive(config_str)))

    # 将json字符串反序列化为dict类型
    config = Config(json.loads(config_str))

    # override config with environment variables.
    # Some online deployment platforms (e.g. Railway) deploy project from github directly. So you shouldn't put your secrets like api key in a config file, instead use environment variables to override the default config.
    for name, value in os.environ.items():
        name = name.lower()
        if name in available_setting:
            logger.info(""[INIT] override config by environ args: {}={}"".format(name, value))
            try:
                config[name] = eval(value)
            except:
                if value == ""false"":
                    config[name] = False
                elif value == ""true"":
                    config[name] = True
                else:
                    config[name] = value

    if config.get(""debug"", False):
        logger.setLevel(logging.DEBUG)
        logger.debug(""[INIT] set log level to DEBUG"")

    logger.info(""[INIT] load config: {}"".format(drag_sensitive(config)))

    config.load_user_datas()


def get_root():
    return os.path.dirname(os.path.abspath(__file__))


def read_file(path):
    with open(path, mode=""r"", encoding=""utf-8"") as f:
        return f.read()


def conf():
    return config


def get_appdata_dir():
    data_path = os.path.join(get_root(), conf().get(""appdata_dir"", """"))
    if not os.path.exists(data_path):
        logger.info(""[INIT] data path not exists, create it: {}"".format(data_path))
        os.makedirs(data_path)
    return data_path


def subscribe_msg():
    trigger_prefix = conf().get(""single_chat_prefix"", [""""])[0]
    msg = conf().get(""subscribe_msg"", """")
    return msg.format(trigger_prefix=trigger_prefix)


# global plugin config
plugin_config = {}


def write_plugin_config(pconf: dict):
    """"""
    写入插件全局配置
    :param pconf: 全量插件配置
    """"""
    global plugin_config
    for k in pconf:
        plugin_config[k.lower()] = pconf[k]

def remove_plugin_config(name: str):
    """"""
    移除待重新加载的插件全局配置
    :param name: 待重载的插件名
    """"""
    global plugin_config
    plugin_config.pop(name.lower(), None)


def pconf(plugin_name: str) -> dict:
    """"""
    根据插件名称获取配置
    :param plugin_name: 插件名称
    :return: 该插件的配置项
    """"""
    return plugin_config.get(plugin_name.lower())


# 全局配置，用于存放全局生效的状态
global_config = {""admin_users"": []}
"
sqlmapproject_sqlmap,sqlmapproject_sqlmap_sqlmap.py,25734,https://raw.githubusercontent.com/sqlmapproject/sqlmap/master/sqlmap.py,"#!/usr/bin/env python

""""""
Copyright (c) 2006-2025 sqlmap developers (https://sqlmap.org/)
See the file 'LICENSE' for copying permission
""""""

from __future__ import print_function

try:
    import sys

    sys.dont_write_bytecode = True

    try:
        __import__(""lib.utils.versioncheck"")  # this has to be the first non-standard import
    except ImportError:
        sys.exit(""[!] wrong installation detected (missing modules). Visit 'https://github.com/sqlmapproject/sqlmap/#installation' for further details"")

    import bdb
    import glob
    import inspect
    import json
    import logging
    import os
    import re
    import shutil
    import sys
    import tempfile
    import threading
    import time
    import traceback
    import warnings

    if ""--deprecations"" not in sys.argv:
        warnings.filterwarnings(action=""ignore"", category=DeprecationWarning)
    else:
        warnings.resetwarnings()
        warnings.filterwarnings(action=""ignore"", message=""'crypt'"", category=DeprecationWarning)
        warnings.simplefilter(""ignore"", category=ImportWarning)
        if sys.version_info >= (3, 0):
            warnings.simplefilter(""ignore"", category=ResourceWarning)

    warnings.filterwarnings(action=""ignore"", message=""Python 2 is no longer supported"")
    warnings.filterwarnings(action=""ignore"", message="".*was already imported"", category=UserWarning)
    warnings.filterwarnings(action=""ignore"", message="".*using a very old release"", category=UserWarning)
    warnings.filterwarnings(action=""ignore"", message="".*default buffer size will be used"", category=RuntimeWarning)
    warnings.filterwarnings(action=""ignore"", category=UserWarning, module=""psycopg2"")

    from lib.core.data import logger

    from lib.core.common import banner
    from lib.core.common import checkPipedInput
    from lib.core.common import checkSums
    from lib.core.common import createGithubIssue
    from lib.core.common import dataToStdout
    from lib.core.common import extractRegexResult
    from lib.core.common import filterNone
    from lib.core.common import getDaysFromLastUpdate
    from lib.core.common import getFileItems
    from lib.core.common import getSafeExString
    from lib.core.common import maskSensitiveData
    from lib.core.common import openFile
    from lib.core.common import setPaths
    from lib.core.common import weAreFrozen
    from lib.core.convert import getUnicode
    from lib.core.common import setColor
    from lib.core.common import unhandledExceptionMessage
    from lib.core.compat import LooseVersion
    from lib.core.compat import xrange
    from lib.core.data import cmdLineOptions
    from lib.core.data import conf
    from lib.core.data import kb
    from lib.core.datatype import OrderedSet
    from lib.core.enums import MKSTEMP_PREFIX
    from lib.core.exception import SqlmapBaseException
    from lib.core.exception import SqlmapShellQuitException
    from lib.core.exception import SqlmapSilentQuitException
    from lib.core.exception import SqlmapUserQuitException
    from lib.core.option import init
    from lib.core.option import initOptions
    from lib.core.patch import dirtyPatches
    from lib.core.patch import resolveCrossReferences
    from lib.core.settings import GIT_PAGE
    from lib.core.settings import IS_WIN
    from lib.core.settings import LAST_UPDATE_NAGGING_DAYS
    from lib.core.settings import LEGAL_DISCLAIMER
    from lib.core.settings import THREAD_FINALIZATION_TIMEOUT
    from lib.core.settings import UNICODE_ENCODING
    from lib.core.settings import VERSION
    from lib.parse.cmdline import cmdLineParser
    from lib.utils.crawler import crawl
except KeyboardInterrupt:
    errMsg = ""user aborted""

    if ""logger"" in globals():
        logger.critical(errMsg)
        raise SystemExit
    else:
        import time
        sys.exit(""\r[%s] [CRITICAL] %s"" % (time.strftime(""%X""), errMsg))

def modulePath():
    """"""
    This will get us the program's directory, even if we are frozen
    using py2exe
    """"""

    try:
        _ = sys.executable if weAreFrozen() else __file__
    except NameError:
        _ = inspect.getsourcefile(modulePath)

    return getUnicode(os.path.dirname(os.path.realpath(_)), encoding=sys.getfilesystemencoding() or UNICODE_ENCODING)

def checkEnvironment():
    try:
        os.path.isdir(modulePath())
    except UnicodeEncodeError:
        errMsg = ""your system does not properly handle non-ASCII paths. ""
        errMsg += ""Please move the sqlmap's directory to the other location""
        logger.critical(errMsg)
        raise SystemExit

    if LooseVersion(VERSION) < LooseVersion(""1.0""):
        errMsg = ""your runtime environment (e.g. PYTHONPATH) is ""
        errMsg += ""broken. Please make sure that you are not running ""
        errMsg += ""newer versions of sqlmap with runtime scripts for older ""
        errMsg += ""versions""
        logger.critical(errMsg)
        raise SystemExit

    # Patch for pip (import) environment
    if ""sqlmap.sqlmap"" in sys.modules:
        for _ in (""cmdLineOptions"", ""conf"", ""kb""):
            globals()[_] = getattr(sys.modules[""lib.core.data""], _)

        for _ in (""SqlmapBaseException"", ""SqlmapShellQuitException"", ""SqlmapSilentQuitException"", ""SqlmapUserQuitException""):
            globals()[_] = getattr(sys.modules[""lib.core.exception""], _)

def main():
    """"""
    Main function of sqlmap when running from command line.
    """"""

    try:
        dirtyPatches()
        resolveCrossReferences()
        checkEnvironment()
        setPaths(modulePath())
        banner()

        # Store original command line options for possible later restoration
        args = cmdLineParser()
        cmdLineOptions.update(args.__dict__ if hasattr(args, ""__dict__"") else args)
        initOptions(cmdLineOptions)

        if checkPipedInput():
            conf.batch = True

        if conf.get(""api""):
            # heavy imports
            from lib.utils.api import StdDbOut
            from lib.utils.api import setRestAPILog

            # Overwrite system standard output and standard error to write
            # to an IPC database
            sys.stdout = StdDbOut(conf.taskid, messagetype=""stdout"")
            sys.stderr = StdDbOut(conf.taskid, messagetype=""stderr"")

            setRestAPILog()

        conf.showTime = True
        dataToStdout(""[!] legal disclaimer: %s\n\n"" % LEGAL_DISCLAIMER, forceOutput=True)
        dataToStdout(""[*] starting @ %s\n\n"" % time.strftime(""%X /%Y-%m-%d/""), forceOutput=True)

        init()

        if not conf.updateAll:
            # Postponed imports (faster start)
            if conf.smokeTest:
                from lib.core.testing import smokeTest
                os._exitcode = 1 - (smokeTest() or 0)
            elif conf.vulnTest:
                from lib.core.testing import vulnTest
                os._exitcode = 1 - (vulnTest() or 0)
            else:
                from lib.controller.controller import start
                if conf.profile:
                    from lib.core.profiling import profile
                    globals()[""start""] = start
                    profile()
                else:
                    try:
                        if conf.crawlDepth and conf.bulkFile:
                            targets = getFileItems(conf.bulkFile)

                            for i in xrange(len(targets)):
                                target = None

                                try:
                                    kb.targets = OrderedSet()
                                    target = targets[i]

                                    if not re.search(r""(?i)\Ahttp[s]*://"", target):
                                        target = ""http://%s"" % target

                                    infoMsg = ""starting crawler for target URL '%s' (%d/%d)"" % (target, i + 1, len(targets))
                                    logger.info(infoMsg)

                                    crawl(target)
                                except Exception as ex:
                                    if target and not isinstance(ex, SqlmapUserQuitException):
                                        errMsg = ""problem occurred while crawling '%s' ('%s')"" % (target, getSafeExString(ex))
                                        logger.error(errMsg)
                                    else:
                                        raise
                                else:
                                    if kb.targets:
                                        start()
                        else:
                            start()
                    except Exception as ex:
                        os._exitcode = 1

                        if ""can't start new thread"" in getSafeExString(ex):
                            errMsg = ""unable to start new threads. Please check OS (u)limits""
                            logger.critical(errMsg)
                            raise SystemExit
                        else:
                            raise

    except SqlmapUserQuitException:
        if not conf.batch:
            errMsg = ""user quit""
            logger.error(errMsg)

    except (SqlmapSilentQuitException, bdb.BdbQuit):
        pass

    except SqlmapShellQuitException:
        cmdLineOptions.sqlmapShell = False

    except SqlmapBaseException as ex:
        errMsg = getSafeExString(ex)
        logger.critical(errMsg)

        os._exitcode = 1

        raise SystemExit

    except KeyboardInterrupt:
        try:
            print()
        except IOError:
            pass

    except EOFError:
        print()

        errMsg = ""exit""
        logger.error(errMsg)

    except SystemExit as ex:
        os._exitcode = ex.code or 0

    except:
        print()
        errMsg = unhandledExceptionMessage()
        excMsg = traceback.format_exc()
        valid = checkSums()

        os._exitcode = 255

        if any(_ in excMsg for _ in (""MemoryError"", ""Cannot allocate memory"")):
            errMsg = ""memory exhaustion detected""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""No space left"", ""Disk quota exceeded"", ""Disk full while accessing"")):
            errMsg = ""no space left on output device""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""The paging file is too small"",)):
            errMsg = ""no space left for paging file""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""Access is denied"", ""subprocess"", ""metasploit"")):
            errMsg = ""permission error occurred while running Metasploit""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""Permission denied"", ""metasploit"")):
            errMsg = ""permission error occurred while using Metasploit""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Read-only file system"" in excMsg:
            errMsg = ""output device is mounted as read-only""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Insufficient system resources"" in excMsg:
            errMsg = ""resource exhaustion detected""
            logger.critical(errMsg)
            raise SystemExit

        elif ""OperationalError: disk I/O error"" in excMsg:
            errMsg = ""I/O error on output device""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Violation of BIDI"" in excMsg:
            errMsg = ""invalid URL (violation of Bidi IDNA rule - RFC 5893)""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Invalid IPv6 URL"" in excMsg:
            errMsg = ""invalid URL ('%s')"" % excMsg.strip().split('\n')[-1]
            logger.critical(errMsg)
            raise SystemExit

        elif ""_mkstemp_inner"" in excMsg:
            errMsg = ""there has been a problem while accessing temporary files""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""tempfile.mkdtemp"", ""tempfile.mkstemp"", ""tempfile.py"")):
            errMsg = ""unable to write to the temporary directory '%s'. "" % tempfile.gettempdir()
            errMsg += ""Please make sure that your disk is not full and ""
            errMsg += ""that you have sufficient write permissions to ""
            errMsg += ""create temporary files and/or directories""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Permission denied: '"" in excMsg:
            match = re.search(r""Permission denied: '([^']*)"", excMsg)
            errMsg = ""permission error occurred while accessing file '%s'"" % match.group(1)
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""twophase"", ""sqlalchemy"")):
            errMsg = ""please update the 'sqlalchemy' package (>= 1.1.11) ""
            errMsg += ""(Reference: 'https://qiita.com/tkprof/items/7d7b2d00df9c5f16fffe')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""invalid maximum character passed to PyUnicode_New"" in excMsg and re.search(r""\A3\.[34]"", sys.version) is not None:
            errMsg = ""please upgrade the Python version (>= 3.5) ""
            errMsg += ""(Reference: 'https://bugs.python.org/issue18183')""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""scramble_caching_sha2"", ""TypeError"")):
            errMsg = ""please downgrade the 'PyMySQL' package (=< 0.8.1) ""
            errMsg += ""(Reference: 'https://github.com/PyMySQL/PyMySQL/issues/700')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""must be pinned buffer, not bytearray"" in excMsg:
            errMsg = ""error occurred at Python interpreter which ""
            errMsg += ""is fixed in 2.7. Please update accordingly ""
            errMsg += ""(Reference: 'https://bugs.python.org/issue8104')""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""OSError: [Errno 22] Invalid argument: '"", ""importlib"")):
            errMsg = ""unable to read file '%s'"" % extractRegexResult(r""OSError: \[Errno 22\] Invalid argument: '(?P<result>[^']+)"", excMsg)
            logger.critical(errMsg)
            raise SystemExit

        elif ""hash_randomization"" in excMsg:
            errMsg = ""error occurred at Python interpreter which ""
            errMsg += ""is fixed in 2.7.3. Please update accordingly ""
            errMsg += ""(Reference: 'https://docs.python.org/2/library/sys.html')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""AttributeError: unable to access item"" in excMsg and re.search(r""3\.11\.\d+a"", sys.version):
            errMsg = ""there is a known issue when sqlmap is run with ALPHA versions of Python 3.11. ""
            errMsg += ""Please downgrade to some stable Python version""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""Resource temporarily unavailable"", ""os.fork()"", ""dictionaryAttack"")):
            errMsg = ""there has been a problem while running the multiprocessing hash cracking. ""
            errMsg += ""Please rerun with option '--threads=1'""
            logger.critical(errMsg)
            raise SystemExit

        elif ""can't start new thread"" in excMsg:
            errMsg = ""there has been a problem while creating new thread instance. ""
            errMsg += ""Please make sure that you are not running too many processes""
            if not IS_WIN:
                errMsg += "" (or increase the 'ulimit -u' value)""
            logger.critical(errMsg)
            raise SystemExit

        elif ""can't allocate read lock"" in excMsg:
            errMsg = ""there has been a problem in regular socket operation ""
            errMsg += ""('%s')"" % excMsg.strip().split('\n')[-1]
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""pymysql"", ""configparser"")):
            errMsg = ""wrong initialization of 'pymsql' detected (using Python3 dependencies)""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""ntlm"", ""socket.error, err"", ""SyntaxError"")):
            errMsg = ""wrong initialization of 'python-ntlm' detected (using Python2 syntax)""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""drda"", ""to_bytes"")):
            errMsg = ""wrong initialization of 'drda' detected (using Python3 syntax)""
            logger.critical(errMsg)
            raise SystemExit

        elif ""'WebSocket' object has no attribute 'status'"" in excMsg:
            errMsg = ""wrong websocket library detected""
            errMsg += "" (Reference: 'https://github.com/sqlmapproject/sqlmap/issues/4572#issuecomment-775041086')""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""window = tkinter.Tk()"",)):
            errMsg = ""there has been a problem in initialization of GUI interface ""
            errMsg += ""('%s')"" % excMsg.strip().split('\n')[-1]
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""unable to access item 'liveTest'"",)):
            errMsg = ""detected usage of files from different versions of sqlmap""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in errMsg for _ in ("": 9.9.9#"",)):
            errMsg = ""LOL xD""
            logger.critical(errMsg)
            raise SystemExit

        elif kb.get(""dumpKeyboardInterrupt""):
            raise SystemExit

        elif any(_ in excMsg for _ in (""Broken pipe"",)):
            raise SystemExit

        elif valid is False:
            errMsg = ""code checksum failed (turning off automatic issue creation). ""
            errMsg += ""You should retrieve the latest development version from official GitHub ""
            errMsg += ""repository at '%s'"" % GIT_PAGE
            logger.critical(errMsg)
            print()
            dataToStdout(excMsg)
            raise SystemExit

        elif any(_ in ""%s\n%s"" % (errMsg, excMsg) for _ in (""tamper/"", ""waf/"", ""--engagement-dojo"")):
            logger.critical(errMsg)
            print()
            dataToStdout(excMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""ImportError"", ""ModuleNotFoundError"", ""<frozen"", ""Can't find file for module"", ""SAXReaderNotAvailable"", ""<built-in function compile> returned NULL without setting an exception"", ""source code string cannot contain null bytes"", ""No module named"", ""tp_name field"", ""module 'sqlite3' has no attribute 'OperationalError'"")):
            errMsg = ""invalid runtime environment ('%s')"" % excMsg.split(""Error: "")[-1].strip()
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""SyntaxError: Non-ASCII character"", "".py on line"", ""but no encoding declared"")):
            errMsg = ""invalid runtime environment ('%s')"" % excMsg.split(""Error: "")[-1].strip()
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""FileNotFoundError: [Errno 2] No such file or directory"", ""cwd = os.getcwd()"")):
            errMsg = ""invalid runtime environment ('%s')"" % excMsg.split(""Error: "")[-1].strip()
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""PermissionError: [WinError 5]"", ""multiprocessing"")):
            errMsg = ""there is a permission problem in running multiprocessing on this system. ""
            errMsg += ""Please rerun with '--disable-multi'""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""No such file"", ""_'"")):
            errMsg = ""corrupted installation detected ('%s'). "" % excMsg.strip().split('\n')[-1]
            errMsg += ""You should retrieve the latest development version from official GitHub ""
            errMsg += ""repository at '%s'"" % GIT_PAGE
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""No such file"", ""sqlmap.conf"", ""Test"")):
            errMsg = ""you are trying to run (hidden) development tests inside the production environment""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""HTTPNtlmAuthHandler"", ""'str' object has no attribute 'decode'"")):
            errMsg = ""package 'python-ntlm' has a known compatibility issue with the ""
            errMsg += ""Python 3 (Reference: 'https://github.com/mullender/python-ntlm/pull/61')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""'DictObject' object has no attribute '"" in excMsg and all(_ in errMsg for _ in (""(fingerprinted)"", ""(identified)"")):
            errMsg = ""there has been a problem in enumeration. ""
            errMsg += ""Because of a considerable chance of false-positive case ""
            errMsg += ""you are advised to rerun with switch '--flush-session'""
            logger.critical(errMsg)
            raise SystemExit

        elif ""database disk image is malformed"" in excMsg:
            errMsg = ""local session file seems to be malformed. Please rerun with '--flush-session'""
            logger.critical(errMsg)
            raise SystemExit

        elif ""AttributeError: 'module' object has no attribute 'F_GETFD'"" in excMsg:
            errMsg = ""invalid runtime (\""%s\"") "" % excMsg.split(""Error: "")[-1].strip()
            errMsg += ""(Reference: 'https://stackoverflow.com/a/38841364' & 'https://bugs.python.org/issue24944#msg249231')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""bad marshal data (unknown type code)"" in excMsg:
            match = re.search(r""\s*(.+)\s+ValueError"", excMsg)
            errMsg = ""one of your .pyc files are corrupted%s"" % ("" ('%s')"" % match.group(1) if match else """")
            errMsg += "". Please delete .pyc files on your system to fix the problem""
            logger.critical(errMsg)
            raise SystemExit

        for match in re.finditer(r'File ""(.+?)"", line', excMsg):
            file_ = match.group(1)
            try:
                file_ = os.path.relpath(file_, os.path.dirname(__file__))
            except ValueError:
                pass
            file_ = file_.replace(""\\"", '/')
            if ""../"" in file_:
                file_ = re.sub(r""(\.\./)+"", '/', file_)
            else:
                file_ = file_.lstrip('/')
            file_ = re.sub(r""/{2,}"", '/', file_)
            excMsg = excMsg.replace(match.group(1), file_)

        errMsg = maskSensitiveData(errMsg)
        excMsg = maskSensitiveData(excMsg)

        if conf.get(""api"") or not valid:
            logger.critical(""%s\n%s"" % (errMsg, excMsg))
        else:
            logger.critical(errMsg)
            dataToStdout(""%s\n"" % setColor(excMsg.strip(), level=logging.CRITICAL))
            createGithubIssue(errMsg, excMsg)

    finally:
        kb.threadContinue = False

        if (getDaysFromLastUpdate() or 0) > LAST_UPDATE_NAGGING_DAYS:
            warnMsg = ""your sqlmap version is outdated""
            logger.warning(warnMsg)

        if conf.get(""showTime""):
            dataToStdout(""\n[*] ending @ %s\n\n"" % time.strftime(""%X /%Y-%m-%d/""), forceOutput=True)

        kb.threadException = True

        if kb.get(""tempDir""):
            for prefix in (MKSTEMP_PREFIX.IPC, MKSTEMP_PREFIX.TESTING, MKSTEMP_PREFIX.COOKIE_JAR, MKSTEMP_PREFIX.BIG_ARRAY):
                for filepath in glob.glob(os.path.join(kb.tempDir, ""%s*"" % prefix)):
                    try:
                        os.remove(filepath)
                    except OSError:
                        pass

            if not filterNone(filepath for filepath in glob.glob(os.path.join(kb.tempDir, '*')) if not any(filepath.endswith(_) for _ in ("".lock"", "".exe"", "".so"", '_'))):  # ignore junk files
                try:
                    shutil.rmtree(kb.tempDir, ignore_errors=True)
                except OSError:
                    pass

        if conf.get(""hashDB""):
            conf.hashDB.flush(True)
            conf.hashDB.close()         # NOTE: because of PyPy

        if conf.get(""harFile""):
            try:
                with openFile(conf.harFile, ""w+b"") as f:
                    json.dump(conf.httpCollector.obtain(), fp=f, indent=4, separators=(',', ': '))
            except SqlmapBaseException as ex:
                errMsg = getSafeExString(ex)
                logger.critical(errMsg)

        if conf.get(""api""):
            conf.databaseCursor.disconnect()

        if conf.get(""dumper""):
            conf.dumper.flush()

        # short delay for thread finalization
        _ = time.time()
        while threading.active_count() > 1 and (time.time() - _) > THREAD_FINALIZATION_TIMEOUT:
            time.sleep(0.01)

        if cmdLineOptions.get(""sqlmapShell""):
            cmdLineOptions.clear()
            conf.clear()
            kb.clear()
            conf.disableBanner = True
            main()

if __name__ == ""__main__"":
    try:
        main()
    except KeyboardInterrupt:
        pass
    except SystemExit:
        raise
    except:
        traceback.print_exc()
    finally:
        # Reference: http://stackoverflow.com/questions/1635080/terminate-a-multi-thread-python-program
        if threading.active_count() > 1:
            os._exit(getattr(os, ""_exitcode"", 0))
        else:
            sys.exit(getattr(os, ""_exitcode"", 0))
else:
    # cancelling postponed imports (because of CI/CD checks)
    __import__(""lib.controller.controller"")
"
huggingface_pytorch-image-models,huggingface_pytorch-image-models_benchmark.py,28471,https://raw.githubusercontent.com/huggingface/pytorch-image-models/main/benchmark.py,"#!/usr/bin/env python3
"""""" Model Benchmark Script

An inference and train step benchmark script for timm models.

Hacked together by Ross Wightman (https://github.com/rwightman)
""""""
import argparse
import csv
import json
import logging
import time
from collections import OrderedDict
from contextlib import suppress
from functools import partial

import torch
import torch.nn as nn
import torch.nn.parallel

from timm.data import resolve_data_config
from timm.layers import set_fast_norm
from timm.models import create_model, is_model, list_models
from timm.optim import create_optimizer_v2
from timm.utils import setup_default_logging, set_jit_fuser, decay_batch_step, check_batch_size_retry, ParseKwargs,\
    reparameterize_model

has_apex = False
try:
    from apex import amp
    has_apex = True
except ImportError:
    pass

try:
    from deepspeed.profiling.flops_profiler import get_model_profile
    has_deepspeed_profiling = True
except ImportError as e:
    has_deepspeed_profiling = False

try:
    from fvcore.nn import FlopCountAnalysis, flop_count_str, ActivationCountAnalysis
    has_fvcore_profiling = True
except ImportError as e:
    FlopCountAnalysis = None
    has_fvcore_profiling = False

try:
    from functorch.compile import memory_efficient_fusion
    has_functorch = True
except ImportError as e:
    has_functorch = False

has_compile = hasattr(torch, 'compile')

if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
_logger = logging.getLogger('validate')


parser = argparse.ArgumentParser(description='PyTorch Benchmark')

# benchmark specific args
parser.add_argument('--model-list', metavar='NAME', default='',
                    help='txt file based list of model names to benchmark')
parser.add_argument('--bench', default='both', type=str,
                    help=""Benchmark mode. One of 'inference', 'train', 'both'. Defaults to 'both'"")
parser.add_argument('--detail', action='store_true', default=False,
                    help='Provide train fwd/bwd/opt breakdown detail if True. Defaults to False')
parser.add_argument('--no-retry', action='store_true', default=False,
                    help='Do not decay batch size and retry on error.')
parser.add_argument('--results-file', default='', type=str,
                    help='Output csv file for validation results (summary)')
parser.add_argument('--results-format', default='csv', type=str,
                    help='Format for results file one of (csv, json) (default: csv).')
parser.add_argument('--num-warm-iter', default=10, type=int,
                    help='Number of warmup iterations (default: 10)')
parser.add_argument('--num-bench-iter', default=40, type=int,
                    help='Number of benchmark iterations (default: 40)')
parser.add_argument('--device', default='cuda', type=str,
                    help=""device to run benchmark on"")

# common inference / train args
parser.add_argument('--model', '-m', metavar='NAME', default='resnet50',
                    help='model architecture (default: resnet50)')
parser.add_argument('-b', '--batch-size', default=256, type=int,
                    metavar='N', help='mini-batch size (default: 256)')
parser.add_argument('--img-size', default=None, type=int,
                    metavar='N', help='Input image dimension, uses model default if empty')
parser.add_argument('--input-size', default=None, nargs=3, type=int, metavar='N',
                    help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')
parser.add_argument('--use-train-size', action='store_true', default=False,
                    help='Run inference at train size, not test-input-size if it exists.')
parser.add_argument('--num-classes', type=int, default=None,
                    help='Number classes in dataset')
parser.add_argument('--gp', default=None, type=str, metavar='POOL',
                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')
parser.add_argument('--channels-last', action='store_true', default=False,
                    help='Use channels_last memory layout')
parser.add_argument('--grad-checkpointing', action='store_true', default=False,
                    help='Enable gradient checkpointing through model blocks/stages')
parser.add_argument('--amp', action='store_true', default=False,
                    help='use PyTorch Native AMP for mixed precision training. Overrides --precision arg.')
parser.add_argument('--amp-dtype', default='float16', type=str,
                    help='lower precision AMP dtype (default: float16). Overrides --precision arg if args.amp True.')
parser.add_argument('--precision', default='float32', type=str,
                    help='Numeric precision. One of (amp, float32, float16, bfloat16, tf32)')
parser.add_argument('--fuser', default='', type=str,
                    help=""Select jit fuser. One of ('', 'te', 'old', 'nvfuser')"")
parser.add_argument('--fast-norm', default=False, action='store_true',
                    help='enable experimental fast-norm')
parser.add_argument('--reparam', default=False, action='store_true',
                    help='Reparameterize model')
parser.add_argument('--model-kwargs', nargs='*', default={}, action=ParseKwargs)
parser.add_argument('--torchcompile-mode', type=str, default=None,
                    help=""torch.compile mode (default: None)."")

# codegen (model compilation) options
scripting_group = parser.add_mutually_exclusive_group()
scripting_group.add_argument('--torchscript', dest='torchscript', action='store_true',
                             help='convert model torchscript for inference')
scripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',
                             help=""Enable compilation w/ specified backend (default: inductor)."")
scripting_group.add_argument('--aot-autograd', default=False, action='store_true',
                             help=""Enable AOT Autograd optimization."")

# train optimizer parameters
parser.add_argument('--opt', default='sgd', type=str, metavar='OPTIMIZER',
                    help='Optimizer (default: ""sgd""')
parser.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',
                    help='Optimizer Epsilon (default: None, use opt default)')
parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',
                    help='Optimizer Betas (default: None, use opt default)')
parser.add_argument('--momentum', type=float, default=0.9, metavar='M',
                    help='Optimizer momentum (default: 0.9)')
parser.add_argument('--weight-decay', type=float, default=0.0001,
                    help='weight decay (default: 0.0001)')
parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',
                    help='Clip gradient norm (default: None, no clipping)')
parser.add_argument('--clip-mode', type=str, default='norm',
                    help='Gradient clipping mode. One of (""norm"", ""value"", ""agc"")')


# model regularization / loss params that impact model or loss fn
parser.add_argument('--smoothing', type=float, default=0.1,
                    help='Label smoothing (default: 0.1)')
parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',
                    help='Dropout rate (default: 0.)')
parser.add_argument('--drop-path', type=float, default=None, metavar='PCT',
                    help='Drop path rate (default: None)')
parser.add_argument('--drop-block', type=float, default=None, metavar='PCT',
                    help='Drop block rate (default: None)')


def timestamp(sync=False):
    return time.perf_counter()


def cuda_timestamp(sync=False, device=None):
    if sync:
        torch.cuda.synchronize(device=device)
    return time.perf_counter()


def count_params(model: nn.Module):
    return sum([m.numel() for m in model.parameters()])


def resolve_precision(precision: str):
    assert precision in ('amp', 'amp_bfloat16', 'float16', 'bfloat16', 'float32')
    amp_dtype = None  # amp disabled
    model_dtype = torch.float32
    data_dtype = torch.float32
    if precision == 'amp':
        amp_dtype = torch.float16
    elif precision == 'amp_bfloat16':
        amp_dtype = torch.bfloat16
    elif precision == 'float16':
        model_dtype = torch.float16
        data_dtype = torch.float16
    elif precision == 'bfloat16':
        model_dtype = torch.bfloat16
        data_dtype = torch.bfloat16
    return amp_dtype, model_dtype, data_dtype


def profile_deepspeed(model, input_size=(3, 224, 224), batch_size=1, detailed=False):
    _, macs, _ = get_model_profile(
        model=model,
        input_shape=(batch_size,) + input_size,  # input shape/resolution
        print_profile=detailed,  # prints the model graph with the measured profile attached to each module
        detailed=detailed,  # print the detailed profile
        warm_up=10,  # the number of warm-ups before measuring the time of each module
        as_string=False,  # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)
        output_file=None,  # path to the output file. If None, the profiler prints to stdout.
        ignore_modules=None)  # the list of modules to ignore in the profiling
    return macs, 0  # no activation count in DS


def profile_fvcore(model, input_size=(3, 224, 224), batch_size=1, detailed=False, force_cpu=False):
    if force_cpu:
        model = model.to('cpu')
    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
    example_input = torch.ones((batch_size,) + input_size, device=device, dtype=dtype)
    fca = FlopCountAnalysis(model, example_input)
    aca = ActivationCountAnalysis(model, example_input)
    if detailed:
        fcs = flop_count_str(fca)
        print(fcs)
    return fca.total(), aca.total()


class BenchmarkRunner:
    def __init__(
            self,
            model_name,
            detail=False,
            device='cuda',
            torchscript=False,
            torchcompile=None,
            torchcompile_mode=None,
            aot_autograd=False,
            reparam=False,
            precision='float32',
            fuser='',
            num_warm_iter=10,
            num_bench_iter=50,
            use_train_size=False,
            **kwargs
    ):
        self.model_name = model_name
        self.detail = detail
        self.device = device
        self.amp_dtype, self.model_dtype, self.data_dtype = resolve_precision(precision)
        self.channels_last = kwargs.pop('channels_last', False)
        if self.amp_dtype is not None:
            self.amp_autocast = partial(torch.amp.autocast, device_type=device, dtype=self.amp_dtype)
        else:
            self.amp_autocast = suppress

        if fuser:
            set_jit_fuser(fuser)
        self.model = create_model(
            model_name,
            num_classes=kwargs.pop('num_classes', None),
            in_chans=3,
            global_pool=kwargs.pop('gp', 'fast'),
            scriptable=torchscript,
            drop_rate=kwargs.pop('drop', 0.),
            drop_path_rate=kwargs.pop('drop_path', None),
            drop_block_rate=kwargs.pop('drop_block', None),
            **kwargs.pop('model_kwargs', {}),
        )
        if reparam:
            self.model = reparameterize_model(self.model)
        self.model.to(
            device=self.device,
            dtype=self.model_dtype,
            memory_format=torch.channels_last if self.channels_last else None,
        )
        self.num_classes = self.model.num_classes
        self.param_count = count_params(self.model)
        _logger.info('Model %s created, param count: %d' % (model_name, self.param_count))

        data_config = resolve_data_config(kwargs, model=self.model, use_test_size=not use_train_size)
        self.input_size = data_config['input_size']
        self.batch_size = kwargs.pop('batch_size', 256)

        self.compiled = False
        if torchscript:
            self.model = torch.jit.script(self.model)
            self.compiled = True
        elif torchcompile:
            assert has_compile, 'A version of torch w/ torch.compile() is required, possibly a nightly.'
            torch._dynamo.reset()
            self.model = torch.compile(self.model, backend=torchcompile, mode=torchcompile_mode)
            self.compiled = True
        elif aot_autograd:
            assert has_functorch, ""functorch is needed for --aot-autograd""
            self.model = memory_efficient_fusion(self.model)
            self.compiled = True

        self.example_inputs = None
        self.num_warm_iter = num_warm_iter
        self.num_bench_iter = num_bench_iter
        self.log_freq = num_bench_iter // 5
        if 'cuda' in self.device:
            self.time_fn = partial(cuda_timestamp, device=self.device)
        else:
            self.time_fn = timestamp

    def _init_input(self):
        self.example_inputs = torch.randn(
            (self.batch_size,) + self.input_size, device=self.device, dtype=self.data_dtype)
        if self.channels_last:
            self.example_inputs = self.example_inputs.contiguous(memory_format=torch.channels_last)


class InferenceBenchmarkRunner(BenchmarkRunner):

    def __init__(
            self,
            model_name,
            device='cuda',
            torchscript=False,
            **kwargs
    ):
        super().__init__(model_name=model_name, device=device, torchscript=torchscript, **kwargs)
        self.model.eval()

    def run(self):
        def _step():
            t_step_start = self.time_fn()
            with self.amp_autocast():
                output = self.model(self.example_inputs)
            t_step_end = self.time_fn(True)
            return t_step_end - t_step_start

        _logger.info(
            f'Running inference benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '
            f'input size {self.input_size} and batch size {self.batch_size}.')

        with torch.no_grad():
            self._init_input()

            for _ in range(self.num_warm_iter):
                _step()

            total_step = 0.
            num_samples = 0
            t_run_start = self.time_fn()
            for i in range(self.num_bench_iter):
                delta_fwd = _step()
                total_step += delta_fwd
                num_samples += self.batch_size
                num_steps = i + 1
                if num_steps % self.log_freq == 0:
                    _logger.info(
                        f""Infer [{num_steps}/{self.num_bench_iter}].""
                        f"" {num_samples / total_step:0.2f} samples/sec.""
                        f"" {1000 * total_step / num_steps:0.3f} ms/step."")
            t_run_end = self.time_fn(True)
            t_run_elapsed = t_run_end - t_run_start

        results = dict(
            samples_per_sec=round(num_samples / t_run_elapsed, 2),
            step_time=round(1000 * total_step / self.num_bench_iter, 3),
            batch_size=self.batch_size,
            img_size=self.input_size[-1],
            param_count=round(self.param_count / 1e6, 2),
        )

        retries = 0 if self.compiled else 2  # skip profiling if model is scripted
        while retries:
            retries -= 1
            try:
                if has_deepspeed_profiling:
                    macs, _ = profile_deepspeed(self.model, self.input_size)
                    results['gmacs'] = round(macs / 1e9, 2)
                elif has_fvcore_profiling:
                    macs, activations = profile_fvcore(self.model, self.input_size, force_cpu=not retries)
                    results['gmacs'] = round(macs / 1e9, 2)
                    results['macts'] = round(activations / 1e6, 2)
            except RuntimeError as e:
                pass

        _logger.info(
            f""Inference benchmark of {self.model_name} done. ""
            f""{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/step"")

        return results


class TrainBenchmarkRunner(BenchmarkRunner):

    def __init__(
            self,
            model_name,
            device='cuda',
            torchscript=False,
            **kwargs
    ):
        super().__init__(model_name=model_name, device=device, torchscript=torchscript, **kwargs)
        self.model.train()

        self.loss = nn.CrossEntropyLoss().to(self.device)
        self.target_shape = tuple()

        self.optimizer = create_optimizer_v2(
            self.model,
            opt=kwargs.pop('opt', 'sgd'),
            lr=kwargs.pop('lr', 1e-4))

        if kwargs.pop('grad_checkpointing', False):
            self.model.set_grad_checkpointing()

    def _gen_target(self, batch_size):
        return torch.empty(
            (batch_size,) + self.target_shape, device=self.device, dtype=torch.long).random_(self.num_classes)

    def run(self):
        def _step(detail=False):
            self.optimizer.zero_grad()  # can this be ignored?
            t_start = self.time_fn()
            t_fwd_end = t_start
            t_bwd_end = t_start
            with self.amp_autocast():
                output = self.model(self.example_inputs)
                if isinstance(output, tuple):
                    output = output[0]
                if detail:
                    t_fwd_end = self.time_fn(True)
                target = self._gen_target(output.shape[0])
                self.loss(output, target).backward()
                if detail:
                    t_bwd_end = self.time_fn(True)
            self.optimizer.step()
            t_end = self.time_fn(True)
            if detail:
                delta_fwd = t_fwd_end - t_start
                delta_bwd = t_bwd_end - t_fwd_end
                delta_opt = t_end - t_bwd_end
                return delta_fwd, delta_bwd, delta_opt
            else:
                delta_step = t_end - t_start
                return delta_step

        _logger.info(
            f'Running train benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '
            f'input size {self.input_size} and batch size {self.batch_size}.')

        self._init_input()

        for _ in range(self.num_warm_iter):
            _step()

        t_run_start = self.time_fn()
        if self.detail:
            total_fwd = 0.
            total_bwd = 0.
            total_opt = 0.
            num_samples = 0
            for i in range(self.num_bench_iter):
                delta_fwd, delta_bwd, delta_opt = _step(True)
                num_samples += self.batch_size
                total_fwd += delta_fwd
                total_bwd += delta_bwd
                total_opt += delta_opt
                num_steps = (i + 1)
                if num_steps % self.log_freq == 0:
                    total_step = total_fwd + total_bwd + total_opt
                    _logger.info(
                        f""Train [{num_steps}/{self.num_bench_iter}].""
                        f"" {num_samples / total_step:0.2f} samples/sec.""
                        f"" {1000 * total_fwd / num_steps:0.3f} ms/step fwd,""
                        f"" {1000 * total_bwd / num_steps:0.3f} ms/step bwd,""
                        f"" {1000 * total_opt / num_steps:0.3f} ms/step opt.""
                    )
            total_step = total_fwd + total_bwd + total_opt
            t_run_elapsed = self.time_fn() - t_run_start
            results = dict(
                samples_per_sec=round(num_samples / t_run_elapsed, 2),
                step_time=round(1000 * total_step / self.num_bench_iter, 3),
                fwd_time=round(1000 * total_fwd / self.num_bench_iter, 3),
                bwd_time=round(1000 * total_bwd / self.num_bench_iter, 3),
                opt_time=round(1000 * total_opt / self.num_bench_iter, 3),
                batch_size=self.batch_size,
                img_size=self.input_size[-1],
                param_count=round(self.param_count / 1e6, 2),
            )
        else:
            total_step = 0.
            num_samples = 0
            for i in range(self.num_bench_iter):
                delta_step = _step(False)
                num_samples += self.batch_size
                total_step += delta_step
                num_steps = (i + 1)
                if num_steps % self.log_freq == 0:
                    _logger.info(
                        f""Train [{num_steps}/{self.num_bench_iter}].""
                        f"" {num_samples / total_step:0.2f} samples/sec.""
                        f"" {1000 * total_step / num_steps:0.3f} ms/step."")
            t_run_elapsed = self.time_fn() - t_run_start
            results = dict(
                samples_per_sec=round(num_samples / t_run_elapsed, 2),
                step_time=round(1000 * total_step / self.num_bench_iter, 3),
                batch_size=self.batch_size,
                img_size=self.input_size[-1],
                param_count=round(self.param_count / 1e6, 2),
            )

        _logger.info(
            f""Train benchmark of {self.model_name} done. ""
            f""{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/sample"")

        return results


class ProfileRunner(BenchmarkRunner):

    def __init__(self, model_name, device='cuda', profiler='', **kwargs):
        super().__init__(model_name=model_name, device=device, **kwargs)
        if not profiler:
            if has_deepspeed_profiling:
                profiler = 'deepspeed'
            elif has_fvcore_profiling:
                profiler = 'fvcore'
        assert profiler, ""One of deepspeed or fvcore needs to be installed for profiling to work.""
        self.profiler = profiler
        self.model.eval()

    def run(self):
        _logger.info(
            f'Running profiler on {self.model_name} w/ '
            f'input size {self.input_size} and batch size {self.batch_size}.')

        macs = 0
        activations = 0
        if self.profiler == 'deepspeed':
            macs, _ = profile_deepspeed(self.model, self.input_size, batch_size=self.batch_size, detailed=True)
        elif self.profiler == 'fvcore':
            macs, activations = profile_fvcore(self.model, self.input_size, batch_size=self.batch_size, detailed=True)

        results = dict(
            gmacs=round(macs / 1e9, 2),
            macts=round(activations / 1e6, 2),
            batch_size=self.batch_size,
            img_size=self.input_size[-1],
            param_count=round(self.param_count / 1e6, 2),
        )

        _logger.info(
            f""Profile of {self.model_name} done. ""
            f""{results['gmacs']:.2f} GMACs, {results['param_count']:.2f} M params."")

        return results


def _try_run(
        model_name,
        bench_fn,
        bench_kwargs,
        initial_batch_size,
        no_batch_size_retry=False
):
    batch_size = initial_batch_size
    results = dict()
    error_str = 'Unknown'
    while batch_size:
        try:
            torch.cuda.empty_cache()
            bench = bench_fn(model_name=model_name, batch_size=batch_size, **bench_kwargs)
            results = bench.run()
            return results
        except RuntimeError as e:
            error_str = str(e)
            _logger.error(f'""{error_str}"" while running benchmark.')
            if not check_batch_size_retry(error_str):
                _logger.error(f'Unrecoverable error encountered while benchmarking {model_name}, skipping.')
                break
            if no_batch_size_retry:
                break
        batch_size = decay_batch_step(batch_size)
        _logger.warning(f'Reducing batch size to {batch_size} for retry.')
    results['error'] = error_str
    return results


def benchmark(args):
    if args.amp:
        _logger.warning(""Overriding precision to 'amp' since --amp flag set."")
        args.precision = 'amp' if args.amp_dtype == 'float16' else '_'.join(['amp', args.amp_dtype])
    _logger.info(f'Benchmarking in {args.precision} precision. '
                 f'{""NHWC"" if args.channels_last else ""NCHW""} layout. '
                 f'torchscript {""enabled"" if args.torchscript else ""disabled""}')

    bench_kwargs = vars(args).copy()
    bench_kwargs.pop('amp')
    model = bench_kwargs.pop('model')
    batch_size = bench_kwargs.pop('batch_size')

    bench_fns = (InferenceBenchmarkRunner,)
    prefixes = ('infer',)
    if args.bench == 'both':
        bench_fns = (
            InferenceBenchmarkRunner,
            TrainBenchmarkRunner
        )
        prefixes = ('infer', 'train')
    elif args.bench == 'train':
        bench_fns = TrainBenchmarkRunner,
        prefixes = 'train',
    elif args.bench.startswith('profile'):
        # specific profiler used if included in bench mode string, otherwise default to deepspeed, fallback to fvcore
        if 'deepspeed' in args.bench:
            assert has_deepspeed_profiling, ""deepspeed must be installed to use deepspeed flop counter""
            bench_kwargs['profiler'] = 'deepspeed'
        elif 'fvcore' in args.bench:
            assert has_fvcore_profiling, ""fvcore must be installed to use fvcore flop counter""
            bench_kwargs['profiler'] = 'fvcore'
        bench_fns = ProfileRunner,
        batch_size = 1

    model_results = OrderedDict(model=model)
    for prefix, bench_fn in zip(prefixes, bench_fns):
        run_results = _try_run(
            model,
            bench_fn,
            bench_kwargs=bench_kwargs,
            initial_batch_size=batch_size,
            no_batch_size_retry=args.no_retry,
        )
        if prefix and 'error' not in run_results:
            run_results = {'_'.join([prefix, k]): v for k, v in run_results.items()}
        model_results.update(run_results)
        if 'error' in run_results:
            break
    if 'error' not in model_results:
        param_count = model_results.pop('infer_param_count', model_results.pop('train_param_count', 0))
        model_results.setdefault('param_count', param_count)
        model_results.pop('train_param_count', 0)
    return model_results


def main():
    setup_default_logging()
    args = parser.parse_args()
    model_cfgs = []
    model_names = []

    if args.fast_norm:
        set_fast_norm()

    if args.model_list:
        args.model = ''
        with open(args.model_list) as f:
            model_names = [line.rstrip() for line in f]
        model_cfgs = [(n, None) for n in model_names]
    elif args.model == 'all':
        # validate all models in a list of names with pretrained checkpoints
        args.pretrained = True
        model_names = list_models(pretrained=True, exclude_filters=['*in21k'])
        model_cfgs = [(n, None) for n in model_names]
    elif not is_model(args.model):
        # model name doesn't exist, try as wildcard filter
        model_names = list_models(args.model)
        model_cfgs = [(n, None) for n in model_names]

    if len(model_cfgs):
        _logger.info('Running bulk validation on these pretrained models: {}'.format(', '.join(model_names)))
        results = []
        try:
            for m, _ in model_cfgs:
                if not m:
                    continue
                args.model = m
                r = benchmark(args)
                if r:
                    results.append(r)
                time.sleep(10)
        except KeyboardInterrupt as e:
            pass
        sort_key = 'infer_samples_per_sec'
        if 'train' in args.bench:
            sort_key = 'train_samples_per_sec'
        elif 'profile' in args.bench:
            sort_key = 'infer_gmacs'
        results = filter(lambda x: sort_key in x, results)
        results = sorted(results, key=lambda x: x[sort_key], reverse=True)
    else:
        results = benchmark(args)

    if args.results_file:
        write_results(args.results_file, results, format=args.results_format)

    # output results in JSON to stdout w/ delimiter for runner script
    print(f'--result\n{json.dumps(results, indent=4)}')


def write_results(results_file, results, format='csv'):
    with open(results_file, mode='w') as cf:
        if format == 'json':
            json.dump(results, cf, indent=4)
        else:
            if not isinstance(results, (list, tuple)):
                results = [results]
            if not results:
                return
            dw = csv.DictWriter(cf, fieldnames=results[0].keys())
            dw.writeheader()
            for r in results:
                dw.writerow(r)
            cf.flush()


if __name__ == '__main__':
    main()
"
XingangPan_DragGAN,XingangPan_DragGAN_visualizer_drag.py,17150,https://raw.githubusercontent.com/XingangPan/DragGAN/main/visualizer_drag.py,"# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import click
import os

import multiprocessing
import numpy as np
import torch
import imgui
import dnnlib
from gui_utils import imgui_window
from gui_utils import imgui_utils
from gui_utils import gl_utils
from gui_utils import text_utils
from viz import renderer
from viz import pickle_widget
from viz import latent_widget
from viz import drag_widget
from viz import capture_widget

#----------------------------------------------------------------------------

class Visualizer(imgui_window.ImguiWindow):
    def __init__(self, capture_dir=None):
        super().__init__(title='DragGAN', window_width=3840, window_height=2160)

        # Internals.
        self._last_error_print  = None
        self._async_renderer    = AsyncRenderer()
        self._defer_rendering   = 0
        self._tex_img           = None
        self._tex_obj           = None
        self._mask_obj          = None
        self._image_area        = None
        self._status            = dnnlib.EasyDict()

        # Widget interface.
        self.args               = dnnlib.EasyDict()
        self.result             = dnnlib.EasyDict()
        self.pane_w             = 0
        self.label_w            = 0
        self.button_w           = 0
        self.image_w            = 0
        self.image_h            = 0

        # Widgets.
        self.pickle_widget      = pickle_widget.PickleWidget(self)
        self.latent_widget      = latent_widget.LatentWidget(self)
        self.drag_widget        = drag_widget.DragWidget(self)
        self.capture_widget     = capture_widget.CaptureWidget(self)

        if capture_dir is not None:
            self.capture_widget.path = capture_dir

        # Initialize window.
        self.set_position(0, 0)
        self._adjust_font_size()
        self.skip_frame() # Layout may change after first frame.

    def close(self):
        super().close()
        if self._async_renderer is not None:
            self._async_renderer.close()
            self._async_renderer = None

    def add_recent_pickle(self, pkl, ignore_errors=False):
        self.pickle_widget.add_recent(pkl, ignore_errors=ignore_errors)

    def load_pickle(self, pkl, ignore_errors=False):
        self.pickle_widget.load(pkl, ignore_errors=ignore_errors)

    def print_error(self, error):
        error = str(error)
        if error != self._last_error_print:
            print('\n' + error + '\n')
            self._last_error_print = error

    def defer_rendering(self, num_frames=1):
        self._defer_rendering = max(self._defer_rendering, num_frames)

    def clear_result(self):
        self._async_renderer.clear_result()

    def set_async(self, is_async):
        if is_async != self._async_renderer.is_async:
            self._async_renderer.set_async(is_async)
            self.clear_result()
            if 'image' in self.result:
                self.result.message = 'Switching rendering process...'
                self.defer_rendering()

    def _adjust_font_size(self):
        old = self.font_size
        self.set_font_size(min(self.content_width / 120, self.content_height / 60))
        if self.font_size != old:
            self.skip_frame() # Layout changed.

    def check_update_mask(self, **args):
        update_mask = False
        if 'pkl' in self._status:
            if self._status.pkl != args['pkl']:
                update_mask = True
        self._status.pkl = args['pkl']
        if 'w0_seed' in self._status:
            if self._status.w0_seed != args['w0_seed']:
                update_mask = True
        self._status.w0_seed = args['w0_seed']
        return update_mask

    def capture_image_frame(self):
        self.capture_next_frame()
        captured_frame = self.pop_captured_frame()
        captured_image = None
        if captured_frame is not None:
            x1, y1, w, h = self._image_area
            captured_image = captured_frame[y1:y1+h, x1:x1+w, :]
        return captured_image

    def get_drag_info(self):
        seed = self.latent_widget.seed
        points = self.drag_widget.points
        targets = self.drag_widget.targets
        mask = self.drag_widget.mask
        w = self._async_renderer._renderer_obj.w
        return seed, points, targets, mask, w

    def draw_frame(self):
        self.begin_frame()
        self.args = dnnlib.EasyDict()
        self.pane_w = self.font_size * 18
        self.button_w = self.font_size * 5
        self.label_w = round(self.font_size * 4.5)

        # Detect mouse dragging in the result area.
        if self._image_area is not None:
            if not hasattr(self.drag_widget, 'width'):
                self.drag_widget.init_mask(self.image_w, self.image_h)
            clicked, down, img_x, img_y = imgui_utils.click_hidden_window(
                '##image_area', self._image_area[0], self._image_area[1], self._image_area[2], self._image_area[3], self.image_w, self.image_h)
            self.drag_widget.action(clicked, down, img_x, img_y)

        # Begin control pane.
        imgui.set_next_window_position(0, 0)
        imgui.set_next_window_size(self.pane_w, self.content_height)
        imgui.begin('##control_pane', closable=False, flags=(imgui.WINDOW_NO_TITLE_BAR | imgui.WINDOW_NO_RESIZE | imgui.WINDOW_NO_MOVE))

        # Widgets.
        expanded, _visible = imgui_utils.collapsing_header('Network & latent', default=True)
        self.pickle_widget(expanded)
        self.latent_widget(expanded)
        expanded, _visible = imgui_utils.collapsing_header('Drag', default=True)
        self.drag_widget(expanded)
        expanded, _visible = imgui_utils.collapsing_header('Capture', default=True)
        self.capture_widget(expanded)

        # Render.
        if self.is_skipping_frames():
            pass
        elif self._defer_rendering > 0:
            self._defer_rendering -= 1
        elif self.args.pkl is not None:
            self._async_renderer.set_args(**self.args)
            result = self._async_renderer.get_result()
            if result is not None:
                self.result = result        
                if 'stop' in self.result and self.result.stop:
                    self.drag_widget.stop_drag()
                if 'points' in self.result:
                    self.drag_widget.set_points(self.result.points)
                if 'init_net' in self.result:
                    if self.result.init_net:
                        self.drag_widget.reset_point()

        # Display.
        max_w = self.content_width - self.pane_w
        max_h = self.content_height
        pos = np.array([self.pane_w + max_w / 2, max_h / 2])
        if 'image' in self.result:
            # Reset mask after loading a new pickle or changing seed.
            if self.check_update_mask(**self.args):
                h, w, _ = self.result.image.shape
                self.drag_widget.init_mask(w, h)

            if self._tex_img is not self.result.image:
                self._tex_img = self.result.image
                if self._tex_obj is None or not self._tex_obj.is_compatible(image=self._tex_img):
                    self._tex_obj = gl_utils.Texture(image=self._tex_img, bilinear=False, mipmap=False)
                else:
                    self._tex_obj.update(self._tex_img)
                self.image_h, self.image_w = self._tex_obj.height, self._tex_obj.width
            zoom = min(max_w / self._tex_obj.width, max_h / self._tex_obj.height)
            zoom = np.floor(zoom) if zoom >= 1 else zoom
            self._tex_obj.draw(pos=pos, zoom=zoom, align=0.5, rint=True)
            if self.drag_widget.show_mask and hasattr(self.drag_widget, 'mask'):
                mask = ((1-self.drag_widget.mask.unsqueeze(-1)) * 255).to(torch.uint8)
                if self._mask_obj is None or not self._mask_obj.is_compatible(image=self._tex_img):
                    self._mask_obj = gl_utils.Texture(image=mask, bilinear=False, mipmap=False)
                else:
                    self._mask_obj.update(mask)
                self._mask_obj.draw(pos=pos, zoom=zoom, align=0.5, rint=True, alpha=0.15)

            if self.drag_widget.mode in ['flexible', 'fixed']:
                posx, posy = imgui.get_mouse_pos()
                if posx >= self.pane_w:
                    pos_c = np.array([posx, posy])
                    gl_utils.draw_circle(center=pos_c, radius=self.drag_widget.r_mask * zoom, alpha=0.5)
            
            rescale = self._tex_obj.width / 512 * zoom
            
            for point in self.drag_widget.targets:
                pos_x = self.pane_w + max_w / 2 + (point[1] - self.image_w//2) * zoom
                pos_y = max_h / 2 + (point[0] - self.image_h//2) * zoom
                gl_utils.draw_circle(center=np.array([pos_x, pos_y]), color=[0,0,1], radius=9 * rescale)
            
            for point in self.drag_widget.points:
                pos_x = self.pane_w + max_w / 2 + (point[1] - self.image_w//2) * zoom
                pos_y = max_h / 2 + (point[0] - self.image_h//2) * zoom
                gl_utils.draw_circle(center=np.array([pos_x, pos_y]), color=[1,0,0], radius=9 * rescale)

            for point, target in zip(self.drag_widget.points, self.drag_widget.targets):
                t_x = self.pane_w + max_w / 2 + (target[1] - self.image_w//2) * zoom
                t_y = max_h / 2 + (target[0] - self.image_h//2) * zoom

                p_x = self.pane_w + max_w / 2 + (point[1] - self.image_w//2) * zoom
                p_y = max_h / 2 + (point[0] - self.image_h//2) * zoom

                gl_utils.draw_arrow(p_x, p_y, t_x, t_y, l=8 * rescale, width = 3 * rescale)

            imshow_w = int(self._tex_obj.width * zoom)
            imshow_h = int(self._tex_obj.height * zoom)
            self._image_area = [int(self.pane_w + max_w / 2 - imshow_w / 2), int(max_h / 2 - imshow_h / 2), imshow_w, imshow_h]
        if 'error' in self.result:
            self.print_error(self.result.error)
            if 'message' not in self.result:
                self.result.message = str(self.result.error)
        if 'message' in self.result:
            tex = text_utils.get_texture(self.result.message, size=self.font_size, max_width=max_w, max_height=max_h, outline=2)
            tex.draw(pos=pos, align=0.5, rint=True, color=1)

        # End frame.
        self._adjust_font_size()
        imgui.end()
        self.end_frame()

#----------------------------------------------------------------------------

class AsyncRenderer:
    def __init__(self):
        self._closed        = False
        self._is_async      = False
        self._cur_args      = None
        self._cur_result    = None
        self._cur_stamp     = 0
        self._renderer_obj  = None
        self._args_queue    = None
        self._result_queue  = None
        self._process       = None

    def close(self):
        self._closed = True
        self._renderer_obj = None
        if self._process is not None:
            self._process.terminate()
        self._process = None
        self._args_queue = None
        self._result_queue = None

    @property
    def is_async(self):
        return self._is_async

    def set_async(self, is_async):
        self._is_async = is_async

    def set_args(self, **args):
        assert not self._closed
        args2 = args.copy()
        args_mask = args2.pop('mask')
        if self._cur_args:
            _cur_args = self._cur_args.copy()
            cur_args_mask = _cur_args.pop('mask')
        else:
            _cur_args = self._cur_args
        # if args != self._cur_args:
        if args2 != _cur_args:
            if self._is_async:
                self._set_args_async(**args)
            else:
                self._set_args_sync(**args)
            self._cur_args = args

    def _set_args_async(self, **args):
        if self._process is None:
            self._args_queue = multiprocessing.Queue()
            self._result_queue = multiprocessing.Queue()
            try:
                multiprocessing.set_start_method('spawn')
            except RuntimeError:
                pass
            self._process = multiprocessing.Process(target=self._process_fn, args=(self._args_queue, self._result_queue), daemon=True)
            self._process.start()
        self._args_queue.put([args, self._cur_stamp])

    def _set_args_sync(self, **args):
        if self._renderer_obj is None:
            self._renderer_obj = renderer.Renderer()
        self._cur_result = self._renderer_obj.render(**args)

    def get_result(self):
        assert not self._closed
        if self._result_queue is not None:
            while self._result_queue.qsize() > 0:
                result, stamp = self._result_queue.get()
                if stamp == self._cur_stamp:
                    self._cur_result = result
        return self._cur_result

    def clear_result(self):
        assert not self._closed
        self._cur_args = None
        self._cur_result = None
        self._cur_stamp += 1

    @staticmethod
    def _process_fn(args_queue, result_queue):
        renderer_obj = renderer.Renderer()
        cur_args = None
        cur_stamp = None
        while True:
            args, stamp = args_queue.get()
            while args_queue.qsize() > 0:
                args, stamp = args_queue.get()
            if args != cur_args or stamp != cur_stamp:
                result = renderer_obj.render(**args)
                if 'error' in result:
                    result.error = renderer.CapturedException(result.error)
                result_queue.put([result, stamp])
                cur_args = args
                cur_stamp = stamp

#----------------------------------------------------------------------------

@click.command()
@click.argument('pkls', metavar='PATH', nargs=-1)
@click.option('--capture-dir', help='Where to save screenshot captures', metavar='PATH', default=None)
@click.option('--browse-dir', help='Specify model path for the \'Browse...\' button', metavar='PATH')
def main(
    pkls,
    capture_dir,
    browse_dir
):
    """"""Interactive model visualizer.

    Optional PATH argument can be used specify which .pkl file to load.
    """"""
    viz = Visualizer(capture_dir=capture_dir)

    if browse_dir is not None:
        viz.pickle_widget.search_dirs = [browse_dir]

    # List pickles.
    if len(pkls) > 0:
        for pkl in pkls:
            viz.add_recent_pickle(pkl)
        viz.load_pickle(pkls[0])
    else:
        pretrained = [
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqcat-512x512.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqdog-512x512.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqv2-512x512.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqwild-512x512.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-brecahad-512x512.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-celebahq-256x256.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-cifar10-32x32.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-512x512.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-1024x1024.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-256x256.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-lsundog-256x256.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-metfaces-1024x1024.pkl',
            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-metfacesu-1024x1024.pkl'
        ]

        # Populate recent pickles list with pretrained model URLs.
        for url in pretrained:
            viz.add_recent_pickle(url)

    # Run.
    while not viz.should_close():
        viz.draw_frame()
    viz.close()

#----------------------------------------------------------------------------

if __name__ == ""__main__"":
    main()

#----------------------------------------------------------------------------
"
geekcomputers_Python,geekcomputers_Python_bookstore_manangement_system.py,22426,https://raw.githubusercontent.com/geekcomputers/Python/master/bookstore_manangement_system.py,"import os


import mysql.connector as mys

mycon = mys.connect(
    host=""localhost"", user=""root"", passwd=""Yksrocks"", database=""book_store_management""
)


if mycon.is_connected():
    print()
    print(""successfully connected"")

mycur = mycon.cursor()


def DBZ():

    # IF  NO.  OF  BOOKS  IS     ZERO(0)     THAN  DELETE  IT  AUTOMATICALLY

    display = ""select * from books""
    mycur.execute(display)
    data2 = mycur.fetchall()

    for y in data2:

        if y[6] <= 0:

            delete = ""delete from books where  Numbers_of_book<=0""
            mycur.execute(delete)
            mycon.commit()


def separator():
    print()
    print(""\t\t========================================"")
    print()


def end_separator():
    print()
    print()


def login():

    user_name = input("" USER NAME  ---  "")
    passw = input("" PASSWORD  ---  "")

    display = ""select * from login""
    mycur.execute(display)
    data2 = mycur.fetchall()

    for y in data2:

        if y[1] == user_name and y[2] == passw:

            pass

        else:

            separator()

            print("" Username  or  Password  is  Incorrect  Try Again"")

            separator()

            user_name = input("" USER NAME  ---  "")
            passw = input("" PASSWORD  ---  "")

            if y[1] == user_name and y[2] == passw:

                pass

            else:

                separator()

                print("" Username  or  Password  is  Again  Incorrect"")
                exit()


def ViewAll():

    print(""\u0332"".join(""BOOK NAMES~~""))
    print(""------------------------------------"")

    display = ""select * from books""
    mycur.execute(display)
    data2 = mycur.fetchall()
    c = 0

    for y in data2:

        c = c + 1
        print(c, ""-->"", y[1])


def CNB1():

    if y[6] == 0:

        separator()

        print("" NOW  THIS  BOOK  IS  NOT  AVAILABLE "")

    elif y[6] > 0 and y[6] <= 8:

        separator()

        print(""WARNING!!!!!!!!!!!!!!!!!!!!!!!"")
        print(""NO.  OF THIS BOOK IS LOW"", ""\tONLY"", y[6] - 1, ""LEFT"")

        print()
        print()

    elif y[6] > 8:

        separator()

        print(""NO.  OF  BOOKS  LEFT  IS "", y[6] - 1)

        print()
        print()


def CNB2():

    if y[6] <= 8:

        separator()

        print(""WARNING!!!!!!!!!!!!!!!!!!!!!!!"")
        print(""NO.  OF THIS BOOK IS LOW"", ""\tONLY"", y[6], ""LEFT"")

    else:

        separator()

        print(""NO.  OF  BOOKS  LEFT  IS "", y[6])


separator()


# LOGIN


display12 = ""select * from visit""
mycur.execute(display12)
data2222 = mycur.fetchall()
for m in data2222:

    if m[0] == 0:

        c = m[0]
        display11 = ""select * from login""
        mycur.execute(display11)
        data222 = mycur.fetchall()

        if c == 0:

            if c == 0:

                print(""\t\t\t\t REGESTER     "")
                print(""\t\t\t\t----------------------------"")

                print()
                print()

                user_name = input(""ENTER  USER  NAME -- "")
                passw = input(""ENTER  PASSWORD  limit 8-20  -- "")
                lenght = len(passw)

                if lenght >= 8 and lenght <= 20:

                    c = c + 1
                    insert55 = (c, user_name, passw)
                    insert22 = ""insert into login values(%s,%s,%s)""
                    mycur.execute(insert22, insert55)
                    mycon.commit()

                    separator()

                    login()

                else:

                    if lenght < 8:

                        separator()

                        print("" Password Is less than  8  Characters  Enter Again"")

                        separator()

                        user_name2 = input(""ENTER  USER  NAME -- "")
                        passw2 = input(""ENTER  PASSWORD AGAIN (limit 8-20) -- "")
                        lenght1 = len(passw2)

                        if lenght1 >= 8 and lenght1 <= 20:

                            c = c + 1
                            insert555 = (c, user_name2, passw2)
                            insert222 = ""insert into login values(%s,%s,%s)""
                            mycur.execute(insert222, insert555)
                            mycon.commit()

                            separator()

                            login()

                        elif lenght > 20:

                            separator()

                            print(
                                "" Password Is  Greater  than  20  Characters  Enter Again""
                            )

                            separator()

                            user_name = input(""ENTER  USER  NAME -- "")
                            passw = input(""ENTER  PASSWORD AGAIN (limit 8-20) -- "")
                            lenght = len(passw)

                            if lenght >= 8 and lenght >= 20:

                                c = c + 1
                                insert55 = (c, user_name, passw)
                                insert22 = ""insert into login values(%s,%s,%s)""
                                mycur.execute(insert22, insert55)
                                mycon.commit()

                                separator()

                                login()

        update33 = ""update visit set visits=%s"" % (c)
        mycur.execute(update33)
        mycon.commit()

    elif m[0] == 1:

        if m[0] == 1:

            login()


separator()


DBZ()


# REPETITION


a = True


while a == True:

    # PROGRAM STARTED

    print(""     *TO VIEW ALL ENTER 1"")
    print(""     *TO SEARCH and BUY BOOK ENTER 2"")
    print(""     *TO ADD BOOK ENTER 3"")
    print(""     *TO UPDATE ENTER 4"")
    print(""     *TO DELETE BOOK ENTER 5"")
    print(""     *TO CLOSE ENTER 6"")

    print()

    choice = int(input(""ENTER YOUR CHOICE -- ""))

    separator()

    # VIEW

    if choice == 1:

        print()

        ViewAll()

        separator()

        rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

        if rep == ""yes"":

            end_separator()

            separator()

            DBZ()

            continue

        else:

            end_separator()

            DBZ()

            os._exit(0)

        end_separator()

    # SEARCH / BUY

    if choice == 2:

        book_name = input(""ENTER BOOK NAME ---- "")

        separator()

        display = ""select * from books where Name='%s'"" % (book_name)
        mycur.execute(display)
        data2 = mycur.fetchone()

        if data2 != None:

            print(""BOOK IS AVAILABLE"")

            # BUY OR NOT

            separator()

            print(""\t*WANT TO BUY PRESS 1"")
            print(""\t*IF NOT PRESS 2"")
            print()

            choice2 = int(input(""ENTER YOUR CHOICE -- ""))

            if choice2 == 1:

                # BUY 1 OR MORE

                separator()

                print(""\t*IF YOU WANT ONE BOOK PRESS 1"")
                print(""\t*IF YOU WANT MORE THAN ONE BOOK PRESS 2"")
                print()

                choice3 = int(input(""ENTER YOUR CHOICE -- ""))

                if choice3 == 1:

                    display = ""select * from books""
                    mycur.execute(display)
                    data2 = mycur.fetchall()

                    for y in data2:

                        if y[1] == book_name:

                            if y[6] > 0:

                                separator()

                                u = (
                                    ""update books set Numbers_of_book=Numbers_of_book - 1 where name='%s';""
                                    % (book_name)
                                )
                                mycur.execute(u)
                                mycon.commit()

                                print(""BOOK WAS BOUGHT"")

                                separator()

                                print(""THANKS FOR COMING"")

                                CNB1()

                                separator()

                                rep = input(
                                    ""Do  You  Want  To  Restart  ??    yes / no  --  ""
                                ).lower()

                                if rep == ""yes"":

                                    end_separator()

                                    separator()

                                    DBZ()

                                    continue

                                else:

                                    end_separator()

                                    DBZ()

                                    os._exit(0)

                if choice3 == 2:

                    separator()

                    wb = int(input(""ENTER NO. OF BOOKS -- ""))

                    separator()

                    display = ""select * from books""
                    mycur.execute(display)
                    data2 = mycur.fetchall()

                    for y in data2:

                        if y[1] == book_name:

                            if wb > y[6]:

                                if y[6] > 0:

                                    print(""YOU CAN'T  BUT  THAT  MUCH  BOOKS"")

                                    separator()

                                    print(""BUT YOU CAN BUY"", y[6], ""BOOKS MAX"")

                                    separator()

                                    choice44 = input(
                                        ""DO YOU WANT TO BUY BOOK ?     Y/N -- ""
                                    )

                                    separator()

                                    k = y[6]

                                    if choice44 == ""y"" or choice44 == ""Y"":

                                        u2 = (
                                            ""update books set numbers_of_book=numbers_of_book -%s where name='%s'""
                                            % (k, book_name)
                                        )
                                        mycur.execute(u2)
                                        mycon.commit()

                                        print(""BOOK WAS BOUGHT"")

                                        separator()

                                        print(""THANKS FOR COMING"")

                                        separator()

                                        display = ""select * from books""
                                        mycur.execute(display)
                                        data2 = mycur.fetchall()

                                        for y in data2:

                                            if y[1] == book_name:

                                                if y[6] <= 8:

                                                    print(
                                                        ""WARNING!!!!!!!!!!!!!!!!!!!!!!!""
                                                    )
                                                    print(
                                                        ""NO.  OF THIS BOOK IS LOW"",
                                                        ""\tONLY"",
                                                        y[6],
                                                        ""LEFT"",
                                                    )

                                                    end_separator()

                                                    break

                                        separator()

                                        rep = input(
                                            ""Do  You  Want  To  Restart  ??    yes / no  --  ""
                                        ).lower()

                                        if rep == ""yes"":

                                            end_separator()

                                            separator()

                                            DBZ()

                                            continue

                                        else:

                                            end_separator()

                                            DBZ()

                                            os._exit(0)

                                    elif choice44 == ""n"" or choice44 == ""N"":

                                        print(
                                            ""SORRY  FOR  INCONVENIENCE  WE  WILL  TRY  TO  FULLFILL  YOUR  REQUIREMENT  AS  SOON  AS  POSSIBLE""
                                        )

                                        end_separator()

                                        separator()

                                        rep = input(
                                            ""Do  You  Want  To  Restart  ??    yes / no  --  ""
                                        ).lower()

                                        if rep == ""yes"":

                                            separator()

                                            DBZ()

                                            continue

                                        else:

                                            end_separator()

                                            DBZ()

                                            os._exit(0)

                                elif y[6] == 0:

                                    print(
                                        ""SORRY  NO  BOOK  LEFT  WE  WILL  TRY  TO  FULLFILL  YOUR  REQUIREMENT  AS  SOON  AS  POSSIBLE""
                                    )

                                    end_separator()

                                    separator()

                                    rep = input(
                                        ""Do  You  Want  To  Restart  ??    yes / no  --  ""
                                    ).lower()

                                    if rep == ""yes"":

                                        separator()

                                        DBZ()

                                        continue

                                    else:

                                        end_separator()

                                        DBZ()

                                        os._exit(0)

                            else:

                                u2 = (
                                    ""update books set numbers_of_book=numbers_of_book -%s where name='%s'""
                                    % (wb, book_name)
                                )
                                mycur.execute(u2)
                                mycon.commit()

                                print(""BOOK WAS BOUGHT"")

                                separator()

                                print(""THANKS FOR COMING"")

                                display = ""select * from books""
                                mycur.execute(display)
                                data2 = mycur.fetchall()

                                for y in data2:

                                    if y[1] == book_name:

                                        CNB2()

                                        separator()

                                        rep = input(
                                            ""Do  You  Want  To  Restart  ??    yes / no  --  ""
                                        ).lower()

                                        if rep == ""yes"":

                                            separator()

                                            DBZ()

                                            continue

                                        else:

                                            end_separator()

                                            DBZ()

                                            os._exit(0)

            else:

                separator()

                print(""NO BOOK IS BOUGHT"")

                end_separator()

                separator()

                rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

                if rep == ""yes"":

                    separator()

                    DBZ()

                    continue

                else:

                    end_separator()

                    DBZ()

                    os._exit(0)

        else:

            separator()

            print(""SORRY NO BOOK WITH THIS NAME EXIST / NAME IS INCORRECT"")

            end_separator()

            separator()

            rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

            if rep == ""yes"":

                separator()

                DBZ()

                continue

            else:

                end_separator()

                DBZ()

                os._exit(0)

    # ADDING BOOK

    if choice == 3:

        q10 = int(input(""ENTER NO. OF BOOKS TO ADD -- ""))

        separator()

        for k in range(q10):

            SNo10 = int(input(""ENTER SNo OF BOOK -- ""))
            name10 = input(""ENTER NAME OF BOOK --- "")
            author10 = input(""ENTER NAME OF AUTHOR -- "")
            year10 = int(input(""ENTER YEAR OF PUBLISHING -- ""))
            ISBN10 = input(""ENTER ISBN OF BOOK -- "")
            price10 = int(input(""ENTER PRICE OF BOOK -- ""))
            nob10 = int(input(""ENTER NO. OF BOOKS -- ""))

            display10 = ""select * from books where ISBN='%s'"" % (ISBN10)
            mycur.execute(display10)
            data20 = mycur.fetchone()

            if data20 != None:

                print(""This  ISBN Already Exists"")

                os._exit(0)

            else:

                insert = (SNo10, name10, author10, year10, ISBN10, price10, nob10)
                insert20 = ""insert into books values(%s,%s,%s,%s,%s,%s,%s)""
                mycur.execute(insert20, insert)
                mycon.commit()

                separator()

                print(""BOOK IS ADDED"")

                separator()

        rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

        if rep == ""yes"":

            separator()

            DBZ()

            continue

        else:

            end_separator()

            DBZ()

            os._exit(0)

    # UPDATING BOOK

    if choice == 4:

        choice4 = input(""ENTER ISBN OF BOOK -- "")

        separator()

        display = ""select * from books where ISBN='%s'"" % (choice4)
        mycur.execute(display)
        data2 = mycur.fetchone()

        if data2 != None:

            SNo1 = int(input(""ENTER NEW SNo OF BOOK -- ""))
            name1 = input(""ENTER NEW NAME OF BOOK --- "")
            author1 = input(""ENTER NEW NAME OF AUTHOR -- "")
            year1 = int(input(""ENTER NEW YEAR OF PUBLISHING -- ""))
            ISBN1 = input(""ENTER NEW ISBN OF BOOK -- "")
            price1 = int(input(""ENTER NEW PRICE OF BOOK -- ""))
            nob = int(input(""ENTER NEW NO. OF BOOKS -- ""))
            insert = (SNo1, name1, author1, year1, ISBN1, price1, nob, choice4)
            update = ""update books set SNo=%s,Name=%s,Author=%s,Year=%s,ISBN=%s,Price=%s,numbers_of_book=%s where ISBN=%s""
            mycur.execute(update, insert)
            mycon.commit()

            separator()

            print(""BOOK IS UPDATED"")

            separator()

            rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

            if rep == ""yes"":

                separator()

                DBZ()

                continue

            else:

                end_separator()

                DBZ()

                os._exit(0)

        else:

            print(""SORRY NO BOOK WITH THIS ISBN IS EXIST  /  INCORRECT ISBN"")

            print()
            print()

            separator()

            rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

            if rep == ""yes"":

                separator()

                DBZ()

                continue

            else:

                end_separator()

                DBZ()

                os._exit(0)

    # DELETING A BOOK

    if choice == 5:

        ISBN1 = input(""ENTER ISBN OF THAT BOOK THAT YOU WANT TO DELETE -- "")
        display = ""select * from books where ISBN='%s'"" % (ISBN1)
        mycur.execute(display)
        data2 = mycur.fetchone()

        if data2 != None:

            separator()

            choice5 = input(""ARE YOU SURE TO DELETE THIS BOOK ENTER Y/N -- "")

            if choice5 == ""Y"" or choice5 == ""y"":

                separator()

                ISBN2 = input(""PLEASE ENTER ISBN AGAIN -- "")
                delete = ""delete from books where ISBN='%s'"" % (ISBN2)
                mycur.execute(delete)
                mycon.commit()

                separator()

                print(""BOOK IS DELETED"")

                print()
                print()

                separator()

                rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

                if rep == ""yes"":

                    separator()

                    DBZ()

                    continue

                else:

                    end_separator()

                    DBZ()

                    os._exit(0)

            else:

                separator()

                print(""NO BOOK IS DELETED"")

                print()
                print()

                separator()

                rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

                if rep == ""yes"":

                    separator()

                    DBZ()

                    continue

                else:

                    end_separator()

                    DBZ()

                    os._exit(0)

        else:

            separator()

            print(""SORRY NO BOOK WITH THIS ISBN AVAILABLE / ISBN IS INCORRECT"")

            print()
            print()

            separator()

            rep = input(""Do  You  Want  To  Restart  ??    yes / no  --  "").lower()

            if rep == ""yes"":

                separator()

                DBZ()

                continue

            else:

                end_separator()

                DBZ()

                os._exit(0)

    # CLOSE

    if choice == 6:

        exit()
        os._exit(0)


# IF  NO.  OF  BOOKS  IS     ZERO(  0  )     THAN  DELETE  IT  AUTOMATICALLY


display = ""select * from books""
mycur.execute(display)
data2 = mycur.fetchall()


for y in data2:

    if y[6] <= 0:

        delete = ""delete from books where  Numbers_of_book<=0""
        mycur.execute(delete)
        mycon.commit()
"
unclecode_crawl4ai,unclecode_crawl4ai_main.py,18467,https://raw.githubusercontent.com/unclecode/crawl4ai/main/main.py,"import asyncio, os
from fastapi import FastAPI, HTTPException
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.templating import Jinja2Templates
from fastapi.responses import RedirectResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi import Depends, Security

from pydantic import BaseModel, HttpUrl, Field
from typing import Optional, List, Dict, Any, Union
import psutil
import time
import uuid
import math
import logging
from enum import Enum
from dataclasses import dataclass
from crawl4ai import AsyncWebCrawler, CrawlResult, CacheMode
from crawl4ai.config import MIN_WORD_THRESHOLD
from crawl4ai.extraction_strategy import (
    LLMExtractionStrategy,
    CosineStrategy,
    JsonCssExtractionStrategy,
)

__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class TaskStatus(str, Enum):
    PENDING = ""pending""
    PROCESSING = ""processing""
    COMPLETED = ""completed""
    FAILED = ""failed""


class CrawlerType(str, Enum):
    BASIC = ""basic""
    LLM = ""llm""
    COSINE = ""cosine""
    JSON_CSS = ""json_css""


class ExtractionConfig(BaseModel):
    type: CrawlerType
    params: Dict[str, Any] = {}


class ChunkingStrategy(BaseModel):
    type: str
    params: Dict[str, Any] = {}


class ContentFilter(BaseModel):
    type: str = ""bm25""
    params: Dict[str, Any] = {}


class CrawlRequest(BaseModel):
    urls: Union[HttpUrl, List[HttpUrl]]
    word_count_threshold: int = MIN_WORD_THRESHOLD
    extraction_config: Optional[ExtractionConfig] = None
    chunking_strategy: Optional[ChunkingStrategy] = None
    content_filter: Optional[ContentFilter] = None
    js_code: Optional[List[str]] = None
    wait_for: Optional[str] = None
    css_selector: Optional[str] = None
    screenshot: bool = False
    magic: bool = False
    extra: Optional[Dict[str, Any]] = {}
    session_id: Optional[str] = None
    cache_mode: Optional[CacheMode] = CacheMode.ENABLED
    priority: int = Field(default=5, ge=1, le=10)
    ttl: Optional[int] = 3600
    crawler_params: Dict[str, Any] = {}


@dataclass
class TaskInfo:
    id: str
    status: TaskStatus
    result: Optional[Union[CrawlResult, List[CrawlResult]]] = None
    error: Optional[str] = None
    created_at: float = time.time()
    ttl: int = 3600


class ResourceMonitor:
    def __init__(self, max_concurrent_tasks: int = 10):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.memory_threshold = 0.85
        self.cpu_threshold = 0.90
        self._last_check = 0
        self._check_interval = 1  # seconds
        self._last_available_slots = max_concurrent_tasks

    async def get_available_slots(self) -> int:
        current_time = time.time()
        if current_time - self._last_check < self._check_interval:
            return self._last_available_slots

        mem_usage = psutil.virtual_memory().percent / 100
        cpu_usage = psutil.cpu_percent() / 100

        memory_factor = max(
            0, (self.memory_threshold - mem_usage) / self.memory_threshold
        )
        cpu_factor = max(0, (self.cpu_threshold - cpu_usage) / self.cpu_threshold)

        self._last_available_slots = math.floor(
            self.max_concurrent_tasks * min(memory_factor, cpu_factor)
        )
        self._last_check = current_time

        return self._last_available_slots


class TaskManager:
    def __init__(self, cleanup_interval: int = 300):
        self.tasks: Dict[str, TaskInfo] = {}
        self.high_priority = asyncio.PriorityQueue()
        self.low_priority = asyncio.PriorityQueue()
        self.cleanup_interval = cleanup_interval
        self.cleanup_task = None

    async def start(self):
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())

    async def stop(self):
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass

    async def add_task(self, task_id: str, priority: int, ttl: int) -> None:
        task_info = TaskInfo(id=task_id, status=TaskStatus.PENDING, ttl=ttl)
        self.tasks[task_id] = task_info
        queue = self.high_priority if priority > 5 else self.low_priority
        await queue.put((-priority, task_id))  # Negative for proper priority ordering

    async def get_next_task(self) -> Optional[str]:
        try:
            # Try high priority first
            _, task_id = await asyncio.wait_for(self.high_priority.get(), timeout=0.1)
            return task_id
        except asyncio.TimeoutError:
            try:
                # Then try low priority
                _, task_id = await asyncio.wait_for(
                    self.low_priority.get(), timeout=0.1
                )
                return task_id
            except asyncio.TimeoutError:
                return None

    def update_task(
        self, task_id: str, status: TaskStatus, result: Any = None, error: str = None
    ):
        if task_id in self.tasks:
            task_info = self.tasks[task_id]
            task_info.status = status
            task_info.result = result
            task_info.error = error

    def get_task(self, task_id: str) -> Optional[TaskInfo]:
        return self.tasks.get(task_id)

    async def _cleanup_loop(self):
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                current_time = time.time()
                expired_tasks = [
                    task_id
                    for task_id, task in self.tasks.items()
                    if current_time - task.created_at > task.ttl
                    and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]
                ]
                for task_id in expired_tasks:
                    del self.tasks[task_id]
            except Exception as e:
                logger.error(f""Error in cleanup loop: {e}"")


class CrawlerPool:
    def __init__(self, max_size: int = 10):
        self.max_size = max_size
        self.active_crawlers: Dict[AsyncWebCrawler, float] = {}
        self._lock = asyncio.Lock()

    async def acquire(self, **kwargs) -> AsyncWebCrawler:
        async with self._lock:
            # Clean up inactive crawlers
            current_time = time.time()
            inactive = [
                crawler
                for crawler, last_used in self.active_crawlers.items()
                if current_time - last_used > 600  # 10 minutes timeout
            ]
            for crawler in inactive:
                await crawler.__aexit__(None, None, None)
                del self.active_crawlers[crawler]

            # Create new crawler if needed
            if len(self.active_crawlers) < self.max_size:
                crawler = AsyncWebCrawler(**kwargs)
                await crawler.__aenter__()
                self.active_crawlers[crawler] = current_time
                return crawler

            # Reuse least recently used crawler
            crawler = min(self.active_crawlers.items(), key=lambda x: x[1])[0]
            self.active_crawlers[crawler] = current_time
            return crawler

    async def release(self, crawler: AsyncWebCrawler):
        async with self._lock:
            if crawler in self.active_crawlers:
                self.active_crawlers[crawler] = time.time()

    async def cleanup(self):
        async with self._lock:
            for crawler in list(self.active_crawlers.keys()):
                await crawler.__aexit__(None, None, None)
            self.active_crawlers.clear()


class CrawlerService:
    def __init__(self, max_concurrent_tasks: int = 10):
        self.resource_monitor = ResourceMonitor(max_concurrent_tasks)
        self.task_manager = TaskManager()
        self.crawler_pool = CrawlerPool(max_concurrent_tasks)
        self._processing_task = None

    async def start(self):
        await self.task_manager.start()
        self._processing_task = asyncio.create_task(self._process_queue())

    async def stop(self):
        if self._processing_task:
            self._processing_task.cancel()
            try:
                await self._processing_task
            except asyncio.CancelledError:
                pass
        await self.task_manager.stop()
        await self.crawler_pool.cleanup()

    def _create_extraction_strategy(self, config: ExtractionConfig):
        if not config:
            return None

        if config.type == CrawlerType.LLM:
            return LLMExtractionStrategy(**config.params)
        elif config.type == CrawlerType.COSINE:
            return CosineStrategy(**config.params)
        elif config.type == CrawlerType.JSON_CSS:
            return JsonCssExtractionStrategy(**config.params)
        return None

    async def submit_task(self, request: CrawlRequest) -> str:
        task_id = str(uuid.uuid4())
        await self.task_manager.add_task(task_id, request.priority, request.ttl or 3600)

        # Store request data with task
        self.task_manager.tasks[task_id].request = request

        return task_id

    async def _process_queue(self):
        while True:
            try:
                available_slots = await self.resource_monitor.get_available_slots()
                if False and available_slots <= 0:
                    await asyncio.sleep(1)
                    continue

                task_id = await self.task_manager.get_next_task()
                if not task_id:
                    await asyncio.sleep(1)
                    continue

                task_info = self.task_manager.get_task(task_id)
                if not task_info:
                    continue

                request = task_info.request
                self.task_manager.update_task(task_id, TaskStatus.PROCESSING)

                try:
                    crawler = await self.crawler_pool.acquire(**request.crawler_params)

                    extraction_strategy = self._create_extraction_strategy(
                        request.extraction_config
                    )

                    if isinstance(request.urls, list):
                        results = await crawler.arun_many(
                            urls=[str(url) for url in request.urls],
                            word_count_threshold=MIN_WORD_THRESHOLD,
                            extraction_strategy=extraction_strategy,
                            js_code=request.js_code,
                            wait_for=request.wait_for,
                            css_selector=request.css_selector,
                            screenshot=request.screenshot,
                            magic=request.magic,
                            session_id=request.session_id,
                            cache_mode=request.cache_mode,
                            **request.extra,
                        )
                    else:
                        results = await crawler.arun(
                            url=str(request.urls),
                            extraction_strategy=extraction_strategy,
                            js_code=request.js_code,
                            wait_for=request.wait_for,
                            css_selector=request.css_selector,
                            screenshot=request.screenshot,
                            magic=request.magic,
                            session_id=request.session_id,
                            cache_mode=request.cache_mode,
                            **request.extra,
                        )

                    await self.crawler_pool.release(crawler)
                    self.task_manager.update_task(
                        task_id, TaskStatus.COMPLETED, results
                    )

                except Exception as e:
                    logger.error(f""Error processing task {task_id}: {str(e)}"")
                    self.task_manager.update_task(
                        task_id, TaskStatus.FAILED, error=str(e)
                    )

            except Exception as e:
                logger.error(f""Error in queue processing: {str(e)}"")
                await asyncio.sleep(1)


app = FastAPI(title=""Crawl4AI API"")

# CORS configuration
origins = [""*""]  # Allow all origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # List of origins that are allowed to make requests
    allow_credentials=True,
    allow_methods=[""*""],  # Allows all methods
    allow_headers=[""*""],  # Allows all headers
)

# API token security
security = HTTPBearer()
CRAWL4AI_API_TOKEN = os.getenv(""CRAWL4AI_API_TOKEN"")


async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    if not CRAWL4AI_API_TOKEN:
        return credentials  # No token verification if CRAWL4AI_API_TOKEN is not set
    if credentials.credentials != CRAWL4AI_API_TOKEN:
        raise HTTPException(status_code=401, detail=""Invalid token"")
    return credentials


def secure_endpoint():
    """"""Returns security dependency only if CRAWL4AI_API_TOKEN is set""""""
    return Depends(verify_token) if CRAWL4AI_API_TOKEN else None


# Check if site directory exists
if os.path.exists(__location__ + ""/site""):
    # Mount the site directory as a static directory
    app.mount(""/mkdocs"", StaticFiles(directory=""site"", html=True), name=""mkdocs"")

site_templates = Jinja2Templates(directory=__location__ + ""/site"")

crawler_service = CrawlerService()


@app.on_event(""startup"")
async def startup_event():
    await crawler_service.start()


@app.on_event(""shutdown"")
async def shutdown_event():
    await crawler_service.stop()


@app.get(""/"")
def read_root():
    if os.path.exists(__location__ + ""/site""):
        return RedirectResponse(url=""/mkdocs"")
    # Return a json response
    return {""message"": ""Crawl4AI API service is running""}


@app.post(""/crawl"", dependencies=[secure_endpoint()] if CRAWL4AI_API_TOKEN else [])
async def crawl(request: CrawlRequest) -> Dict[str, str]:
    task_id = await crawler_service.submit_task(request)
    return {""task_id"": task_id}


@app.get(
    ""/task/{task_id}"", dependencies=[secure_endpoint()] if CRAWL4AI_API_TOKEN else []
)
async def get_task_status(task_id: str):
    task_info = crawler_service.task_manager.get_task(task_id)
    if not task_info:
        raise HTTPException(status_code=404, detail=""Task not found"")

    response = {
        ""status"": task_info.status,
        ""created_at"": task_info.created_at,
    }

    if task_info.status == TaskStatus.COMPLETED:
        # Convert CrawlResult to dict for JSON response
        if isinstance(task_info.result, list):
            response[""results""] = [result.dict() for result in task_info.result]
        else:
            response[""result""] = task_info.result.dict()
    elif task_info.status == TaskStatus.FAILED:
        response[""error""] = task_info.error

    return response


@app.post(""/crawl_sync"", dependencies=[secure_endpoint()] if CRAWL4AI_API_TOKEN else [])
async def crawl_sync(request: CrawlRequest) -> Dict[str, Any]:
    task_id = await crawler_service.submit_task(request)

    # Wait up to 60 seconds for task completion
    for _ in range(60):
        task_info = crawler_service.task_manager.get_task(task_id)
        if not task_info:
            raise HTTPException(status_code=404, detail=""Task not found"")

        if task_info.status == TaskStatus.COMPLETED:
            # Return same format as /task/{task_id} endpoint
            if isinstance(task_info.result, list):
                return {
                    ""status"": task_info.status,
                    ""results"": [result.dict() for result in task_info.result],
                }
            return {""status"": task_info.status, ""result"": task_info.result.dict()}

        if task_info.status == TaskStatus.FAILED:
            raise HTTPException(status_code=500, detail=task_info.error)

        await asyncio.sleep(1)

    # If we get here, task didn't complete within timeout
    raise HTTPException(status_code=408, detail=""Task timed out"")


@app.post(
    ""/crawl_direct"", dependencies=[secure_endpoint()] if CRAWL4AI_API_TOKEN else []
)
async def crawl_direct(request: CrawlRequest) -> Dict[str, Any]:
    try:
        crawler = await crawler_service.crawler_pool.acquire(**request.crawler_params)
        extraction_strategy = crawler_service._create_extraction_strategy(
            request.extraction_config
        )

        try:
            if isinstance(request.urls, list):
                results = await crawler.arun_many(
                    urls=[str(url) for url in request.urls],
                    extraction_strategy=extraction_strategy,
                    js_code=request.js_code,
                    wait_for=request.wait_for,
                    css_selector=request.css_selector,
                    screenshot=request.screenshot,
                    magic=request.magic,
                    cache_mode=request.cache_mode,
                    session_id=request.session_id,
                    **request.extra,
                )
                return {""results"": [result.dict() for result in results]}
            else:
                result = await crawler.arun(
                    url=str(request.urls),
                    extraction_strategy=extraction_strategy,
                    js_code=request.js_code,
                    wait_for=request.wait_for,
                    css_selector=request.css_selector,
                    screenshot=request.screenshot,
                    magic=request.magic,
                    cache_mode=request.cache_mode,
                    session_id=request.session_id,
                    **request.extra,
                )
                return {""result"": result.dict()}
        finally:
            await crawler_service.crawler_pool.release(crawler)
    except Exception as e:
        logger.error(f""Error in direct crawl: {str(e)}"")
        raise HTTPException(status_code=500, detail=str(e))


@app.get(""/health"")
async def health_check():
    available_slots = await crawler_service.resource_monitor.get_available_slots()
    memory = psutil.virtual_memory()
    return {
        ""status"": ""healthy"",
        ""available_slots"": available_slots,
        ""memory_usage"": memory.percent,
        ""cpu_usage"": psutil.cpu_percent(),
    }


if __name__ == ""__main__"":
    import uvicorn

    uvicorn.run(app, host=""0.0.0.0"", port=11235)
"
huggingface_pytorch-image-models,huggingface_pytorch-image-models_inference.py,16697,https://raw.githubusercontent.com/huggingface/pytorch-image-models/main/inference.py,"#!/usr/bin/env python3
""""""PyTorch Inference Script

An example inference script that outputs top-k class ids for images in a folder into a csv.

Hacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)
""""""
import argparse
import json
import logging
import os
import time
from contextlib import suppress
from functools import partial
from sys import maxsize

import numpy as np
import pandas as pd
import torch

from timm.data import create_dataset, create_loader, resolve_data_config, ImageNetInfo, infer_imagenet_subset
from timm.layers import apply_test_time_pool
from timm.models import create_model
from timm.utils import AverageMeter, setup_default_logging, set_jit_fuser, ParseKwargs

try:
    from apex import amp
    has_apex = True
except ImportError:
    has_apex = False

try:
    from functorch.compile import memory_efficient_fusion
    has_functorch = True
except ImportError as e:
    has_functorch = False

has_compile = hasattr(torch, 'compile')


_FMT_EXT = {
    'json': '.json',
    'json-record': '.json',
    'json-split': '.json',
    'parquet': '.parquet',
    'csv': '.csv',
}

torch.backends.cudnn.benchmark = True
_logger = logging.getLogger('inference')


parser = argparse.ArgumentParser(description='PyTorch ImageNet Inference')
parser.add_argument('data', nargs='?', metavar='DIR', const=None,
                    help='path to dataset (*deprecated*, use --data-dir)')
parser.add_argument('--data-dir', metavar='DIR',
                    help='path to dataset (root dir)')
parser.add_argument('--dataset', metavar='NAME', default='',
                    help='dataset type + name (""<type>/<name>"") (default: ImageFolder or ImageTar if empty)')
parser.add_argument('--split', metavar='NAME', default='validation',
                    help='dataset split (default: validation)')
parser.add_argument('--model', '-m', metavar='MODEL', default='resnet50',
                    help='model architecture (default: resnet50)')
parser.add_argument('-j', '--workers', default=2, type=int, metavar='N',
                    help='number of data loading workers (default: 2)')
parser.add_argument('-b', '--batch-size', default=256, type=int,
                    metavar='N', help='mini-batch size (default: 256)')
parser.add_argument('--img-size', default=None, type=int,
                    metavar='N', help='Input image dimension, uses model default if empty')
parser.add_argument('--in-chans', type=int, default=None, metavar='N',
                    help='Image input channels (default: None => 3)')
parser.add_argument('--input-size', default=None, nargs=3, type=int, metavar='N',
                    help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')
parser.add_argument('--use-train-size', action='store_true', default=False,
                    help='force use of train input size, even when test size is specified in pretrained cfg')
parser.add_argument('--crop-pct', default=None, type=float,
                    metavar='N', help='Input image center crop pct')
parser.add_argument('--crop-mode', default=None, type=str,
                    metavar='N', help='Input image crop mode (squash, border, center). Model default if None.')
parser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',
                    help='Override mean pixel value of dataset')
parser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',
                    help='Override std deviation of of dataset')
parser.add_argument('--interpolation', default='', type=str, metavar='NAME',
                    help='Image resize interpolation type (overrides model)')
parser.add_argument('--num-classes', type=int, default=None,
                    help='Number classes in dataset')
parser.add_argument('--class-map', default='', type=str, metavar='FILENAME',
                    help='path to class to idx mapping file (default: """")')
parser.add_argument('--log-freq', default=10, type=int,
                    metavar='N', help='batch logging frequency (default: 10)')
parser.add_argument('--checkpoint', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--pretrained', dest='pretrained', action='store_true',
                    help='use pre-trained model')
parser.add_argument('--num-gpu', type=int, default=1,
                    help='Number of GPUS to use')
parser.add_argument('--test-pool', dest='test_pool', action='store_true',
                    help='enable test time pool')
parser.add_argument('--channels-last', action='store_true', default=False,
                    help='Use channels_last memory layout')
parser.add_argument('--device', default='cuda', type=str,
                    help=""Device (accelerator) to use."")
parser.add_argument('--amp', action='store_true', default=False,
                    help='use Native AMP for mixed precision training')
parser.add_argument('--amp-dtype', default='float16', type=str,
                    help='lower precision AMP dtype (default: float16)')
parser.add_argument('--model-dtype', default=None, type=str,
                   help='Model dtype override (non-AMP) (default: float32)')
parser.add_argument('--fuser', default='', type=str,
                    help=""Select jit fuser. One of ('', 'te', 'old', 'nvfuser')"")
parser.add_argument('--model-kwargs', nargs='*', default={}, action=ParseKwargs)
parser.add_argument('--torchcompile-mode', type=str, default=None,
                    help=""torch.compile mode (default: None)."")

scripting_group = parser.add_mutually_exclusive_group()
scripting_group.add_argument('--torchscript', default=False, action='store_true',
                             help='torch.jit.script the full model')
scripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',
                             help=""Enable compilation w/ specified backend (default: inductor)."")
scripting_group.add_argument('--aot-autograd', default=False, action='store_true',
                             help=""Enable AOT Autograd support."")

parser.add_argument('--results-dir', type=str, default=None,
                    help='folder for output results')
parser.add_argument('--results-file', type=str, default=None,
                    help='results filename (relative to results-dir)')
parser.add_argument('--results-format', type=str, nargs='+', default=['csv'],
                    help='results format (one of ""csv"", ""json"", ""json-split"", ""parquet"")')
parser.add_argument('--results-separate-col', action='store_true', default=False,
                    help='separate output columns per result index.')
parser.add_argument('--topk', default=1, type=int,
                    metavar='N', help='Top-k to output to CSV')
parser.add_argument('--fullname', action='store_true', default=False,
                    help='use full sample name in output (not just basename).')
parser.add_argument('--filename-col', type=str, default='filename',
                    help='name for filename / sample name column')
parser.add_argument('--index-col', type=str, default='index',
                    help='name for output indices column(s)')
parser.add_argument('--label-col', type=str, default='label',
                    help='name for output indices column(s)')
parser.add_argument('--output-col', type=str, default=None,
                    help='name for logit/probs output column(s)')
parser.add_argument('--output-type', type=str, default='prob',
                    help='output type colum (""prob"" for probabilities, ""logit"" for raw logits)')
parser.add_argument('--label-type', type=str, default='description',
                    help='type of label to output, one of  ""none"", ""name"", ""description"", ""detailed""')
parser.add_argument('--include-index', action='store_true', default=False,
                    help='include the class index in results')
parser.add_argument('--exclude-output', action='store_true', default=False,
                    help='exclude logits/probs from results, just indices. topk must be set !=0.')
parser.add_argument('--no-console-results', action='store_true', default=False,
                    help='disable printing the inference results to the console')


def main():
    setup_default_logging()
    args = parser.parse_args()
    # might as well try to do something useful...
    args.pretrained = args.pretrained or not args.checkpoint

    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

    device = torch.device(args.device)

    model_dtype = None
    if args.model_dtype:
        assert args.model_dtype in ('float32', 'float16', 'bfloat16')
        model_dtype = getattr(torch, args.model_dtype)

    # resolve AMP arguments based on PyTorch / Apex availability
    amp_autocast = suppress
    if args.amp:
        assert model_dtype is None or model_dtype == torch.float32, 'float32 model dtype must be used with AMP'
        assert args.amp_dtype in ('float16', 'bfloat16')
        amp_dtype = torch.bfloat16 if args.amp_dtype == 'bfloat16' else torch.float16
        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)
        _logger.info('Running inference in mixed precision with native PyTorch AMP.')
    else:
        _logger.info('Running inference in float32. AMP not enabled.')

    if args.fuser:
        set_jit_fuser(args.fuser)

    # create model
    in_chans = 3
    if args.in_chans is not None:
        in_chans = args.in_chans
    elif args.input_size is not None:
        in_chans = args.input_size[0]

    model = create_model(
        args.model,
        num_classes=args.num_classes,
        in_chans=in_chans,
        pretrained=args.pretrained,
        checkpoint_path=args.checkpoint,
        **args.model_kwargs,
    )
    if args.num_classes is None:
        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'
        args.num_classes = model.num_classes

    _logger.info(
        f'Model {args.model} created, param count: {sum([m.numel() for m in model.parameters()])}')

    data_config = resolve_data_config(vars(args), model=model)
    test_time_pool = False
    if args.test_pool:
        model, test_time_pool = apply_test_time_pool(model, data_config)

    model = model.to(device=device, dtype=model_dtype)
    model.eval()
    if args.channels_last:
        model = model.to(memory_format=torch.channels_last)

    if args.torchscript:
        model = torch.jit.script(model)
    elif args.torchcompile:
        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'
        torch._dynamo.reset()
        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)
    elif args.aot_autograd:
        assert has_functorch, ""functorch is needed for --aot-autograd""
        model = memory_efficient_fusion(model)

    if args.num_gpu > 1:
        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))

    root_dir = args.data or args.data_dir
    dataset = create_dataset(
        root=root_dir,
        name=args.dataset,
        split=args.split,
        class_map=args.class_map,
    )

    if test_time_pool:
        data_config['crop_pct'] = 1.0

    workers = 1 if 'tfds' in args.dataset or 'wds' in args.dataset else args.workers
    loader = create_loader(
        dataset,
        batch_size=args.batch_size,
        use_prefetcher=True,
        num_workers=workers,
        device=device,
        img_dtype=model_dtype or torch.float32,
        **data_config,
    )

    to_label = None
    if args.label_type in ('name', 'description', 'detail'):
        imagenet_subset = infer_imagenet_subset(model)
        if imagenet_subset is not None:
            dataset_info = ImageNetInfo(imagenet_subset)
            if args.label_type == 'name':
                to_label = lambda x: dataset_info.index_to_label_name(x)
            elif args.label_type == 'detail':
                to_label = lambda x: dataset_info.index_to_description(x, detailed=True)
            else:
                to_label = lambda x: dataset_info.index_to_description(x)
            to_label = np.vectorize(to_label)
        else:
            _logger.error(""Cannot deduce ImageNet subset from model, no labelling will be performed."")

    top_k = min(args.topk, args.num_classes)
    batch_time = AverageMeter()
    end = time.time()
    all_indices = []
    all_labels = []
    all_outputs = []
    use_probs = args.output_type == 'prob'
    with torch.no_grad():
        for batch_idx, (input, _) in enumerate(loader):

            with amp_autocast():
                output = model(input)

            if use_probs:
                output = output.softmax(-1)

            if top_k:
                output, indices = output.topk(top_k)
                np_indices = indices.cpu().numpy()
                if args.include_index:
                    all_indices.append(np_indices)
                if to_label is not None:
                    np_labels = to_label(np_indices)
                    all_labels.append(np_labels)

            all_outputs.append(output.float().cpu().numpy())

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if batch_idx % args.log_freq == 0:
                _logger.info('Predict: [{0}/{1}] Time {batch_time.val:.3f} ({batch_time.avg:.3f})'.format(
                    batch_idx, len(loader), batch_time=batch_time))

    all_indices = np.concatenate(all_indices, axis=0) if all_indices else None
    all_labels = np.concatenate(all_labels, axis=0) if all_labels else None
    all_outputs = np.concatenate(all_outputs, axis=0).astype(np.float32)
    filenames = loader.dataset.filenames(basename=not args.fullname)

    output_col = args.output_col or ('prob' if use_probs else 'logit')
    data_dict = {args.filename_col: filenames}
    if args.results_separate_col and all_outputs.shape[-1] > 1:
        if all_indices is not None:
            for i in range(all_indices.shape[-1]):
                data_dict[f'{args.index_col}_{i}'] = all_indices[:, i]
        if all_labels is not None:
            for i in range(all_labels.shape[-1]):
                data_dict[f'{args.label_col}_{i}'] = all_labels[:, i]
        for i in range(all_outputs.shape[-1]):
            data_dict[f'{output_col}_{i}'] = all_outputs[:, i]
    else:
        if all_indices is not None:
            if all_indices.shape[-1] == 1:
                all_indices = all_indices.squeeze(-1)
            data_dict[args.index_col] = list(all_indices)
        if all_labels is not None:
            if all_labels.shape[-1] == 1:
                all_labels = all_labels.squeeze(-1)
            data_dict[args.label_col] = list(all_labels)
        if all_outputs.shape[-1] == 1:
            all_outputs = all_outputs.squeeze(-1)
        data_dict[output_col] = list(all_outputs)

    df = pd.DataFrame(data=data_dict)

    results_filename = args.results_file
    if results_filename:
        filename_no_ext, ext = os.path.splitext(results_filename)
        if ext and ext in _FMT_EXT.values():
            # if filename provided with one of expected ext,
            # remove it as it will be added back
            results_filename = filename_no_ext
    else:
        # base default filename on model name + img-size
        img_size = data_config[""input_size""][1]
        results_filename = f'{args.model}-{img_size}'

    if args.results_dir:
        results_filename = os.path.join(args.results_dir, results_filename)

    for fmt in args.results_format:
        save_results(df, results_filename, fmt)

    if not args.no_console_results:
        print(f'--result')
        print(df.set_index(args.filename_col).to_json(orient='index', indent=4))


def save_results(df, results_filename, results_format='csv', filename_col='filename'):
    np.set_printoptions(threshold=maxsize)
    results_filename += _FMT_EXT[results_format]
    if results_format == 'parquet':
        df.set_index(filename_col).to_parquet(results_filename)
    elif results_format == 'json':
        df.set_index(filename_col).to_json(results_filename, indent=4, orient='index')
    elif results_format == 'json-records':
        df.to_json(results_filename, lines=True, orient='records')
    elif results_format == 'json-split':
        df.to_json(results_filename, indent=4, orient='split', index=False)
    else:
        df.to_csv(results_filename, index=False)


if __name__ == '__main__':
    main()
"
huggingface_pytorch-image-models,huggingface_pytorch-image-models_validate.py,22020,https://raw.githubusercontent.com/huggingface/pytorch-image-models/main/validate.py,"#!/usr/bin/env python3
"""""" ImageNet Validation Script

This is intended to be a lean and easily modifiable ImageNet validation script for evaluating pretrained
models or training checkpoints against ImageNet or similarly organized image datasets. It prioritizes
canonical PyTorch, standard Python style, and good performance. Repurpose as you see fit.

Hacked together by Ross Wightman (https://github.com/rwightman)
""""""
import argparse
import csv
import glob
import json
import logging
import os
import time
from collections import OrderedDict
from contextlib import suppress
from functools import partial

import torch
import torch.nn as nn
import torch.nn.parallel

from timm.data import create_dataset, create_loader, resolve_data_config, RealLabelsImagenet
from timm.layers import apply_test_time_pool, set_fast_norm
from timm.models import create_model, load_checkpoint, is_model, list_models
from timm.utils import accuracy, AverageMeter, natural_key, setup_default_logging, set_jit_fuser, \
    decay_batch_step, check_batch_size_retry, ParseKwargs, reparameterize_model

try:
    from apex import amp
    has_apex = True
except ImportError:
    has_apex = False

try:
    from functorch.compile import memory_efficient_fusion
    has_functorch = True
except ImportError as e:
    has_functorch = False

has_compile = hasattr(torch, 'compile')

_logger = logging.getLogger('validate')


parser = argparse.ArgumentParser(description='PyTorch ImageNet Validation')
parser.add_argument('data', nargs='?', metavar='DIR', const=None,
                    help='path to dataset (*deprecated*, use --data-dir)')
parser.add_argument('--data-dir', metavar='DIR',
                    help='path to dataset (root dir)')
parser.add_argument('--dataset', metavar='NAME', default='',
                    help='dataset type + name (""<type>/<name>"") (default: ImageFolder or ImageTar if empty)')
parser.add_argument('--split', metavar='NAME', default='validation',
                    help='dataset split (default: validation)')
parser.add_argument('--num-samples', default=None, type=int,
                    metavar='N', help='Manually specify num samples in dataset split, for IterableDatasets.')
parser.add_argument('--dataset-download', action='store_true', default=False,
                    help='Allow download of dataset for torch/ and tfds/ datasets that support it.')
parser.add_argument('--class-map', default='', type=str, metavar='FILENAME',
                    help='path to class to idx mapping file (default: """")')
parser.add_argument('--input-key', default=None, type=str,
                   help='Dataset key for input images.')
parser.add_argument('--input-img-mode', default=None, type=str,
                   help='Dataset image conversion mode for input images.')
parser.add_argument('--target-key', default=None, type=str,
                   help='Dataset key for target labels.')
parser.add_argument('--dataset-trust-remote-code', action='store_true', default=False,
                   help='Allow huggingface dataset import to execute code downloaded from the dataset\'s repo.')

parser.add_argument('--model', '-m', metavar='NAME', default='dpn92',
                    help='model architecture (default: dpn92)')
parser.add_argument('--pretrained', dest='pretrained', action='store_true',
                    help='use pre-trained model')
parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('-b', '--batch-size', default=256, type=int,
                    metavar='N', help='mini-batch size (default: 256)')
parser.add_argument('--img-size', default=None, type=int,
                    metavar='N', help='Input image dimension, uses model default if empty')
parser.add_argument('--in-chans', type=int, default=None, metavar='N',
                    help='Image input channels (default: None => 3)')
parser.add_argument('--input-size', default=None, nargs=3, type=int, metavar='N',
                    help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')
parser.add_argument('--use-train-size', action='store_true', default=False,
                    help='force use of train input size, even when test size is specified in pretrained cfg')
parser.add_argument('--crop-pct', default=None, type=float,
                    metavar='N', help='Input image center crop pct')
parser.add_argument('--crop-mode', default=None, type=str,
                    metavar='N', help='Input image crop mode (squash, border, center). Model default if None.')
parser.add_argument('--crop-border-pixels', type=int, default=None,
                    help='Crop pixels from image border.')
parser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',
                    help='Override mean pixel value of dataset')
parser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',
                    help='Override std deviation of of dataset')
parser.add_argument('--interpolation', default='', type=str, metavar='NAME',
                    help='Image resize interpolation type (overrides model)')
parser.add_argument('--num-classes', type=int, default=None,
                    help='Number classes in dataset')
parser.add_argument('--gp', default=None, type=str, metavar='POOL',
                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')
parser.add_argument('--log-freq', default=10, type=int,
                    metavar='N', help='batch logging frequency (default: 10)')
parser.add_argument('--checkpoint', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('--num-gpu', type=int, default=1,
                    help='Number of GPUS to use')
parser.add_argument('--test-pool', dest='test_pool', action='store_true',
                    help='enable test time pool')
parser.add_argument('--no-prefetcher', action='store_true', default=False,
                    help='disable fast prefetcher')
parser.add_argument('--pin-mem', action='store_true', default=False,
                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')
parser.add_argument('--channels-last', action='store_true', default=False,
                    help='Use channels_last memory layout')
parser.add_argument('--device', default='cuda', type=str,
                    help=""Device (accelerator) to use."")
parser.add_argument('--amp', action='store_true', default=False,
                    help='use NVIDIA Apex AMP or Native AMP for mixed precision training')
parser.add_argument('--amp-dtype', default='float16', type=str,
                    help='lower precision AMP dtype (default: float16)')
parser.add_argument('--amp-impl', default='native', type=str,
                    help='AMP impl to use, ""native"" or ""apex"" (default: native)')
parser.add_argument('--model-dtype', default=None, type=str,
                   help='Model dtype override (non-AMP) (default: float32)')
parser.add_argument('--tf-preprocessing', action='store_true', default=False,
                    help='Use Tensorflow preprocessing pipeline (require CPU TF installed')
parser.add_argument('--use-ema', dest='use_ema', action='store_true',
                    help='use ema version of weights if present')
parser.add_argument('--fuser', default='', type=str,
                    help=""Select jit fuser. One of ('', 'te', 'old', 'nvfuser')"")
parser.add_argument('--fast-norm', default=False, action='store_true',
                    help='enable experimental fast-norm')
parser.add_argument('--reparam', default=False, action='store_true',
                    help='Reparameterize model')
parser.add_argument('--model-kwargs', nargs='*', default={}, action=ParseKwargs)
parser.add_argument('--torchcompile-mode', type=str, default=None,
                    help=""torch.compile mode (default: None)."")

scripting_group = parser.add_mutually_exclusive_group()
scripting_group.add_argument('--torchscript', default=False, action='store_true',
                             help='torch.jit.script the full model')
scripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',
                             help=""Enable compilation w/ specified backend (default: inductor)."")
scripting_group.add_argument('--aot-autograd', default=False, action='store_true',
                             help=""Enable AOT Autograd support."")

parser.add_argument('--results-file', default='', type=str, metavar='FILENAME',
                    help='Output csv file for validation results (summary)')
parser.add_argument('--results-format', default='csv', type=str,
                    help='Format for results file one of (csv, json) (default: csv).')
parser.add_argument('--real-labels', default='', type=str, metavar='FILENAME',
                    help='Real labels JSON file for imagenet evaluation')
parser.add_argument('--valid-labels', default='', type=str, metavar='FILENAME',
                    help='Valid label indices txt file for validation of partial label space')
parser.add_argument('--retry', default=False, action='store_true',
                    help='Enable batch size decay & retry for single model validation')


def validate(args):
    # might as well try to validate something
    args.pretrained = args.pretrained or not args.checkpoint
    args.prefetcher = not args.no_prefetcher

    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

    device = torch.device(args.device)

    model_dtype = None
    if args.model_dtype:
        assert args.model_dtype in ('float32', 'float16', 'bfloat16')
        model_dtype = getattr(torch, args.model_dtype)

    # resolve AMP arguments based on PyTorch / Apex availability
    use_amp = None
    amp_autocast = suppress
    if args.amp:
        assert model_dtype is None or model_dtype == torch.float32, 'float32 model dtype must be used with AMP'
        if args.amp_impl == 'apex':
            assert has_apex, 'AMP impl specified as APEX but APEX is not installed.'
            assert args.amp_dtype == 'float16'
            use_amp = 'apex'
            _logger.info('Validating in mixed precision with NVIDIA APEX AMP.')
        else:
            assert args.amp_dtype in ('float16', 'bfloat16')
            use_amp = 'native'
            amp_dtype = torch.bfloat16 if args.amp_dtype == 'bfloat16' else torch.float16
            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)
            _logger.info('Validating in mixed precision with native PyTorch AMP.')
    else:
        _logger.info(f'Validating in {model_dtype or torch.float32}. AMP not enabled.')

    if args.fuser:
        set_jit_fuser(args.fuser)

    if args.fast_norm:
        set_fast_norm()

    # create model
    in_chans = 3
    if args.in_chans is not None:
        in_chans = args.in_chans
    elif args.input_size is not None:
        in_chans = args.input_size[0]

    model = create_model(
        args.model,
        pretrained=args.pretrained,
        num_classes=args.num_classes,
        in_chans=in_chans,
        global_pool=args.gp,
        scriptable=args.torchscript,
        **args.model_kwargs,
    )
    if args.num_classes is None:
        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'
        args.num_classes = model.num_classes

    if args.checkpoint:
        load_checkpoint(model, args.checkpoint, args.use_ema)

    if args.reparam:
        model = reparameterize_model(model)

    param_count = sum([m.numel() for m in model.parameters()])
    _logger.info('Model %s created, param count: %d' % (args.model, param_count))

    data_config = resolve_data_config(
        vars(args),
        model=model,
        use_test_size=not args.use_train_size,
        verbose=True,
    )
    test_time_pool = False
    if args.test_pool:
        model, test_time_pool = apply_test_time_pool(model, data_config)

    model = model.to(device=device, dtype=model_dtype)  # FIXME move model device & dtype into create_model
    if args.channels_last:
        model = model.to(memory_format=torch.channels_last)

    if args.torchscript:
        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'
        model = torch.jit.script(model)
    elif args.torchcompile:
        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'
        torch._dynamo.reset()
        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)
    elif args.aot_autograd:
        assert has_functorch, ""functorch is needed for --aot-autograd""
        model = memory_efficient_fusion(model)

    if use_amp == 'apex':
        model = amp.initialize(model, opt_level='O1')

    if args.num_gpu > 1:
        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))

    criterion = nn.CrossEntropyLoss().to(device)

    root_dir = args.data or args.data_dir
    if args.input_img_mode is None:
        input_img_mode = 'RGB' if data_config['input_size'][0] == 3 else 'L'
    else:
        input_img_mode = args.input_img_mode
    dataset = create_dataset(
        root=root_dir,
        name=args.dataset,
        split=args.split,
        download=args.dataset_download,
        load_bytes=args.tf_preprocessing,
        class_map=args.class_map,
        num_samples=args.num_samples,
        input_key=args.input_key,
        input_img_mode=input_img_mode,
        target_key=args.target_key,
        trust_remote_code=args.dataset_trust_remote_code,
    )

    if args.valid_labels:
        with open(args.valid_labels, 'r') as f:
            valid_labels = [int(line.rstrip()) for line in f]
    else:
        valid_labels = None

    if args.real_labels:
        real_labels = RealLabelsImagenet(dataset.filenames(basename=True), real_json=args.real_labels)
    else:
        real_labels = None

    crop_pct = 1.0 if test_time_pool else data_config['crop_pct']
    loader = create_loader(
        dataset,
        input_size=data_config['input_size'],
        batch_size=args.batch_size,
        use_prefetcher=args.prefetcher,
        interpolation=data_config['interpolation'],
        mean=data_config['mean'],
        std=data_config['std'],
        num_workers=args.workers,
        crop_pct=crop_pct,
        crop_mode=data_config['crop_mode'],
        crop_border_pixels=args.crop_border_pixels,
        pin_memory=args.pin_mem,
        device=device,
        img_dtype=model_dtype or torch.float32,
        tf_preprocessing=args.tf_preprocessing,
    )

    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()

    model.eval()
    with torch.no_grad():
        # warmup, reduce variability of first batch time, especially for comparing torchscript vs non
        input = torch.randn((args.batch_size,) + tuple(data_config['input_size'])).to(device=device, dtype=model_dtype)
        if args.channels_last:
            input = input.contiguous(memory_format=torch.channels_last)
        with amp_autocast():
            model(input)

        end = time.time()
        for batch_idx, (input, target) in enumerate(loader):
            if args.no_prefetcher:
                target = target.to(device=device)
                input = input.to(device=device, dtype=model_dtype)
            if args.channels_last:
                input = input.contiguous(memory_format=torch.channels_last)

            # compute output
            with amp_autocast():
                output = model(input)

                if valid_labels is not None:
                    output = output[:, valid_labels]
                loss = criterion(output, target)

            if real_labels is not None:
                real_labels.add_result(output)

            # measure accuracy and record loss
            acc1, acc5 = accuracy(output.detach(), target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1.item(), input.size(0))
            top5.update(acc5.item(), input.size(0))

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if batch_idx % args.log_freq == 0:
                _logger.info(
                    'Test: [{0:>4d}/{1}]  '
                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '
                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '
                    'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  '
                    'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})'.format(
                        batch_idx,
                        len(loader),
                        batch_time=batch_time,
                        rate_avg=input.size(0) / batch_time.avg,
                        loss=losses,
                        top1=top1,
                        top5=top5
                    )
                )

    if real_labels is not None:
        # real labels mode replaces topk values at the end
        top1a, top5a = real_labels.get_accuracy(k=1), real_labels.get_accuracy(k=5)
    else:
        top1a, top5a = top1.avg, top5.avg
    results = OrderedDict(
        model=args.model,
        top1=round(top1a, 4), top1_err=round(100 - top1a, 4),
        top5=round(top5a, 4), top5_err=round(100 - top5a, 4),
        param_count=round(param_count / 1e6, 2),
        img_size=data_config['input_size'][-1],
        crop_pct=crop_pct,
        interpolation=data_config['interpolation'],
    )

    _logger.info(' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})'.format(
       results['top1'], results['top1_err'], results['top5'], results['top5_err']))

    return results


def _try_run(args, initial_batch_size):
    batch_size = initial_batch_size
    results = OrderedDict()
    error_str = 'Unknown'
    while batch_size:
        args.batch_size = batch_size * args.num_gpu  # multiply by num-gpu for DataParallel case
        try:
            if 'cuda' in args.device and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif ""npu"" in args.device and torch.npu.is_available():
                torch.npu.empty_cache()
            results = validate(args)
            return results
        except RuntimeError as e:
            error_str = str(e)
            _logger.error(f'""{error_str}"" while running validation.')
            if not check_batch_size_retry(error_str):
                break
        batch_size = decay_batch_step(batch_size)
        _logger.warning(f'Reducing batch size to {batch_size} for retry.')
    results['model'] = args.model
    results['error'] = error_str
    _logger.error(f'{args.model} failed to validate ({error_str}).')
    return results


_NON_IN1K_FILTERS = ['*_in21k', '*_in22k', '*in12k', '*_dino', '*fcmae', '*seer']


def main():
    setup_default_logging()
    args = parser.parse_args()
    model_cfgs = []
    model_names = []
    if os.path.isdir(args.checkpoint):
        # validate all checkpoints in a path with same model
        checkpoints = glob.glob(args.checkpoint + '/*.pth.tar')
        checkpoints += glob.glob(args.checkpoint + '/*.pth')
        model_names = list_models(args.model)
        model_cfgs = [(args.model, c) for c in sorted(checkpoints, key=natural_key)]
    else:
        if args.model == 'all':
            # validate all models in a list of names with pretrained checkpoints
            args.pretrained = True
            model_names = list_models(
                pretrained=True,
                exclude_filters=_NON_IN1K_FILTERS,
            )
            model_cfgs = [(n, '') for n in model_names]
        elif not is_model(args.model):
            # model name doesn't exist, try as wildcard filter
            model_names = list_models(
                args.model,
                pretrained=True,
            )
            model_cfgs = [(n, '') for n in model_names]

        if not model_cfgs and os.path.isfile(args.model):
            with open(args.model) as f:
                model_names = [line.rstrip() for line in f]
            model_cfgs = [(n, None) for n in model_names if n]

    if len(model_cfgs):
        _logger.info('Running bulk validation on these pretrained models: {}'.format(', '.join(model_names)))
        results = []
        try:
            initial_batch_size = args.batch_size
            for m, c in model_cfgs:
                args.model = m
                args.checkpoint = c
                r = _try_run(args, initial_batch_size)
                if 'error' in r:
                    continue
                if args.checkpoint:
                    r['checkpoint'] = args.checkpoint
                results.append(r)
        except KeyboardInterrupt as e:
            pass
        results = sorted(results, key=lambda x: x['top1'], reverse=True)
    else:
        if args.retry:
            results = _try_run(args, args.batch_size)
        else:
            results = validate(args)

    if args.results_file:
        write_results(args.results_file, results, format=args.results_format)

    # output results in JSON to stdout w/ delimiter for runner script
    print(f'--result\n{json.dumps(results, indent=4)}')


def write_results(results_file, results, format='csv'):
    with open(results_file, mode='w') as cf:
        if format == 'json':
            json.dump(results, cf, indent=4)
        else:
            if not isinstance(results, (list, tuple)):
                results = [results]
            if not results:
                return
            dw = csv.DictWriter(cf, fieldnames=results[0].keys())
            dw.writeheader()
            for r in results:
                dw.writerow(r)
            cf.flush()



if __name__ == '__main__':
    main()
"
xinntao_Real-ESRGAN,xinntao_Real-ESRGAN_inference_realesrgan_video.py,16910,https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/inference_realesrgan_video.py,"import argparse
import cv2
import glob
import mimetypes
import numpy as np
import os
import shutil
import subprocess
import torch
from basicsr.archs.rrdbnet_arch import RRDBNet
from basicsr.utils.download_util import load_file_from_url
from os import path as osp
from tqdm import tqdm

from realesrgan import RealESRGANer
from realesrgan.archs.srvgg_arch import SRVGGNetCompact

try:
    import ffmpeg
except ImportError:
    import pip
    pip.main(['install', '--user', 'ffmpeg-python'])
    import ffmpeg


def get_video_meta_info(video_path):
    ret = {}
    probe = ffmpeg.probe(video_path)
    video_streams = [stream for stream in probe['streams'] if stream['codec_type'] == 'video']
    has_audio = any(stream['codec_type'] == 'audio' for stream in probe['streams'])
    ret['width'] = video_streams[0]['width']
    ret['height'] = video_streams[0]['height']
    ret['fps'] = eval(video_streams[0]['avg_frame_rate'])
    ret['audio'] = ffmpeg.input(video_path).audio if has_audio else None
    ret['nb_frames'] = int(video_streams[0]['nb_frames'])
    return ret


def get_sub_video(args, num_process, process_idx):
    if num_process == 1:
        return args.input
    meta = get_video_meta_info(args.input)
    duration = int(meta['nb_frames'] / meta['fps'])
    part_time = duration // num_process
    print(f'duration: {duration}, part_time: {part_time}')
    os.makedirs(osp.join(args.output, f'{args.video_name}_inp_tmp_videos'), exist_ok=True)
    out_path = osp.join(args.output, f'{args.video_name}_inp_tmp_videos', f'{process_idx:03d}.mp4')
    cmd = [
        args.ffmpeg_bin, f'-i {args.input}', '-ss', f'{part_time * process_idx}',
        f'-to {part_time * (process_idx + 1)}' if process_idx != num_process - 1 else '', '-async 1', out_path, '-y'
    ]
    print(' '.join(cmd))
    subprocess.call(' '.join(cmd), shell=True)
    return out_path


class Reader:

    def __init__(self, args, total_workers=1, worker_idx=0):
        self.args = args
        input_type = mimetypes.guess_type(args.input)[0]
        self.input_type = 'folder' if input_type is None else input_type
        self.paths = []  # for image&folder type
        self.audio = None
        self.input_fps = None
        if self.input_type.startswith('video'):
            video_path = get_sub_video(args, total_workers, worker_idx)
            self.stream_reader = (
                ffmpeg.input(video_path).output('pipe:', format='rawvideo', pix_fmt='bgr24',
                                                loglevel='error').run_async(
                                                    pipe_stdin=True, pipe_stdout=True, cmd=args.ffmpeg_bin))
            meta = get_video_meta_info(video_path)
            self.width = meta['width']
            self.height = meta['height']
            self.input_fps = meta['fps']
            self.audio = meta['audio']
            self.nb_frames = meta['nb_frames']

        else:
            if self.input_type.startswith('image'):
                self.paths = [args.input]
            else:
                paths = sorted(glob.glob(os.path.join(args.input, '*')))
                tot_frames = len(paths)
                num_frame_per_worker = tot_frames // total_workers + (1 if tot_frames % total_workers else 0)
                self.paths = paths[num_frame_per_worker * worker_idx:num_frame_per_worker * (worker_idx + 1)]

            self.nb_frames = len(self.paths)
            assert self.nb_frames > 0, 'empty folder'
            from PIL import Image
            tmp_img = Image.open(self.paths[0])
            self.width, self.height = tmp_img.size
        self.idx = 0

    def get_resolution(self):
        return self.height, self.width

    def get_fps(self):
        if self.args.fps is not None:
            return self.args.fps
        elif self.input_fps is not None:
            return self.input_fps
        return 24

    def get_audio(self):
        return self.audio

    def __len__(self):
        return self.nb_frames

    def get_frame_from_stream(self):
        img_bytes = self.stream_reader.stdout.read(self.width * self.height * 3)  # 3 bytes for one pixel
        if not img_bytes:
            return None
        img = np.frombuffer(img_bytes, np.uint8).reshape([self.height, self.width, 3])
        return img

    def get_frame_from_list(self):
        if self.idx >= self.nb_frames:
            return None
        img = cv2.imread(self.paths[self.idx])
        self.idx += 1
        return img

    def get_frame(self):
        if self.input_type.startswith('video'):
            return self.get_frame_from_stream()
        else:
            return self.get_frame_from_list()

    def close(self):
        if self.input_type.startswith('video'):
            self.stream_reader.stdin.close()
            self.stream_reader.wait()


class Writer:

    def __init__(self, args, audio, height, width, video_save_path, fps):
        out_width, out_height = int(width * args.outscale), int(height * args.outscale)
        if out_height > 2160:
            print('You are generating video that is larger than 4K, which will be very slow due to IO speed.',
                  'We highly recommend to decrease the outscale(aka, -s).')

        if audio is not None:
            self.stream_writer = (
                ffmpeg.input('pipe:', format='rawvideo', pix_fmt='bgr24', s=f'{out_width}x{out_height}',
                             framerate=fps).output(
                                 audio,
                                 video_save_path,
                                 pix_fmt='yuv420p',
                                 vcodec='libx264',
                                 loglevel='error',
                                 acodec='copy').overwrite_output().run_async(
                                     pipe_stdin=True, pipe_stdout=True, cmd=args.ffmpeg_bin))
        else:
            self.stream_writer = (
                ffmpeg.input('pipe:', format='rawvideo', pix_fmt='bgr24', s=f'{out_width}x{out_height}',
                             framerate=fps).output(
                                 video_save_path, pix_fmt='yuv420p', vcodec='libx264',
                                 loglevel='error').overwrite_output().run_async(
                                     pipe_stdin=True, pipe_stdout=True, cmd=args.ffmpeg_bin))

    def write_frame(self, frame):
        frame = frame.astype(np.uint8).tobytes()
        self.stream_writer.stdin.write(frame)

    def close(self):
        self.stream_writer.stdin.close()
        self.stream_writer.wait()


def inference_video(args, video_save_path, device=None, total_workers=1, worker_idx=0):
    # ---------------------- determine models according to model names ---------------------- #
    args.model_name = args.model_name.split('.pth')[0]
    if args.model_name == 'RealESRGAN_x4plus':  # x4 RRDBNet model
        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)
        netscale = 4
        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth']
    elif args.model_name == 'RealESRNet_x4plus':  # x4 RRDBNet model
        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)
        netscale = 4
        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth']
    elif args.model_name == 'RealESRGAN_x4plus_anime_6B':  # x4 RRDBNet model with 6 blocks
        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)
        netscale = 4
        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth']
    elif args.model_name == 'RealESRGAN_x2plus':  # x2 RRDBNet model
        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)
        netscale = 2
        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth']
    elif args.model_name == 'realesr-animevideov3':  # x4 VGG-style model (XS size)
        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')
        netscale = 4
        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth']
    elif args.model_name == 'realesr-general-x4v3':  # x4 VGG-style model (S size)
        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')
        netscale = 4
        file_url = [
            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth',
            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth'
        ]

    # ---------------------- determine model paths ---------------------- #
    model_path = os.path.join('weights', args.model_name + '.pth')
    if not os.path.isfile(model_path):
        ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
        for url in file_url:
            # model_path will be updated
            model_path = load_file_from_url(
                url=url, model_dir=os.path.join(ROOT_DIR, 'weights'), progress=True, file_name=None)

    # use dni to control the denoise strength
    dni_weight = None
    if args.model_name == 'realesr-general-x4v3' and args.denoise_strength != 1:
        wdn_model_path = model_path.replace('realesr-general-x4v3', 'realesr-general-wdn-x4v3')
        model_path = [model_path, wdn_model_path]
        dni_weight = [args.denoise_strength, 1 - args.denoise_strength]

    # restorer
    upsampler = RealESRGANer(
        scale=netscale,
        model_path=model_path,
        dni_weight=dni_weight,
        model=model,
        tile=args.tile,
        tile_pad=args.tile_pad,
        pre_pad=args.pre_pad,
        half=not args.fp32,
        device=device,
    )

    if 'anime' in args.model_name and args.face_enhance:
        print('face_enhance is not supported in anime models, we turned this option off for you. '
              'if you insist on turning it on, please manually comment the relevant lines of code.')
        args.face_enhance = False

    if args.face_enhance:  # Use GFPGAN for face enhancement
        from gfpgan import GFPGANer
        face_enhancer = GFPGANer(
            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',
            upscale=args.outscale,
            arch='clean',
            channel_multiplier=2,
            bg_upsampler=upsampler)  # TODO support custom device
    else:
        face_enhancer = None

    reader = Reader(args, total_workers, worker_idx)
    audio = reader.get_audio()
    height, width = reader.get_resolution()
    fps = reader.get_fps()
    writer = Writer(args, audio, height, width, video_save_path, fps)

    pbar = tqdm(total=len(reader), unit='frame', desc='inference')
    while True:
        img = reader.get_frame()
        if img is None:
            break

        try:
            if args.face_enhance:
                _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)
            else:
                output, _ = upsampler.enhance(img, outscale=args.outscale)
        except RuntimeError as error:
            print('Error', error)
            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')
        else:
            writer.write_frame(output)

        torch.cuda.synchronize(device)
        pbar.update(1)

    reader.close()
    writer.close()


def run(args):
    args.video_name = osp.splitext(os.path.basename(args.input))[0]
    video_save_path = osp.join(args.output, f'{args.video_name}_{args.suffix}.mp4')

    if args.extract_frame_first:
        tmp_frames_folder = osp.join(args.output, f'{args.video_name}_inp_tmp_frames')
        os.makedirs(tmp_frames_folder, exist_ok=True)
        os.system(f'ffmpeg -i {args.input} -qscale:v 1 -qmin 1 -qmax 1 -vsync 0  {tmp_frames_folder}/frame%08d.png')
        args.input = tmp_frames_folder

    num_gpus = torch.cuda.device_count()
    num_process = num_gpus * args.num_process_per_gpu
    if num_process == 1:
        inference_video(args, video_save_path)
        return

    ctx = torch.multiprocessing.get_context('spawn')
    pool = ctx.Pool(num_process)
    os.makedirs(osp.join(args.output, f'{args.video_name}_out_tmp_videos'), exist_ok=True)
    pbar = tqdm(total=num_process, unit='sub_video', desc='inference')
    for i in range(num_process):
        sub_video_save_path = osp.join(args.output, f'{args.video_name}_out_tmp_videos', f'{i:03d}.mp4')
        pool.apply_async(
            inference_video,
            args=(args, sub_video_save_path, torch.device(i % num_gpus), num_process, i),
            callback=lambda arg: pbar.update(1))
    pool.close()
    pool.join()

    # combine sub videos
    # prepare vidlist.txt
    with open(f'{args.output}/{args.video_name}_vidlist.txt', 'w') as f:
        for i in range(num_process):
            f.write(f'file \'{args.video_name}_out_tmp_videos/{i:03d}.mp4\'\n')

    cmd = [
        args.ffmpeg_bin, '-f', 'concat', '-safe', '0', '-i', f'{args.output}/{args.video_name}_vidlist.txt', '-c',
        'copy', f'{video_save_path}'
    ]
    print(' '.join(cmd))
    subprocess.call(cmd)
    shutil.rmtree(osp.join(args.output, f'{args.video_name}_out_tmp_videos'))
    if osp.exists(osp.join(args.output, f'{args.video_name}_inp_tmp_videos')):
        shutil.rmtree(osp.join(args.output, f'{args.video_name}_inp_tmp_videos'))
    os.remove(f'{args.output}/{args.video_name}_vidlist.txt')


def main():
    """"""Inference demo for Real-ESRGAN.
    It mainly for restoring anime videos.

    """"""
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input video, image or folder')
    parser.add_argument(
        '-n',
        '--model_name',
        type=str,
        default='realesr-animevideov3',
        help=('Model names: realesr-animevideov3 | RealESRGAN_x4plus_anime_6B | RealESRGAN_x4plus | RealESRNet_x4plus |'
              ' RealESRGAN_x2plus | realesr-general-x4v3'
              'Default:realesr-animevideov3'))
    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')
    parser.add_argument(
        '-dn',
        '--denoise_strength',
        type=float,
        default=0.5,
        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '
              'Only used for the realesr-general-x4v3 model'))
    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')
    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored video')
    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')
    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')
    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')
    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')
    parser.add_argument(
        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')
    parser.add_argument('--fps', type=float, default=None, help='FPS of the output video')
    parser.add_argument('--ffmpeg_bin', type=str, default='ffmpeg', help='The path to ffmpeg')
    parser.add_argument('--extract_frame_first', action='store_true')
    parser.add_argument('--num_process_per_gpu', type=int, default=1)

    parser.add_argument(
        '--alpha_upsampler',
        type=str,
        default='realesrgan',
        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')
    parser.add_argument(
        '--ext',
        type=str,
        default='auto',
        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')
    args = parser.parse_args()

    args.input = args.input.rstrip('/').rstrip('\\')
    os.makedirs(args.output, exist_ok=True)

    if mimetypes.guess_type(args.input)[0] is not None and mimetypes.guess_type(args.input)[0].startswith('video'):
        is_video = True
    else:
        is_video = False

    if is_video and args.input.endswith('.flv'):
        mp4_path = args.input.replace('.flv', '.mp4')
        os.system(f'ffmpeg -i {args.input} -codec copy {mp4_path}')
        args.input = mp4_path

    if args.extract_frame_first and not is_video:
        args.extract_frame_first = False

    run(args)

    if args.extract_frame_first:
        tmp_frames_folder = osp.join(args.output, f'{args.video_name}_inp_tmp_frames')
        shutil.rmtree(tmp_frames_folder)


if __name__ == '__main__':
    main()
"
feder-cr_Jobs_Applier_AI_Agent_AIHawk,feder-cr_Jobs_Applier_AI_Agent_AIHawk_main.py,22143,https://raw.githubusercontent.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk/main/main.py,"import base64
import sys
from pathlib import Path
import traceback
from typing import List, Optional, Tuple, Dict

import click
import inquirer
import yaml
from selenium import webdriver
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
import re
from src.libs.resume_and_cover_builder import ResumeFacade, ResumeGenerator, StyleManager
from src.resume_schemas.job_application_profile import JobApplicationProfile
from src.resume_schemas.resume import Resume
from src.logging import logger
from src.utils.chrome_utils import init_browser
from src.utils.constants import (
    PLAIN_TEXT_RESUME_YAML,
    SECRETS_YAML,
    WORK_PREFERENCES_YAML,
)
# from ai_hawk.bot_facade import AIHawkBotFacade
# from ai_hawk.job_manager import AIHawkJobManager
# from ai_hawk.llm.llm_manager import GPTAnswerer


class ConfigError(Exception):
    """"""Custom exception for configuration-related errors.""""""
    pass


class ConfigValidator:
    """"""Validates configuration and secrets YAML files.""""""

    EMAIL_REGEX = re.compile(r""^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"")
    REQUIRED_CONFIG_KEYS = {
        ""remote"": bool,
        ""experience_level"": dict,
        ""job_types"": dict,
        ""date"": dict,
        ""positions"": list,
        ""locations"": list,
        ""location_blacklist"": list,
        ""distance"": int,
        ""company_blacklist"": list,
        ""title_blacklist"": list,
    }
    EXPERIENCE_LEVELS = [
        ""internship"",
        ""entry"",
        ""associate"",
        ""mid_senior_level"",
        ""director"",
        ""executive"",
    ]
    JOB_TYPES = [
        ""full_time"",
        ""contract"",
        ""part_time"",
        ""temporary"",
        ""internship"",
        ""other"",
        ""volunteer"",
    ]
    DATE_FILTERS = [""all_time"", ""month"", ""week"", ""24_hours""]
    APPROVED_DISTANCES = {0, 5, 10, 25, 50, 100}

    @staticmethod
    def validate_email(email: str) -> bool:
        """"""Validate the format of an email address.""""""
        return bool(ConfigValidator.EMAIL_REGEX.match(email))

    @staticmethod
    def load_yaml(yaml_path: Path) -> dict:
        """"""Load and parse a YAML file.""""""
        try:
            with open(yaml_path, ""r"") as stream:
                return yaml.safe_load(stream)
        except yaml.YAMLError as exc:
            raise ConfigError(f""Error reading YAML file {yaml_path}: {exc}"")
        except FileNotFoundError:
            raise ConfigError(f""YAML file not found: {yaml_path}"")

    @classmethod
    def validate_config(cls, config_yaml_path: Path) -> dict:
        """"""Validate the main configuration YAML file.""""""
        parameters = cls.load_yaml(config_yaml_path)
        # Check for required keys and their types
        for key, expected_type in cls.REQUIRED_CONFIG_KEYS.items():
            if key not in parameters:
                if key in [""company_blacklist"", ""title_blacklist"", ""location_blacklist""]:
                    parameters[key] = []
                else:
                    raise ConfigError(f""Missing required key '{key}' in {config_yaml_path}"")
            elif not isinstance(parameters[key], expected_type):
                if key in [""company_blacklist"", ""title_blacklist"", ""location_blacklist""] and parameters[key] is None:
                    parameters[key] = []
                else:
                    raise ConfigError(
                        f""Invalid type for key '{key}' in {config_yaml_path}. Expected {expected_type.__name__}.""
                    )
        cls._validate_experience_levels(parameters[""experience_level""], config_yaml_path)
        cls._validate_job_types(parameters[""job_types""], config_yaml_path)
        cls._validate_date_filters(parameters[""date""], config_yaml_path)
        cls._validate_list_of_strings(parameters, [""positions"", ""locations""], config_yaml_path)
        cls._validate_distance(parameters[""distance""], config_yaml_path)
        cls._validate_blacklists(parameters, config_yaml_path)
        return parameters

    @classmethod
    def _validate_experience_levels(cls, experience_levels: dict, config_path: Path):
        """"""Ensure experience levels are booleans.""""""
        for level in cls.EXPERIENCE_LEVELS:
            if not isinstance(experience_levels.get(level), bool):
                raise ConfigError(
                    f""Experience level '{level}' must be a boolean in {config_path}""
                )

    @classmethod
    def _validate_job_types(cls, job_types: dict, config_path: Path):
        """"""Ensure job types are booleans.""""""
        for job_type in cls.JOB_TYPES:
            if not isinstance(job_types.get(job_type), bool):
                raise ConfigError(
                    f""Job type '{job_type}' must be a boolean in {config_path}""
                )

    @classmethod
    def _validate_date_filters(cls, date_filters: dict, config_path: Path):
        """"""Ensure date filters are booleans.""""""
        for date_filter in cls.DATE_FILTERS:
            if not isinstance(date_filters.get(date_filter), bool):
                raise ConfigError(
                    f""Date filter '{date_filter}' must be a boolean in {config_path}""
                )

    @classmethod
    def _validate_list_of_strings(cls, parameters: dict, keys: list, config_path: Path):
        """"""Ensure specified keys are lists of strings.""""""
        for key in keys:
            if not all(isinstance(item, str) for item in parameters[key]):
                raise ConfigError(
                    f""'{key}' must be a list of strings in {config_path}""
                )

    @classmethod
    def _validate_distance(cls, distance: int, config_path: Path):
        """"""Validate the distance value.""""""
        if distance not in cls.APPROVED_DISTANCES:
            raise ConfigError(
                f""Invalid distance value '{distance}' in {config_path}. Must be one of: {cls.APPROVED_DISTANCES}""
            )

    @classmethod
    def _validate_blacklists(cls, parameters: dict, config_path: Path):
        """"""Ensure blacklists are lists.""""""
        for blacklist in [""company_blacklist"", ""title_blacklist"", ""location_blacklist""]:
            if not isinstance(parameters.get(blacklist), list):
                raise ConfigError(
                    f""'{blacklist}' must be a list in {config_path}""
                )
            if parameters[blacklist] is None:
                parameters[blacklist] = []

    @staticmethod
    def validate_secrets(secrets_yaml_path: Path) -> str:
        """"""Validate the secrets YAML file and retrieve the LLM API key.""""""
        secrets = ConfigValidator.load_yaml(secrets_yaml_path)
        mandatory_secrets = [""llm_api_key""]

        for secret in mandatory_secrets:
            if secret not in secrets:
                raise ConfigError(f""Missing secret '{secret}' in {secrets_yaml_path}"")

            if not secrets[secret]:
                raise ConfigError(f""Secret '{secret}' cannot be empty in {secrets_yaml_path}"")

        return secrets[""llm_api_key""]


class FileManager:
    """"""Handles file system operations and validations.""""""

    REQUIRED_FILES = [SECRETS_YAML, WORK_PREFERENCES_YAML, PLAIN_TEXT_RESUME_YAML]

    @staticmethod
    def validate_data_folder(app_data_folder: Path) -> Tuple[Path, Path, Path, Path]:
        """"""Validate the existence of the data folder and required files.""""""
        if not app_data_folder.is_dir():
            raise FileNotFoundError(f""Data folder not found: {app_data_folder}"")

        missing_files = [file for file in FileManager.REQUIRED_FILES if not (app_data_folder / file).exists()]
        if missing_files:
            raise FileNotFoundError(f""Missing files in data folder: {', '.join(missing_files)}"")

        output_folder = app_data_folder / ""output""
        output_folder.mkdir(exist_ok=True)

        return (
            app_data_folder / SECRETS_YAML,
            app_data_folder / WORK_PREFERENCES_YAML,
            app_data_folder / PLAIN_TEXT_RESUME_YAML,
            output_folder,
        )

    @staticmethod
    def get_uploads(plain_text_resume_file: Path) -> Dict[str, Path]:
        """"""Convert resume file paths to a dictionary.""""""
        if not plain_text_resume_file.exists():
            raise FileNotFoundError(f""Plain text resume file not found: {plain_text_resume_file}"")

        uploads = {""plainTextResume"": plain_text_resume_file}

        return uploads


def create_cover_letter(parameters: dict, llm_api_key: str):
    """"""
    Logic to create a CV.
    """"""
    try:
        logger.info(""Generating a CV based on provided parameters."")

        # Carica il resume in testo semplice
        with open(parameters[""uploads""][""plainTextResume""], ""r"", encoding=""utf-8"") as file:
            plain_text_resume = file.read()

        style_manager = StyleManager()
        available_styles = style_manager.get_styles()

        if not available_styles:
            logger.warning(""No styles available. Proceeding without style selection."")
        else:
            # Present style choices to the user
            choices = style_manager.format_choices(available_styles)
            questions = [
                inquirer.List(
                    ""style"",
                    message=""Select a style for the resume:"",
                    choices=choices,
                )
            ]
            style_answer = inquirer.prompt(questions)
            if style_answer and ""style"" in style_answer:
                selected_choice = style_answer[""style""]
                for style_name, (file_name, author_link) in available_styles.items():
                    if selected_choice.startswith(style_name):
                        style_manager.set_selected_style(style_name)
                        logger.info(f""Selected style: {style_name}"")
                        break
            else:
                logger.warning(""No style selected. Proceeding with default style."")
        questions = [
    inquirer.Text('job_url', message=""Please enter the URL of the job description:"")
        ]
        answers = inquirer.prompt(questions)
        job_url = answers.get('job_url')
        resume_generator = ResumeGenerator()
        resume_object = Resume(plain_text_resume)
        driver = init_browser()
        resume_generator.set_resume_object(resume_object)
        resume_facade = ResumeFacade(            
            api_key=llm_api_key,
            style_manager=style_manager,
            resume_generator=resume_generator,
            resume_object=resume_object,
            output_path=Path(""data_folder/output""),
        )
        resume_facade.set_driver(driver)
        resume_facade.link_to_job(job_url)
        result_base64, suggested_name = resume_facade.create_cover_letter()         

        # Decodifica Base64 in dati binari
        try:
            pdf_data = base64.b64decode(result_base64)
        except base64.binascii.Error as e:
            logger.error(""Error decoding Base64: %s"", e)
            raise

        # Definisci il percorso della cartella di output utilizzando `suggested_name`
        output_dir = Path(parameters[""outputFileDirectory""]) / suggested_name

        # Crea la cartella se non esiste
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f""Cartella di output creata o già esistente: {output_dir}"")
        except IOError as e:
            logger.error(""Error creating output directory: %s"", e)
            raise
        
        output_path = output_dir / ""cover_letter_tailored.pdf""
        try:
            with open(output_path, ""wb"") as file:
                file.write(pdf_data)
            logger.info(f""CV salvato in: {output_path}"")
        except IOError as e:
            logger.error(""Error writing file: %s"", e)
            raise
    except Exception as e:
        logger.exception(f""An error occurred while creating the CV: {e}"")
        raise


def create_resume_pdf_job_tailored(parameters: dict, llm_api_key: str):
    """"""
    Logic to create a CV.
    """"""
    try:
        logger.info(""Generating a CV based on provided parameters."")

        # Carica il resume in testo semplice
        with open(parameters[""uploads""][""plainTextResume""], ""r"", encoding=""utf-8"") as file:
            plain_text_resume = file.read()

        style_manager = StyleManager()
        available_styles = style_manager.get_styles()

        if not available_styles:
            logger.warning(""No styles available. Proceeding without style selection."")
        else:
            # Present style choices to the user
            choices = style_manager.format_choices(available_styles)
            questions = [
                inquirer.List(
                    ""style"",
                    message=""Select a style for the resume:"",
                    choices=choices,
                )
            ]
            style_answer = inquirer.prompt(questions)
            if style_answer and ""style"" in style_answer:
                selected_choice = style_answer[""style""]
                for style_name, (file_name, author_link) in available_styles.items():
                    if selected_choice.startswith(style_name):
                        style_manager.set_selected_style(style_name)
                        logger.info(f""Selected style: {style_name}"")
                        break
            else:
                logger.warning(""No style selected. Proceeding with default style."")
        questions = [inquirer.Text('job_url', message=""Please enter the URL of the job description:"")]
        answers = inquirer.prompt(questions)
        job_url = answers.get('job_url')
        resume_generator = ResumeGenerator()
        resume_object = Resume(plain_text_resume)
        driver = init_browser()
        resume_generator.set_resume_object(resume_object)
        resume_facade = ResumeFacade(            
            api_key=llm_api_key,
            style_manager=style_manager,
            resume_generator=resume_generator,
            resume_object=resume_object,
            output_path=Path(""data_folder/output""),
        )
        resume_facade.set_driver(driver)
        resume_facade.link_to_job(job_url)
        result_base64, suggested_name = resume_facade.create_resume_pdf_job_tailored()         

        # Decodifica Base64 in dati binari
        try:
            pdf_data = base64.b64decode(result_base64)
        except base64.binascii.Error as e:
            logger.error(""Error decoding Base64: %s"", e)
            raise

        # Definisci il percorso della cartella di output utilizzando `suggested_name`
        output_dir = Path(parameters[""outputFileDirectory""]) / suggested_name

        # Crea la cartella se non esiste
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f""Cartella di output creata o già esistente: {output_dir}"")
        except IOError as e:
            logger.error(""Error creating output directory: %s"", e)
            raise
        
        output_path = output_dir / ""resume_tailored.pdf""
        try:
            with open(output_path, ""wb"") as file:
                file.write(pdf_data)
            logger.info(f""CV salvato in: {output_path}"")
        except IOError as e:
            logger.error(""Error writing file: %s"", e)
            raise
    except Exception as e:
        logger.exception(f""An error occurred while creating the CV: {e}"")
        raise


def create_resume_pdf(parameters: dict, llm_api_key: str):
    """"""
    Logic to create a CV.
    """"""
    try:
        logger.info(""Generating a CV based on provided parameters."")

        # Load the plain text resume
        with open(parameters[""uploads""][""plainTextResume""], ""r"", encoding=""utf-8"") as file:
            plain_text_resume = file.read()

        # Initialize StyleManager
        style_manager = StyleManager()
        available_styles = style_manager.get_styles()

        if not available_styles:
            logger.warning(""No styles available. Proceeding without style selection."")
        else:
            # Present style choices to the user
            choices = style_manager.format_choices(available_styles)
            questions = [
                inquirer.List(
                    ""style"",
                    message=""Select a style for the resume:"",
                    choices=choices,
                )
            ]
            style_answer = inquirer.prompt(questions)
            if style_answer and ""style"" in style_answer:
                selected_choice = style_answer[""style""]
                for style_name, (file_name, author_link) in available_styles.items():
                    if selected_choice.startswith(style_name):
                        style_manager.set_selected_style(style_name)
                        logger.info(f""Selected style: {style_name}"")
                        break
            else:
                logger.warning(""No style selected. Proceeding with default style."")

        # Initialize the Resume Generator
        resume_generator = ResumeGenerator()
        resume_object = Resume(plain_text_resume)
        driver = init_browser()
        resume_generator.set_resume_object(resume_object)

        # Create the ResumeFacade
        resume_facade = ResumeFacade(
            api_key=llm_api_key,
            style_manager=style_manager,
            resume_generator=resume_generator,
            resume_object=resume_object,
            output_path=Path(""data_folder/output""),
        )
        resume_facade.set_driver(driver)
        result_base64 = resume_facade.create_resume_pdf()

        # Decode Base64 to binary data
        try:
            pdf_data = base64.b64decode(result_base64)
        except base64.binascii.Error as e:
            logger.error(""Error decoding Base64: %s"", e)
            raise

        # Define the output directory using `suggested_name`
        output_dir = Path(parameters[""outputFileDirectory""])

        # Write the PDF file
        output_path = output_dir / ""resume_base.pdf""
        try:
            with open(output_path, ""wb"") as file:
                file.write(pdf_data)
            logger.info(f""Resume saved at: {output_path}"")
        except IOError as e:
            logger.error(""Error writing file: %s"", e)
            raise
    except Exception as e:
        logger.exception(f""An error occurred while creating the CV: {e}"")
        raise

        
def handle_inquiries(selected_actions: List[str], parameters: dict, llm_api_key: str):
    """"""
    Decide which function to call based on the selected user actions.

    :param selected_actions: List of actions selected by the user.
    :param parameters: Configuration parameters dictionary.
    :param llm_api_key: API key for the language model.
    """"""
    try:
        if selected_actions:
            if ""Generate Resume"" == selected_actions:
                logger.info(""Crafting a standout professional resume..."")
                create_resume_pdf(parameters, llm_api_key)
                
            if ""Generate Resume Tailored for Job Description"" == selected_actions:
                logger.info(""Customizing your resume to enhance your job application..."")
                create_resume_pdf_job_tailored(parameters, llm_api_key)
                
            if ""Generate Tailored Cover Letter for Job Description"" == selected_actions:
                logger.info(""Designing a personalized cover letter to enhance your job application..."")
                create_cover_letter(parameters, llm_api_key)

        else:
            logger.warning(""No actions selected. Nothing to execute."")
    except Exception as e:
        logger.exception(f""An error occurred while handling inquiries: {e}"")
        raise

def prompt_user_action() -> str:
    """"""
    Use inquirer to ask the user which action they want to perform.

    :return: Selected action.
    """"""
    try:
        questions = [
            inquirer.List(
                'action',
                message=""Select the action you want to perform:"",
                choices=[
                    ""Generate Resume"",
                    ""Generate Resume Tailored for Job Description"",
                    ""Generate Tailored Cover Letter for Job Description"",
                ],
            ),
        ]
        answer = inquirer.prompt(questions)
        if answer is None:
            print(""No answer provided. The user may have interrupted."")
            return """"
        return answer.get('action', """")
    except Exception as e:
        print(f""An error occurred: {e}"")
        return """"


def main():
    """"""Main entry point for the AIHawk Job Application Bot.""""""
    try:
        # Define and validate the data folder
        data_folder = Path(""data_folder"")
        secrets_file, config_file, plain_text_resume_file, output_folder = FileManager.validate_data_folder(data_folder)

        # Validate configuration and secrets
        config = ConfigValidator.validate_config(config_file)
        llm_api_key = ConfigValidator.validate_secrets(secrets_file)

        # Prepare parameters
        config[""uploads""] = FileManager.get_uploads(plain_text_resume_file)
        config[""outputFileDirectory""] = output_folder

        # Interactive prompt for user to select actions
        selected_actions = prompt_user_action()

        # Handle selected actions and execute them
        handle_inquiries(selected_actions, config, llm_api_key)

    except ConfigError as ce:
        logger.error(f""Configuration error: {ce}"")
        logger.error(
            ""Refer to the configuration guide for troubleshooting: ""
            ""https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk?tab=readme-ov-file#configuration""
        )
    except FileNotFoundError as fnf:
        logger.error(f""File not found: {fnf}"")
        logger.error(""Ensure all required files are present in the data folder."")
    except RuntimeError as re:
        logger.error(f""Runtime error: {re}"")
        logger.debug(traceback.format_exc())
    except Exception as e:
        logger.exception(f""An unexpected error occurred: {e}"")


if __name__ == ""__main__"":
    main()
"
RVC-Project_Retrieval-based-Voice-Conversion-WebUI,RVC-Project_Retrieval-based-Voice-Conversion-WebUI_api_231006.py,19023,https://raw.githubusercontent.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/main/api_231006.py,"#api for 231006 release version by Xiaokai
import os
import sys
import json
import re
import time
import librosa
import torch
import numpy as np
import torch.nn.functional as F
import torchaudio.transforms as tat
import sounddevice as sd
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import threading
import uvicorn
import logging

# Initialize the logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define FastAPI app
app = FastAPI()

class GUIConfig:
    def __init__(self) -> None:
        self.pth_path: str = """"
        self.index_path: str = """"
        self.pitch: int = 0
        self.samplerate: int = 40000
        self.block_time: float = 1.0  # s
        self.buffer_num: int = 1
        self.threhold: int = -60
        self.crossfade_time: float = 0.05
        self.extra_time: float = 2.5
        self.I_noise_reduce = False
        self.O_noise_reduce = False
        self.rms_mix_rate = 0.0
        self.index_rate = 0.3
        self.f0method = ""rmvpe""
        self.sg_input_device = """"
        self.sg_output_device = """"

class ConfigData(BaseModel):
    pth_path: str
    index_path: str
    sg_input_device: str
    sg_output_device: str
    threhold: int = -60
    pitch: int = 0
    index_rate: float = 0.3
    rms_mix_rate: float = 0.0
    block_time: float = 0.25
    crossfade_length: float = 0.05
    extra_time: float = 2.5
    n_cpu: int = 4
    I_noise_reduce: bool = False
    O_noise_reduce: bool = False

class AudioAPI:
    def __init__(self) -> None:
        self.gui_config = GUIConfig()
        self.config = None  # Initialize Config object as None
        self.flag_vc = False
        self.function = ""vc""
        self.delay_time = 0
        self.rvc = None  # Initialize RVC object as None

    def load(self):
        input_devices, output_devices, _, _ = self.get_devices()
        try:
            with open(""configs/config.json"", ""r"", encoding='utf-8') as j:
                data = json.load(j)
                data[""rmvpe""] = True  # Ensure rmvpe is the only f0method
                if data[""sg_input_device""] not in input_devices:
                    data[""sg_input_device""] = input_devices[sd.default.device[0]]
                if data[""sg_output_device""] not in output_devices:
                    data[""sg_output_device""] = output_devices[sd.default.device[1]]
        except Exception as e:
            logger.error(f""Failed to load configuration: {e}"")
            with open(""configs/config.json"", ""w"", encoding='utf-8') as j:
                data = {
                    ""pth_path"": "" "",
                    ""index_path"": "" "",
                    ""sg_input_device"": input_devices[sd.default.device[0]],
                    ""sg_output_device"": output_devices[sd.default.device[1]],
                    ""threhold"": ""-60"",
                    ""pitch"": ""0"",
                    ""index_rate"": ""0"",
                    ""rms_mix_rate"": ""0"",
                    ""block_time"": ""0.25"",
                    ""crossfade_length"": ""0.05"",
                    ""extra_time"": ""2.5"",
                    ""f0method"": ""rmvpe"",
                    ""use_jit"": False,
                }
                data[""rmvpe""] = True  # Ensure rmvpe is the only f0method
                json.dump(data, j, ensure_ascii=False)
        return data

    def set_values(self, values):
        logger.info(f""Setting values: {values}"")
        if not values.pth_path.strip():
            raise HTTPException(status_code=400, detail=""Please select a .pth file"")
        if not values.index_path.strip():
            raise HTTPException(status_code=400, detail=""Please select an index file"")
        self.set_devices(values.sg_input_device, values.sg_output_device)
        self.config.use_jit = False
        self.gui_config.pth_path = values.pth_path
        self.gui_config.index_path = values.index_path
        self.gui_config.threhold = values.threhold
        self.gui_config.pitch = values.pitch
        self.gui_config.block_time = values.block_time
        self.gui_config.crossfade_time = values.crossfade_length
        self.gui_config.extra_time = values.extra_time
        self.gui_config.I_noise_reduce = values.I_noise_reduce
        self.gui_config.O_noise_reduce = values.O_noise_reduce
        self.gui_config.rms_mix_rate = values.rms_mix_rate
        self.gui_config.index_rate = values.index_rate
        self.gui_config.n_cpu = values.n_cpu
        self.gui_config.f0method = ""rmvpe""
        return True

    def start_vc(self):
        torch.cuda.empty_cache()
        self.flag_vc = True
        self.rvc = rvc_for_realtime.RVC(
            self.gui_config.pitch,
            self.gui_config.pth_path,
            self.gui_config.index_path,
            self.gui_config.index_rate,
            0,
            0,
            0,
            self.config,
            self.rvc if self.rvc else None,
        )
        self.gui_config.samplerate = self.rvc.tgt_sr
        self.zc = self.rvc.tgt_sr // 100
        self.block_frame = (
            int(
                np.round(
                    self.gui_config.block_time
                    * self.gui_config.samplerate
                    / self.zc
                )
            )
            * self.zc
        )
        self.block_frame_16k = 160 * self.block_frame // self.zc
        self.crossfade_frame = (
            int(
                np.round(
                    self.gui_config.crossfade_time
                    * self.gui_config.samplerate
                    / self.zc
                )
            )
            * self.zc
        )
        self.sola_search_frame = self.zc
        self.extra_frame = (
            int(
                np.round(
                    self.gui_config.extra_time
                    * self.gui_config.samplerate
                    / self.zc
                )
            )
            * self.zc
        )
        self.input_wav = torch.zeros(
            self.extra_frame + self.crossfade_frame + self.sola_search_frame + self.block_frame,
            device=self.config.device,
            dtype=torch.float32,
        )
        self.input_wav_res = torch.zeros(
            160 * self.input_wav.shape[0] // self.zc,
            device=self.config.device,
            dtype=torch.float32,
        )
        self.pitch = np.zeros(self.input_wav.shape[0] // self.zc, dtype=""int32"")
        self.pitchf = np.zeros(self.input_wav.shape[0] // self.zc, dtype=""float64"")
        self.sola_buffer = torch.zeros(self.crossfade_frame, device=self.config.device, dtype=torch.float32)
        self.nr_buffer = self.sola_buffer.clone()
        self.output_buffer = self.input_wav.clone()
        self.res_buffer = torch.zeros(2 * self.zc, device=self.config.device, dtype=torch.float32)
        self.valid_rate = 1 - (self.extra_frame - 1) / self.input_wav.shape[0]
        self.fade_in_window = (
            torch.sin(0.5 * np.pi * torch.linspace(0.0, 1.0, steps=self.crossfade_frame, device=self.config.device, dtype=torch.float32)) ** 2
        )
        self.fade_out_window = 1 - self.fade_in_window
        self.resampler = tat.Resample(
            orig_freq=self.gui_config.samplerate,
            new_freq=16000,
            dtype=torch.float32,
        ).to(self.config.device)
        self.tg = TorchGate(
            sr=self.gui_config.samplerate, n_fft=4 * self.zc, prop_decrease=0.9
        ).to(self.config.device)
        thread_vc = threading.Thread(target=self.soundinput)
        thread_vc.start()

    def soundinput(self):
        channels = 1 if sys.platform == ""darwin"" else 2
        with sd.Stream(
            channels=channels,
            callback=self.audio_callback,
            blocksize=self.block_frame,
            samplerate=self.gui_config.samplerate,
            dtype=""float32"",
        ) as stream:
            global stream_latency
            stream_latency = stream.latency[-1]
            while self.flag_vc:
                time.sleep(self.gui_config.block_time)
                logger.info(""Audio block passed."")
        logger.info(""Ending VC"")

    def audio_callback(self, indata: np.ndarray, outdata: np.ndarray, frames, times, status):
        start_time = time.perf_counter()
        indata = librosa.to_mono(indata.T)
        if self.gui_config.threhold > -60:
            rms = librosa.feature.rms(y=indata, frame_length=4 * self.zc, hop_length=self.zc)
            db_threhold = (librosa.amplitude_to_db(rms, ref=1.0)[0] < self.gui_config.threhold)
            for i in range(db_threhold.shape[0]):
                if db_threhold[i]:
                    indata[i * self.zc : (i + 1) * self.zc] = 0
        self.input_wav[: -self.block_frame] = self.input_wav[self.block_frame :].clone()
        self.input_wav[-self.block_frame :] = torch.from_numpy(indata).to(self.config.device)
        self.input_wav_res[: -self.block_frame_16k] = self.input_wav_res[self.block_frame_16k :].clone()
        if self.gui_config.I_noise_reduce and self.function == ""vc"":
            input_wav = self.input_wav[-self.crossfade_frame - self.block_frame - 2 * self.zc :]
            input_wav = self.tg(input_wav.unsqueeze(0), self.input_wav.unsqueeze(0))[0, 2 * self.zc :]
            input_wav[: self.crossfade_frame] *= self.fade_in_window
            input_wav[: self.crossfade_frame] += self.nr_buffer * self.fade_out_window
            self.nr_buffer[:] = input_wav[-self.crossfade_frame :]
            input_wav = torch.cat((self.res_buffer[:], input_wav[: self.block_frame]))
            self.res_buffer[:] = input_wav[-2 * self.zc :]
            self.input_wav_res[-self.block_frame_16k - 160 :] = self.resampler(input_wav)[160:]
        else:
            self.input_wav_res[-self.block_frame_16k - 160 :] = self.resampler(self.input_wav[-self.block_frame - 2 * self.zc :])[160:]
        if self.function == ""vc"":
            f0_extractor_frame = self.block_frame_16k + 800
            if self.gui_config.f0method == ""rmvpe"":
                f0_extractor_frame = (5120 * ((f0_extractor_frame - 1) // 5120 + 1) - 160)
            infer_wav = self.rvc.infer(
                self.input_wav_res,
                self.input_wav_res[-f0_extractor_frame:].cpu().numpy(),
                self.block_frame_16k,
                self.valid_rate,
                self.pitch,
                self.pitchf,
                self.gui_config.f0method,
            )
            infer_wav = infer_wav[-self.crossfade_frame - self.sola_search_frame - self.block_frame :]
        else:
            infer_wav = self.input_wav[-self.crossfade_frame - self.sola_search_frame - self.block_frame :].clone()
        if (self.gui_config.O_noise_reduce and self.function == ""vc"") or (self.gui_config.I_noise_reduce and self.function == ""im""):
            self.output_buffer[: -self.block_frame] = self.output_buffer[self.block_frame :].clone()
            self.output_buffer[-self.block_frame :] = infer_wav[-self.block_frame :]
            infer_wav = self.tg(infer_wav.unsqueeze(0), self.output_buffer.unsqueeze(0)).squeeze(0)
        if self.gui_config.rms_mix_rate < 1 and self.function == ""vc"":
            rms1 = librosa.feature.rms(y=self.input_wav_res[-160 * infer_wav.shape[0] // self.zc :].cpu().numpy(), frame_length=640, hop_length=160)
            rms1 = torch.from_numpy(rms1).to(self.config.device)
            rms1 = F.interpolate(rms1.unsqueeze(0), size=infer_wav.shape[0] + 1, mode=""linear"", align_corners=True)[0, 0, :-1]
            rms2 = librosa.feature.rms(y=infer_wav[:].cpu().numpy(), frame_length=4 * self.zc, hop_length=self.zc)
            rms2 = torch.from_numpy(rms2).to(self.config.device)
            rms2 = F.interpolate(rms2.unsqueeze(0), size=infer_wav.shape[0] + 1, mode=""linear"", align_corners=True)[0, 0, :-1]
            rms2 = torch.max(rms2, torch.zeros_like(rms2) + 1e-3)
            infer_wav *= torch.pow(rms1 / rms2, torch.tensor(1 - self.gui_config.rms_mix_rate))
        conv_input = infer_wav[None, None, : self.crossfade_frame + self.sola_search_frame]
        cor_nom = F.conv1d(conv_input, self.sola_buffer[None, None, :])
        cor_den = torch.sqrt(F.conv1d(conv_input**2, torch.ones(1, 1, self.crossfade_frame, device=self.config.device)) + 1e-8)
        if sys.platform == ""darwin"":
            _, sola_offset = torch.max(cor_nom[0, 0] / cor_den[0, 0])
            sola_offset = sola_offset.item()
        else:
            sola_offset = torch.argmax(cor_nom[0, 0] / cor_den[0, 0])
        logger.info(f""sola_offset = {sola_offset}"")
        infer_wav = infer_wav[sola_offset : sola_offset + self.block_frame + self.crossfade_frame]
        infer_wav[: self.crossfade_frame] *= self.fade_in_window
        infer_wav[: self.crossfade_frame] += self.sola_buffer * self.fade_out_window
        self.sola_buffer[:] = infer_wav[-self.crossfade_frame :]
        if sys.platform == ""darwin"":
            outdata[:] = infer_wav[: -self.crossfade_frame].cpu().numpy()[:, np.newaxis]
        else:
            outdata[:] = infer_wav[: -self.crossfade_frame].repeat(2, 1).t().cpu().numpy()
        total_time = time.perf_counter() - start_time
        logger.info(f""Infer time: {total_time:.2f}"")

    def get_devices(self, update: bool = True):
        if update:
            sd._terminate()
            sd._initialize()
        devices = sd.query_devices()
        hostapis = sd.query_hostapis()
        for hostapi in hostapis:
            for device_idx in hostapi[""devices""]:
                devices[device_idx][""hostapi_name""] = hostapi[""name""]
        input_devices = [
            f""{d['name']} ({d['hostapi_name']})""
            for d in devices
            if d[""max_input_channels""] > 0
        ]
        output_devices = [
            f""{d['name']} ({d['hostapi_name']})""
            for d in devices
            if d[""max_output_channels""] > 0
        ]
        input_devices_indices = [
            d[""index""] if ""index"" in d else d[""name""]
            for d in devices
            if d[""max_input_channels""] > 0
        ]
        output_devices_indices = [
            d[""index""] if ""index"" in d else d[""name""]
            for d in devices
            if d[""max_output_channels""] > 0
        ]
        return (
            input_devices,
            output_devices,
            input_devices_indices,
            output_devices_indices,
        )

    def set_devices(self, input_device, output_device):
        (
            input_devices,
            output_devices,
            input_device_indices,
            output_device_indices,
        ) = self.get_devices()
        logger.debug(f""Available input devices: {input_devices}"")
        logger.debug(f""Available output devices: {output_devices}"")
        logger.debug(f""Selected input device: {input_device}"")
        logger.debug(f""Selected output device: {output_device}"")

        if input_device not in input_devices:
            logger.error(f""Input device '{input_device}' is not in the list of available devices"")
            raise HTTPException(status_code=400, detail=f""Input device '{input_device}' is not available"")
        
        if output_device not in output_devices:
            logger.error(f""Output device '{output_device}' is not in the list of available devices"")
            raise HTTPException(status_code=400, detail=f""Output device '{output_device}' is not available"")

        sd.default.device[0] = input_device_indices[input_devices.index(input_device)]
        sd.default.device[1] = output_device_indices[output_devices.index(output_device)]
        logger.info(f""Input device set to {sd.default.device[0]}: {input_device}"")
        logger.info(f""Output device set to {sd.default.device[1]}: {output_device}"")

audio_api = AudioAPI()

@app.get(""/inputDevices"", response_model=list)
def get_input_devices():
    try:
        input_devices, _, _, _ = audio_api.get_devices()
        return input_devices
    except Exception as e:
        logger.error(f""Failed to get input devices: {e}"")
        raise HTTPException(status_code=500, detail=""Failed to get input devices"")

@app.get(""/outputDevices"", response_model=list)
def get_output_devices():
    try:
        _, output_devices, _, _ = audio_api.get_devices()
        return output_devices
    except Exception as e:
        logger.error(f""Failed to get output devices: {e}"")
        raise HTTPException(status_code=500, detail=""Failed to get output devices"")

@app.post(""/config"")
def configure_audio(config_data: ConfigData):
    try:
        logger.info(f""Configuring audio with data: {config_data}"")
        if audio_api.set_values(config_data):
            settings = config_data.dict()
            settings[""use_jit""] = False
            settings[""f0method""] = ""rmvpe""
            with open(""configs/config.json"", ""w"", encoding='utf-8') as j:
                json.dump(settings, j, ensure_ascii=False)
            logger.info(""Configuration set successfully"")
            return {""message"": ""Configuration set successfully""}
    except HTTPException as e:
        logger.error(f""Configuration error: {e.detail}"")
        raise
    except Exception as e:
        logger.error(f""Configuration failed: {e}"")
        raise HTTPException(status_code=400, detail=f""Configuration failed: {e}"")

@app.post(""/start"")
def start_conversion():
    try:
        if not audio_api.flag_vc:
            audio_api.start_vc()
            return {""message"": ""Audio conversion started""}
        else:
            logger.warning(""Audio conversion already running"")
            raise HTTPException(status_code=400, detail=""Audio conversion already running"")
    except HTTPException as e:
        logger.error(f""Start conversion error: {e.detail}"")
        raise
    except Exception as e:
        logger.error(f""Failed to start conversion: {e}"")
        raise HTTPException(status_code=500, detail=f""Failed to start conversion: {e}"")

@app.post(""/stop"")
def stop_conversion():
    try:
        if audio_api.flag_vc:
            audio_api.flag_vc = False
            global stream_latency
            stream_latency = -1
            return {""message"": ""Audio conversion stopped""}
        else:
            logger.warning(""Audio conversion not running"")
            raise HTTPException(status_code=400, detail=""Audio conversion not running"")
    except HTTPException as e:
        logger.error(f""Stop conversion error: {e.detail}"")
        raise
    except Exception as e:
        logger.error(f""Failed to stop conversion: {e}"")
        raise HTTPException(status_code=500, detail=f""Failed to stop conversion: {e}"")

if __name__ == ""__main__"":
    if sys.platform == ""win32"":
        from multiprocessing import freeze_support
        freeze_support()
    load_dotenv()
    os.environ[""OMP_NUM_THREADS""] = ""4""
    if sys.platform == ""darwin"":
        os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""
    from tools.torchgate import TorchGate
    import tools.rvc_for_realtime as rvc_for_realtime
    from configs.config import Config
    audio_api.config = Config()
    uvicorn.run(app, host=""0.0.0.0"", port=6242)
"
RVC-Project_Retrieval-based-Voice-Conversion-WebUI,RVC-Project_Retrieval-based-Voice-Conversion-WebUI_api_240604.py,22181,https://raw.githubusercontent.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/main/api_240604.py,"#api for 240604 release version by Xiaokai
import os
import sys
import json
import re
import time
import librosa
import torch
import numpy as np
import torch.nn.functional as F
import torchaudio.transforms as tat
import sounddevice as sd
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import threading
import uvicorn
import logging
from multiprocessing import Queue, Process, cpu_count, freeze_support

# Initialize the logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define FastAPI app
app = FastAPI()

class GUIConfig:
    def __init__(self) -> None:
        self.pth_path: str = """"
        self.index_path: str = """"
        self.pitch: int = 0
        self.formant: float = 0.0
        self.sr_type: str = ""sr_model""
        self.block_time: float = 0.25  # s
        self.threhold: int = -60
        self.crossfade_time: float = 0.05
        self.extra_time: float = 2.5
        self.I_noise_reduce: bool = False
        self.O_noise_reduce: bool = False
        self.use_pv: bool = False
        self.rms_mix_rate: float = 0.0
        self.index_rate: float = 0.0
        self.n_cpu: int = 4
        self.f0method: str = ""fcpe""
        self.sg_input_device: str = """"
        self.sg_output_device: str = """"

class ConfigData(BaseModel):
    pth_path: str
    index_path: str
    sg_input_device: str
    sg_output_device: str
    threhold: int = -60
    pitch: int = 0
    formant: float = 0.0
    index_rate: float = 0.3
    rms_mix_rate: float = 0.0
    block_time: float = 0.25
    crossfade_length: float = 0.05
    extra_time: float = 2.5
    n_cpu: int = 4
    I_noise_reduce: bool = False
    O_noise_reduce: bool = False
    use_pv: bool = False
    f0method: str = ""fcpe""

class Harvest(Process):
    def __init__(self, inp_q, opt_q):
        super(Harvest, self).__init__()
        self.inp_q = inp_q
        self.opt_q = opt_q

    def run(self):
        import numpy as np
        import pyworld
        while True:
            idx, x, res_f0, n_cpu, ts = self.inp_q.get()
            f0, t = pyworld.harvest(
                x.astype(np.double),
                fs=16000,
                f0_ceil=1100,
                f0_floor=50,
                frame_period=10,
            )
            res_f0[idx] = f0
            if len(res_f0.keys()) >= n_cpu:
                self.opt_q.put(ts)

class AudioAPI:
    def __init__(self) -> None:
        self.gui_config = GUIConfig()
        self.config = None  # Initialize Config object as None
        self.flag_vc = False
        self.function = ""vc""
        self.delay_time = 0
        self.rvc = None  # Initialize RVC object as None
        self.inp_q = None
        self.opt_q = None
        self.n_cpu = min(cpu_count(), 8)

    def initialize_queues(self):
        self.inp_q = Queue()
        self.opt_q = Queue()
        for _ in range(self.n_cpu):
            p = Harvest(self.inp_q, self.opt_q)
            p.daemon = True
            p.start()

    def load(self):
        input_devices, output_devices, _, _ = self.get_devices()
        try:
            with open(""configs/config.json"", ""r"", encoding='utf-8') as j:
                data = json.load(j)
                if data[""sg_input_device""] not in input_devices:
                    data[""sg_input_device""] = input_devices[sd.default.device[0]]
                if data[""sg_output_device""] not in output_devices:
                    data[""sg_output_device""] = output_devices[sd.default.device[1]]
        except Exception as e:
            logger.error(f""Failed to load configuration: {e}"")
            with open(""configs/config.json"", ""w"", encoding='utf-8') as j:
                data = {
                    ""pth_path"": """",
                    ""index_path"": """",
                    ""sg_input_device"": input_devices[sd.default.device[0]],
                    ""sg_output_device"": output_devices[sd.default.device[1]],
                    ""threhold"": -60,
                    ""pitch"": 0,
                    ""formant"": 0.0,
                    ""index_rate"": 0,
                    ""rms_mix_rate"": 0,
                    ""block_time"": 0.25,
                    ""crossfade_length"": 0.05,
                    ""extra_time"": 2.5,
                    ""n_cpu"": 4,
                    ""f0method"": ""fcpe"",
                    ""use_jit"": False,
                    ""use_pv"": False,
                }
                json.dump(data, j, ensure_ascii=False)
        return data

    def set_values(self, values):
        logger.info(f""Setting values: {values}"")
        if not values.pth_path.strip():
            raise HTTPException(status_code=400, detail=""Please select a .pth file"")
        if not values.index_path.strip():
            raise HTTPException(status_code=400, detail=""Please select an index file"")
        self.set_devices(values.sg_input_device, values.sg_output_device)
        self.config.use_jit = False
        self.gui_config.pth_path = values.pth_path
        self.gui_config.index_path = values.index_path
        self.gui_config.threhold = values.threhold
        self.gui_config.pitch = values.pitch
        self.gui_config.formant = values.formant
        self.gui_config.block_time = values.block_time
        self.gui_config.crossfade_time = values.crossfade_length
        self.gui_config.extra_time = values.extra_time
        self.gui_config.I_noise_reduce = values.I_noise_reduce
        self.gui_config.O_noise_reduce = values.O_noise_reduce
        self.gui_config.rms_mix_rate = values.rms_mix_rate
        self.gui_config.index_rate = values.index_rate
        self.gui_config.n_cpu = values.n_cpu
        self.gui_config.use_pv = values.use_pv
        self.gui_config.f0method = values.f0method
        return True

    def start_vc(self):
        torch.cuda.empty_cache()
        self.flag_vc = True
        self.rvc = rvc_for_realtime.RVC(
            self.gui_config.pitch,
            self.gui_config.pth_path,
            self.gui_config.index_path,
            self.gui_config.index_rate,
            self.gui_config.n_cpu,
            self.inp_q,
            self.opt_q,
            self.config,
            self.rvc if self.rvc else None,
        )
        self.gui_config.samplerate = (
            self.rvc.tgt_sr
            if self.gui_config.sr_type == ""sr_model""
            else self.get_device_samplerate()
        )
        self.zc = self.gui_config.samplerate // 100
        self.block_frame = (
            int(
                np.round(
                    self.gui_config.block_time
                    * self.gui_config.samplerate
                    / self.zc
                )
            )
            * self.zc
        )
        self.block_frame_16k = 160 * self.block_frame // self.zc
        self.crossfade_frame = (
            int(
                np.round(
                    self.gui_config.crossfade_time
                    * self.gui_config.samplerate
                    / self.zc
                )
            )
            * self.zc
        )
        self.sola_buffer_frame = min(self.crossfade_frame, 4 * self.zc)
        self.sola_search_frame = self.zc
        self.extra_frame = (
            int(
                np.round(
                    self.gui_config.extra_time
                    * self.gui_config.samplerate
                    / self.zc
                )
            )
            * self.zc
        )
        self.input_wav = torch.zeros(
            self.extra_frame
            + self.crossfade_frame
            + self.sola_search_frame
            + self.block_frame,
            device=self.config.device,
            dtype=torch.float32,
        )
        self.input_wav_denoise = self.input_wav.clone()
        self.input_wav_res = torch.zeros(
            160 * self.input_wav.shape[0] // self.zc,
            device=self.config.device,
            dtype=torch.float32,
        )
        self.rms_buffer = np.zeros(4 * self.zc, dtype=""float32"")
        self.sola_buffer = torch.zeros(
            self.sola_buffer_frame, device=self.config.device, dtype=torch.float32
        )
        self.nr_buffer = self.sola_buffer.clone()
        self.output_buffer = self.input_wav.clone()
        self.skip_head = self.extra_frame // self.zc
        self.return_length = (
            self.block_frame + self.sola_buffer_frame + self.sola_search_frame
        ) // self.zc
        self.fade_in_window = (
            torch.sin(
                0.5
                * np.pi
                * torch.linspace(
                    0.0,
                    1.0,
                    steps=self.sola_buffer_frame,
                    device=self.config.device,
                    dtype=torch.float32,
                )
            )
            ** 2
        )
        self.fade_out_window = 1 - self.fade_in_window
        self.resampler = tat.Resample(
            orig_freq=self.gui_config.samplerate,
            new_freq=16000,
            dtype=torch.float32,
        ).to(self.config.device)
        if self.rvc.tgt_sr != self.gui_config.samplerate:
            self.resampler2 = tat.Resample(
                orig_freq=self.rvc.tgt_sr,
                new_freq=self.gui_config.samplerate,
                dtype=torch.float32,
            ).to(self.config.device)
        else:
            self.resampler2 = None
        self.tg = TorchGate(
            sr=self.gui_config.samplerate, n_fft=4 * self.zc, prop_decrease=0.9
        ).to(self.config.device)
        thread_vc = threading.Thread(target=self.soundinput)
        thread_vc.start()

    def soundinput(self):
        channels = 1 if sys.platform == ""darwin"" else 2
        with sd.Stream(
            channels=channels,
            callback=self.audio_callback,
            blocksize=self.block_frame,
            samplerate=self.gui_config.samplerate,
            dtype=""float32"",
        ) as stream:
            global stream_latency
            stream_latency = stream.latency[-1]
            while self.flag_vc:
                time.sleep(self.gui_config.block_time)
                logger.info(""Audio block passed."")
        logger.info(""Ending VC"")

    def audio_callback(self, indata: np.ndarray, outdata: np.ndarray, frames, times, status):
        start_time = time.perf_counter()
        indata = librosa.to_mono(indata.T)
        if self.gui_config.threhold > -60:
            indata = np.append(self.rms_buffer, indata)
            rms = librosa.feature.rms(y=indata, frame_length=4 * self.zc, hop_length=self.zc)[:, 2:]
            self.rms_buffer[:] = indata[-4 * self.zc :]
            indata = indata[2 * self.zc - self.zc // 2 :]
            db_threhold = (
                librosa.amplitude_to_db(rms, ref=1.0)[0] < self.gui_config.threhold
            )
            for i in range(db_threhold.shape[0]):
                if db_threhold[i]:
                    indata[i * self.zc : (i + 1) * self.zc] = 0
            indata = indata[self.zc // 2 :]
        self.input_wav[: -self.block_frame] = self.input_wav[self.block_frame :].clone()
        self.input_wav[-indata.shape[0] :] = torch.from_numpy(indata).to(self.config.device)
        self.input_wav_res[: -self.block_frame_16k] = self.input_wav_res[self.block_frame_16k :].clone()
        # input noise reduction and resampling
        if self.gui_config.I_noise_reduce:
            self.input_wav_denoise[: -self.block_frame] = self.input_wav_denoise[self.block_frame :].clone()
            input_wav = self.input_wav[-self.sola_buffer_frame - self.block_frame :]
            input_wav = self.tg(input_wav.unsqueeze(0), self.input_wav.unsqueeze(0)).squeeze(0)
            input_wav[: self.sola_buffer_frame] *= self.fade_in_window
            input_wav[: self.sola_buffer_frame] += self.nr_buffer * self.fade_out_window
            self.input_wav_denoise[-self.block_frame :] = input_wav[: self.block_frame]
            self.nr_buffer[:] = input_wav[self.block_frame :]
            self.input_wav_res[-self.block_frame_16k - 160 :] = self.resampler(
                self.input_wav_denoise[-self.block_frame - 2 * self.zc :]
            )[160:]
        else:
            self.input_wav_res[-160 * (indata.shape[0] // self.zc + 1) :] = (
                self.resampler(self.input_wav[-indata.shape[0] - 2 * self.zc :])[160:]
            )
        # infer
        if self.function == ""vc"":
            infer_wav = self.rvc.infer(
                self.input_wav_res,
                self.block_frame_16k,
                self.skip_head,
                self.return_length,
                self.gui_config.f0method,
            )
            if self.resampler2 is not None:
                infer_wav = self.resampler2(infer_wav)
        elif self.gui_config.I_noise_reduce:
            infer_wav = self.input_wav_denoise[self.extra_frame :].clone()
        else:
            infer_wav = self.input_wav[self.extra_frame :].clone()
        # output noise reduction
        if self.gui_config.O_noise_reduce and self.function == ""vc"":
            self.output_buffer[: -self.block_frame] = self.output_buffer[self.block_frame :].clone()
            self.output_buffer[-self.block_frame :] = infer_wav[-self.block_frame :]
            infer_wav = self.tg(infer_wav.unsqueeze(0), self.output_buffer.unsqueeze(0)).squeeze(0)
        # volume envelop mixing
        if self.gui_config.rms_mix_rate < 1 and self.function == ""vc"":
            if self.gui_config.I_noise_reduce:
                input_wav = self.input_wav_denoise[self.extra_frame :]
            else:
                input_wav = self.input_wav[self.extra_frame :]
            rms1 = librosa.feature.rms(
                y=input_wav[: infer_wav.shape[0]].cpu().numpy(),
                frame_length=4 * self.zc,
                hop_length=self.zc,
            )
            rms1 = torch.from_numpy(rms1).to(self.config.device)
            rms1 = F.interpolate(
                rms1.unsqueeze(0),
                size=infer_wav.shape[0] + 1,
                mode=""linear"",
                align_corners=True,
            )[0, 0, :-1]
            rms2 = librosa.feature.rms(
                y=infer_wav[:].cpu().numpy(),
                frame_length=4 * self.zc,
                hop_length=self.zc,
            )
            rms2 = torch.from_numpy(rms2).to(self.config.device)
            rms2 = F.interpolate(
                rms2.unsqueeze(0),
                size=infer_wav.shape[0] + 1,
                mode=""linear"",
                align_corners=True,
            )[0, 0, :-1]
            rms2 = torch.max(rms2, torch.zeros_like(rms2) + 1e-3)
            infer_wav *= torch.pow(
                rms1 / rms2, torch.tensor(1 - self.gui_config.rms_mix_rate)
            )
        # SOLA algorithm from https://github.com/yxlllc/DDSP-SVC
        conv_input = infer_wav[None, None, : self.sola_buffer_frame + self.sola_search_frame]
        cor_nom = F.conv1d(conv_input, self.sola_buffer[None, None, :])
        cor_den = torch.sqrt(
            F.conv1d(
                conv_input**2,
                torch.ones(1, 1, self.sola_buffer_frame, device=self.config.device),
            )
            + 1e-8
        )
        if sys.platform == ""darwin"":
            _, sola_offset = torch.max(cor_nom[0, 0] / cor_den[0, 0])
            sola_offset = sola_offset.item()
        else:
            sola_offset = torch.argmax(cor_nom[0, 0] / cor_den[0, 0])
        logger.info(f""sola_offset = {sola_offset}"")
        infer_wav = infer_wav[sola_offset:]
        if ""privateuseone"" in str(self.config.device) or not self.gui_config.use_pv:
            infer_wav[: self.sola_buffer_frame] *= self.fade_in_window
            infer_wav[: self.sola_buffer_frame] += self.sola_buffer * self.fade_out_window
        else:
            infer_wav[: self.sola_buffer_frame] = phase_vocoder(
                self.sola_buffer,
                infer_wav[: self.sola_buffer_frame],
                self.fade_out_window,
                self.fade_in_window,
            )
        self.sola_buffer[:] = infer_wav[
            self.block_frame : self.block_frame + self.sola_buffer_frame
        ]
        if sys.platform == ""darwin"":
            outdata[:] = infer_wav[: self.block_frame].cpu().numpy()[:, np.newaxis]
        else:
            outdata[:] = infer_wav[: self.block_frame].repeat(2, 1).t().cpu().numpy()
        total_time = time.perf_counter() - start_time
        logger.info(f""Infer time: {total_time:.2f}"")

    def get_devices(self, update: bool = True):
        if update:
            sd._terminate()
            sd._initialize()
        devices = sd.query_devices()
        hostapis = sd.query_hostapis()
        for hostapi in hostapis:
            for device_idx in hostapi[""devices""]:
                devices[device_idx][""hostapi_name""] = hostapi[""name""]
        input_devices = [
            f""{d['name']} ({d['hostapi_name']})""
            for d in devices
            if d[""max_input_channels""] > 0
        ]
        output_devices = [
            f""{d['name']} ({d['hostapi_name']})""
            for d in devices
            if d[""max_output_channels""] > 0
        ]
        input_devices_indices = [
            d[""index""] if ""index"" in d else d[""name""]
            for d in devices
            if d[""max_input_channels""] > 0
        ]
        output_devices_indices = [
            d[""index""] if ""index"" in d else d[""name""]
            for d in devices
            if d[""max_output_channels""] > 0
        ]
        return (
            input_devices,
            output_devices,
            input_devices_indices,
            output_devices_indices,
        )

    def set_devices(self, input_device, output_device):
        (
            input_devices,
            output_devices,
            input_device_indices,
            output_device_indices,
        ) = self.get_devices()
        logger.debug(f""Available input devices: {input_devices}"")
        logger.debug(f""Available output devices: {output_devices}"")
        logger.debug(f""Selected input device: {input_device}"")
        logger.debug(f""Selected output device: {output_device}"")

        if input_device not in input_devices:
            logger.error(f""Input device '{input_device}' is not in the list of available devices"")
            raise HTTPException(status_code=400, detail=f""Input device '{input_device}' is not available"")
        
        if output_device not in output_devices:
            logger.error(f""Output device '{output_device}' is not in the list of available devices"")
            raise HTTPException(status_code=400, detail=f""Output device '{output_device}' is not available"")

        sd.default.device[0] = input_device_indices[input_devices.index(input_device)]
        sd.default.device[1] = output_device_indices[output_devices.index(output_device)]
        logger.info(f""Input device set to {sd.default.device[0]}: {input_device}"")
        logger.info(f""Output device set to {sd.default.device[1]}: {output_device}"")

audio_api = AudioAPI()

@app.get(""/inputDevices"", response_model=list)
def get_input_devices():
    try:
        input_devices, _, _, _ = audio_api.get_devices()
        return input_devices
    except Exception as e:
        logger.error(f""Failed to get input devices: {e}"")
        raise HTTPException(status_code=500, detail=""Failed to get input devices"")

@app.get(""/outputDevices"", response_model=list)
def get_output_devices():
    try:
        _, output_devices, _, _ = audio_api.get_devices()
        return output_devices
    except Exception as e:
        logger.error(f""Failed to get output devices: {e}"")
        raise HTTPException(status_code=500, detail=""Failed to get output devices"")

@app.post(""/config"")
def configure_audio(config_data: ConfigData):
    try:
        logger.info(f""Configuring audio with data: {config_data}"")
        if audio_api.set_values(config_data):
            settings = config_data.dict()
            settings[""use_jit""] = False
            with open(""configs/config.json"", ""w"", encoding='utf-8') as j:
                json.dump(settings, j, ensure_ascii=False)
            logger.info(""Configuration set successfully"")
            return {""message"": ""Configuration set successfully""}
    except HTTPException as e:
        logger.error(f""Configuration error: {e.detail}"")
        raise
    except Exception as e:
        logger.error(f""Configuration failed: {e}"")
        raise HTTPException(status_code=400, detail=f""Configuration failed: {e}"")

@app.post(""/start"")
def start_conversion():
    try:
        if not audio_api.flag_vc:
            audio_api.start_vc()
            return {""message"": ""Audio conversion started""}
        else:
            logger.warning(""Audio conversion already running"")
            raise HTTPException(status_code=400, detail=""Audio conversion already running"")
    except HTTPException as e:
        logger.error(f""Start conversion error: {e.detail}"")
        raise
    except Exception as e:
        logger.error(f""Failed to start conversion: {e}"")
        raise HTTPException(status_code=500, detail=""Failed to start conversion: {e}"")

@app.post(""/stop"")
def stop_conversion():
    try:
        if audio_api.flag_vc:
            audio_api.flag_vc = False
            global stream_latency
            stream_latency = -1
            return {""message"": ""Audio conversion stopped""}
        else:
            logger.warning(""Audio conversion not running"")
            raise HTTPException(status_code=400, detail=""Audio conversion not running"")
    except HTTPException as e:
        logger.error(f""Stop conversion error: {e.detail}"")
        raise
    except Exception as e:
        logger.error(f""Failed to stop conversion: {e}"")
        raise HTTPException(status_code=500, detail=""Failed to stop conversion: {e}"")

if __name__ == ""__main__"":
    if sys.platform == ""win32"":
        freeze_support()
    load_dotenv()
    os.environ[""OMP_NUM_THREADS""] = ""4""
    if sys.platform == ""darwin"":
        os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""
    from tools.torchgate import TorchGate
    import tools.rvc_for_realtime as rvc_for_realtime
    from configs.config import Config
    audio_api.config = Config()
    audio_api.initialize_queues()
    uvicorn.run(app, host=""0.0.0.0"", port=6242)
"
kovidgoyal_kitty,kovidgoyal_kitty_publish.py,22498,https://raw.githubusercontent.com/kovidgoyal/kitty/master/publish.py,"#!/usr/bin/env python
# License: GPL v3 Copyright: 2017, Kovid Goyal <kovid at kovidgoyal.net>

import argparse
import base64
import contextlib
import datetime
import glob
import io
import json
import mimetypes
import os
import pprint
import re
import shlex
import shutil
import subprocess
import sys
import tempfile
import time
from contextlib import contextmanager, suppress
from http.client import HTTPResponse, HTTPSConnection
from typing import Any, Callable, Dict, Generator, Iterable, List, Optional, Tuple, Union
from urllib.parse import urlencode, urlparse

os.chdir(os.path.dirname(os.path.abspath(__file__)))
docs_dir = os.path.abspath('docs')
publish_dir = os.path.abspath(os.path.join('..', 'kovidgoyal.github.io', 'kitty'))
building_nightly = False
with open('kitty/constants.py') as f:
    raw = f.read()
nv = re.search(r'^version: Version\s+=\s+Version\((\d+), (\d+), (\d+)\)', raw, flags=re.MULTILINE)
if nv is not None:
    version = f'{nv.group(1)}.{nv.group(2)}.{nv.group(3)}'
ap = re.search(r""^appname: str\s+=\s+'([^']+)'"", raw, flags=re.MULTILINE)
if ap is not None:
    appname = ap.group(1)

ALL_ACTIONS = 'local_build man html build tag sdist upload website'.split()
NIGHTLY_ACTIONS = 'local_build man html build sdist upload_nightly'.split()


def echo_cmd(cmd: Iterable[str]) -> None:
    isatty = sys.stdout.isatty()
    end = '\n'
    if isatty:
        end = f'\x1b[m{end}'
        print('\x1b[32m', end='')
    print(shlex.join(cmd), end=end, flush=True)


def call(*cmd: str, cwd: Optional[str] = None, echo: bool = False) -> None:
    if len(cmd) == 1:
        q = shlex.split(cmd[0])
    else:
        q = list(cmd)
    if echo:
        echo_cmd(cmd)
    ret = subprocess.Popen(q, cwd=cwd).wait()
    if ret != 0:
        raise SystemExit(ret)


def run_local_build(args: Any) -> None:
    call('make debug')


def run_build(args: Any) -> None:
    import runpy

    m = runpy.run_path('./setup.py', run_name='__publish__')
    vcs_rev: str = m['get_vcs_rev']()

    def run_with_retry(cmd: str) -> None:
        try:
            call(cmd, echo=True)
        except (SystemExit, Exception):
            needs_retry = building_nightly and 'linux' not in cmd
            if not needs_retry:
                raise
            print('Build failed, retrying in a minute seconds...', file=sys.stderr)
            if 'macos' in cmd:
                call('python ../bypy macos shutdown')
            time.sleep(60)
            call(cmd, echo=True)

    for x in ('64', 'arm64'):
        prefix = f'python ../bypy linux --arch {x} '
        run_with_retry(prefix + f'program --non-interactive --extra-program-data ""{vcs_rev}""')
    run_with_retry(f'python ../bypy macos program --sign-installers --notarize --non-interactive --extra-program-data ""{vcs_rev}""')
    call('python ../bypy macos shutdown', echo=True)
    call('make debug')
    call('./setup.py build-static-binaries')


def run_tag(args: Any) -> None:
    call('git push')
    call('git tag -s v{0} -m version-{0}'.format(version))
    call(f'git push origin v{version}')


def run_man(args: Any) -> None:
    call('make FAIL_WARN=1 man', cwd=docs_dir)


def run_html(args: Any) -> None:
    # Force a fresh build otherwise the search index is not correct
    with suppress(FileNotFoundError):
        shutil.rmtree(os.path.join(docs_dir, '_build', 'dirhtml'))
    call('make FAIL_WARN=1 ""OPTS=-D analytics_id=G-XTJK3R7GF2"" dirhtml', cwd=docs_dir)
    add_old_redirects('docs/_build/dirhtml')

    with suppress(FileNotFoundError):
        shutil.rmtree(os.path.join(docs_dir, '_build', 'html'))
    call('make FAIL_WARN=1 ""OPTS=-D analytics_id=G-XTJK3R7GF2"" html', cwd=docs_dir)


def generate_redirect_html(link_name: str, bname: str) -> None:
    with open(link_name, 'w') as f:
        f.write(f'''
<html>
<head>
<title>Redirecting...</title>
<link rel=""canonical"" href=""{bname}/"" />
<noscript>
<meta http-equiv=""refresh"" content=""0;url={bname}/"" />
</noscript>
<script type=""text/javascript"">
window.location.replace('./{bname}/' + window.location.hash);
</script>
</head>
<body>
<p>Redirecting, please wait...</p>
</body>
</html>
''')


def add_old_redirects(loc: str) -> None:
    for dirpath, dirnames, filenames in os.walk(loc):
        if dirpath != loc:
            for fname in filenames:
                if fname == 'index.html':
                    bname = os.path.basename(dirpath)
                    base = os.path.dirname(dirpath)
                    link_name = os.path.join(base, f'{bname}.html') if base else f'{bname}.html'
                    generate_redirect_html(link_name, bname)

    old_unicode_input_path = os.path.join(loc, 'kittens', 'unicode-input')
    os.makedirs(old_unicode_input_path, exist_ok=True)
    generate_redirect_html(os.path.join(old_unicode_input_path, 'index.html'), '../unicode_input')
    generate_redirect_html(f'{old_unicode_input_path}.html', 'unicode_input')


def run_docs(args: Any) -> None:
    subprocess.check_call(['make', 'docs'])


def run_website(args: Any) -> None:
    if os.path.exists(publish_dir):
        shutil.rmtree(publish_dir)
    shutil.copytree(os.path.join(docs_dir, '_build', 'dirhtml'), publish_dir, symlinks=True)
    with open(os.path.join(publish_dir, 'current-version.txt'), 'w') as f:
        f.write(version)
    shutil.copy2(os.path.join(docs_dir, 'installer.sh'), publish_dir)
    os.chdir(os.path.dirname(publish_dir))
    subprocess.check_call(['optipng', '-o7'] + glob.glob('kitty/_images/social_previews/*.png'))
    subprocess.check_call(['git', 'add', 'kitty'])
    subprocess.check_call(['git', 'commit', '-m', 'kitty website updates'])
    subprocess.check_call(['git', 'push'])


def sign_file(path: str) -> None:
    dest = f'{path}.sig'
    with suppress(FileNotFoundError):
        os.remove(dest)
    subprocess.check_call([
        os.environ['PENV'] + '/gpg-as-kovid', '--output', f'{path}.sig',
        '--detach-sig', path
    ])


def run_sdist(args: Any) -> None:
    with tempfile.TemporaryDirectory() as tdir:
        base = os.path.join(tdir, f'kitty-{version}')
        os.mkdir(base)
        subprocess.check_call(f'git archive HEAD | tar -x -C {base}', shell=True)
        dest = os.path.join(base, 'docs', '_build')
        os.mkdir(dest)
        for x in 'html man'.split():
            shutil.copytree(os.path.join(docs_dir, '_build', x), os.path.join(dest, x))
        dest = os.path.abspath(os.path.join('build', f'kitty-{version}.tar'))
        subprocess.check_call(['tar', '-cf', dest, os.path.basename(base)], cwd=tdir)
        with suppress(FileNotFoundError):
            os.remove(f'{dest}.xz')
        subprocess.check_call(['xz', '-9', dest])
        sign_file(f'{dest}.xz')


class ReadFileWithProgressReporting(io.FileIO):  # {{{
    def __init__(self, path: str):
        super().__init__(path, 'rb')
        self.seek(0, os.SEEK_END)
        self._total = self.tell()
        self.seek(0)
        self.start_time = time.monotonic()
        print('Starting upload of:', os.path.basename(path), 'size:', self._total)

    def __len__(self) -> int:
        return self._total

    def read(self, size: Optional[int] = -1) -> bytes:
        data = io.FileIO.read(self, size)
        if data:
            self.report_progress(len(data))
        return data

    def report_progress(self, size: int) -> None:
        def write(*args: str) -> None:
            print(*args, end='')

        frac = int(self.tell() * 100 / self._total)
        mb_pos = self.tell() / float(1024**2)
        mb_tot = self._total / float(1024**2)
        kb_pos = self.tell() / 1024.0
        kb_rate = kb_pos / (time.monotonic() - self.start_time)
        bit_rate = kb_rate * 1024
        eta = int((self._total - self.tell()) / bit_rate) + 1
        eta_m, eta_s = divmod(eta, 60)
        if sys.stdout.isatty():
            write(
                f'\r\033[K\033[?7h {frac}% {mb_pos:.1f}/{mb_tot:.1f}MB {kb_rate:.1f} KB/sec {eta_m} minutes, {eta_s} seconds left\033[?7l')
        if self.tell() >= self._total:
            t = int(time.monotonic() - self.start_time) + 1
            print(f'\nUpload took {t//60} minutes and {t%60} seconds at {kb_rate:.1f} KB/sec')
        sys.stdout.flush()


# }}}


class GitHub:  # {{{

    API = 'https://api.github.com'

    def __init__(
        self,
        files: Dict[str, str],
        reponame: str,
        version: str,
        username: str,
        password: str,
        replace: bool = False
    ):
        self.files, self.reponame, self.version, self.username, self.password, self.replace = (
            files, reponame, version, username, password, replace)
        self.current_tag_name = self.version if self.version == 'nightly' else f'v{self.version}'
        self.is_nightly = self.current_tag_name == 'nightly'
        self.auth = 'Basic ' + base64.standard_b64encode(f'{self.username}:{self.password}'.encode()).decode()
        self.url_base = f'{self.API}/repos/{self.username}/{self.reponame}/releases'

    def info(self, *args: Any) -> None:
        print(*args, flush=True)

    def error(self, *args: Any) -> None:
        print(*args, flush=True, file=sys.stderr)

    def make_request(
        self, url: str, data: Optional[Dict[str, Any]] = None, method:str = 'GET',
        upload_data: Optional[ReadFileWithProgressReporting] = None,
        params: Optional[Dict[str, str]] = None,
    ) -> HTTPSConnection:
        headers={
            'Authorization': self.auth,
            'Accept': 'application/vnd.github+json',
            'User-Agent': 'kitty',
            'X-GitHub-Api-Version': '2022-11-28',
        }
        if params:
            url += '?' + urlencode(params)
        rdata: Optional[Union[bytes, io.FileIO]] = None
        if data is not None:
            rdata = json.dumps(data).encode('utf-8')
            headers['Content-Type'] = 'application/json'
            headers['Content-Length'] = str(len(rdata))
        elif upload_data is not None:
            rdata = upload_data
            mime_type = mimetypes.guess_type(os.path.basename(str(upload_data.name)))[0] or 'application/octet-stream'
            headers['Content-Type'] = mime_type
            headers['Content-Length'] = str(upload_data._total)
        purl = urlparse(url)
        conn = HTTPSConnection(purl.netloc, timeout=60)
        conn.request(method, url, body=rdata, headers=headers)
        return conn

    def make_request_with_retries(
        self, url: str, data: Optional[Dict[str, str]] = None, method:str = 'GET',
        num_tries: int = 2, sleep_between_tries: float = 15,
        success_codes: Tuple[int, ...] = (200,),
        failure_msg: str = 'Request failed',
        return_data: bool = False,
        upload_path: str = '',
        params: Optional[Dict[str, str]] = None,
        failure_callback: Callable[[HTTPResponse], None] = lambda r: None,
    ) -> Any:
        for i in range(num_tries):
            is_last_try = i == num_tries - 1
            try:
                if upload_path:
                    conn = self.make_request(url, method='POST', upload_data=ReadFileWithProgressReporting(upload_path), params=params)
                else:
                    conn = self.make_request(url, data, method, params=params)
                with contextlib.closing(conn):
                    r = conn.getresponse()
                    if r.status in success_codes:
                        return json.loads(r.read()) if return_data else None
                    if is_last_try:
                        self.fail(r, failure_msg)
                    else:
                        self.print_failed_response_details(r, failure_msg)
                        failure_callback(r)
            except Exception as e:
                self.error(failure_msg, 'with error:', e)
            self.error(f'Retrying after {sleep_between_tries} seconds')
            if is_last_try:
                break
            time.sleep(sleep_between_tries)
        raise SystemExit('All retries failed, giving up')

    def patch(self, url: str, fail_msg: str, **data: str) -> None:
        self.make_request_with_retries(url, data, method='PATCH', failure_msg=fail_msg)

    def update_nightly_description(self, release_id: int) -> None:
        url = f'{self.url_base}/{release_id}'
        now = str(datetime.datetime.now(datetime.timezone.utc)).split('.')[0] + ' UTC'
        commit = subprocess.check_output(['git', 'rev-parse', '--verify', '--end-of-options', 'master^{commit}']).decode('utf-8').strip()
        self.patch(
            url, 'Failed to update nightly release description',
            body=f'Nightly release, generated on: {now} from commit: {commit}.'
            ' For how to install nightly builds, see: https://sw.kovidgoyal.net/kitty/binary/#customizing-the-installation'
        )

    def __call__(self) -> None:
        # See https://docs.github.com/en/rest/releases/assets#upload-a-release-asset
        release = self.create_release()
        upload_url = release['upload_url'].partition('{')[0]
        all_assest_for_release = self.existing_assets_for_release(release)
        assets_by_fname = {a['name']:a for a in all_assest_for_release}

        def delete_asset(asset: Dict[str, Any], allow_not_found: bool = True) -> None:
            success_codes = [204]
            if allow_not_found:
                success_codes.append(404)
            self.make_request_with_retries(
                asset['url'], method='DELETE', num_tries=5, sleep_between_tries=2, success_codes=tuple(success_codes),
                failure_msg='Failed to delete asset from GitHub')

        def upload_with_retries(path: str, desc: str, num_tries: int = 8, sleep_time: float = 60.0) -> None:
            fname = os.path.basename(path)
            if self.is_nightly:
                fname = fname.replace(version, 'nightly')
            if fname in assets_by_fname:
                self.info(f'Deleting {fname} from GitHub with id: {assets_by_fname[fname][""id""]}')
                delete_asset(assets_by_fname.pop(fname))
            params = {'name': fname, 'label': desc}

            self.make_request_with_retries(
                upload_url, upload_path=path, params=params, num_tries=num_tries, sleep_between_tries=sleep_time,
                failure_msg=f'Failed to upload file: {fname}', success_codes=(201,),
            )

        if self.is_nightly:
            for fname in tuple(assets_by_fname):
                self.info(f'Deleting {fname} from GitHub with id: {assets_by_fname[fname][""id""]}')
                delete_asset(assets_by_fname.pop(fname))
        for path, desc in self.files.items():
            self.info('')
            upload_with_retries(path, desc)
        if self.is_nightly:
            self.update_nightly_description(release['id'])

    def print_failed_response_details(self, r: HTTPResponse, msg: str) -> None:
        self.error(msg, f'\nStatus Code: {r.status} {r.reason}')
        try:
            jr = json.loads(r.read())
        except Exception:
            pass
        else:
            self.error('JSON from response:')
            pprint.pprint(jr, stream=sys.stderr)

    def fail(self, r: HTTPResponse, msg: str) -> None:
        self.print_failed_response_details(r, msg)
        raise SystemExit(1)

    def existing_assets_for_release(self, release: Dict[str, Any]) -> List[Dict[str, Any]]:
        if 'assets' in release:
            d: List[Dict[str, Any]] = release['assets']
        else:
            d = self.make_request_with_retries(
                release['assets_url'], params={'per_page': '64'}, failure_msg='Failed to get assets for release', return_data=True)
        return d

    def create_release(self) -> Dict[str, Any]:
        ' Create a release on GitHub or if it already exists, return the existing release '
        # Check for existing release
        url = f'{self.url_base}/tags/{self.current_tag_name}'
        with contextlib.closing(self.make_request(url)) as conn:
            r = conn.getresponse()
            if r.status == 200:
                return {str(k): v for k, v in json.loads(r.read()).items()}
        if self.is_nightly:
            self.fail(r, 'No existing nightly release found on GitHub')
        data = {
            'tag_name': self.current_tag_name,
            'target_commitish': 'master',
            'name': f'version {self.version}',
            'body': f'Release version {self.version}.'
            ' For changelog, see https://sw.kovidgoyal.net/kitty/changelog/#detailed-list-of-changes'
            ' GPG key used for signing tarballs is: https://calibre-ebook.com/signatures/kovid.gpg',
            'draft': False,
            'prerelease': False
        }
        with contextlib.closing(self.make_request(self.url_base, method='POST', data=data)) as conn:
            r = conn.getresponse()
            if r.status != 201:
                self.fail(r, f'Failed to create release for version: {self.version}')
            return {str(k): v for k, v in json.loads(r.read()).items()}
# }}}


def get_github_data() -> Dict[str, str]:
    with open(os.environ['PENV'] + '/github-token') as f:
        un, pw = f.read().strip().split(':')
    return {'username': un, 'password': pw}


def files_for_upload() -> Dict[str, str]:
    files = {}
    signatures = {}
    for f, desc in {
        'macos/dist/kitty-{}.dmg': 'macOS dmg',
        'linux/64/dist/kitty-{}-x86_64.txz': 'Linux amd64 binary bundle',
        'linux/arm64/dist/kitty-{}-arm64.txz': 'Linux arm64 binary bundle',
    }.items():
        path = os.path.join('bypy', 'b', f.format(version))
        if not os.path.exists(path):
            raise SystemExit(f'The installer {path} does not exist')
        files[path] = desc
        signatures[path] = f'GPG signature for {desc}'
    b = len(files)
    for path in glob.glob('build/static/kitten-*'):
        if path.endswith('.sig'):
            continue
        path = os.path.abspath(path)
        exe_name = os.path.basename(path)
        files[path] = f'Static {exe_name} executable'
        signatures[path] = f'GPG signature for static {exe_name} executable'
    if len(files) == b:
        raise SystemExit('No static binaries found')

    files[f'build/kitty-{version}.tar.xz'] = 'Source code'
    files[f'build/kitty-{version}.tar.xz.sig'] = 'Source code GPG signature'
    for path, desc in signatures.items():
        sign_file(path)
        files[f'{path}.sig'] = desc
    for f in files:
        if not os.path.exists(f):
            raise SystemExit(f'The release artifact {f} does not exist')
    return files


def run_upload(args: Any) -> None:
    gd = get_github_data()
    files = files_for_upload()
    gh = GitHub(files, appname, version, gd['username'], gd['password'])
    gh()


def run_upload_nightly(args: Any) -> None:
    subprocess.check_call(['git', 'tag', '-f', 'nightly'])
    subprocess.check_call(['git', 'push', 'origin', 'nightly', '-f'])
    gd = get_github_data()
    files = files_for_upload()
    gh = GitHub(files, appname, 'nightly', gd['username'], gd['password'])
    gh()


def current_branch() -> str:
    return subprocess.check_output(['git', 'symbolic-ref', '--short', 'HEAD']).decode('utf-8').strip()


def require_git_master(branch: str = 'master') -> None:
    if current_branch() != branch:
        raise SystemExit(f'You must be in the {branch} git branch')


def safe_read(path: str) -> str:
    with suppress(FileNotFoundError):
        with open(path) as f:
            return f.read()
    return ''


def remove_pycache_only_folders() -> None:
    folders_to_remove = []
    for dirpath, folders, files in os.walk('.'):
        if not files and folders == ['__pycache__']:
            folders_to_remove.append(dirpath)
    for x in folders_to_remove:
        shutil.rmtree(x)


@contextmanager
def change_to_git_master() -> Generator[None, None, None]:
    stash_ref_before = safe_read('.git/refs/stash')
    subprocess.check_call(['git', 'stash', '-u'])
    try:
        branch_before = current_branch()
        if branch_before != 'master':
            subprocess.check_call(['git', 'switch', 'master'])
            remove_pycache_only_folders()
            subprocess.check_call(['make', 'clean', 'debug'])
        try:
            yield
        finally:
            if branch_before != 'master':
                subprocess.check_call(['git', 'switch', branch_before])
                subprocess.check_call(['make', 'clean', 'debug'])
    finally:
        if stash_ref_before != safe_read('.git/refs/stash'):
            subprocess.check_call(['git', 'stash', 'pop'])


def require_penv() -> None:
    if 'PENV' not in os.environ:
        raise SystemExit('The PENV env var is not present, required for uploading releases')


def exec_actions(actions: Iterable[str], args: Any) -> None:
    for action in actions:
        print('Running', action)
        cwd = os.getcwd()
        globals()[f'run_{action}'](args)
        os.chdir(cwd)


def main() -> None:
    global building_nightly
    parser = argparse.ArgumentParser(description='Publish kitty')
    parser.add_argument(
        '--only',
        default=False,
        action='store_true',
        help='Only run the specified action, by default the specified action and all sub-sequent actions are run')
    parser.add_argument(
        '--nightly',
        default=False,
        action='store_true',
        help='Upload a nightly release, ignores all other arguments')
    parser.add_argument(
        'action',
        default='all',
        nargs='?',
        choices=list(ALL_ACTIONS) + ['all', 'upload_nightly'],
        help='The action to start with')
    args = parser.parse_args()
    require_penv()
    if args.nightly:
        with change_to_git_master():
            building_nightly = True
            exec_actions(NIGHTLY_ACTIONS, args)
            subprocess.run(['make', 'clean', 'debug'])
        return
    require_git_master()
    if args.action == 'all':
        actions = list(ALL_ACTIONS)
    elif args.action == 'upload_nightly':
        actions = ['upload_nightly']
    else:
        idx = ALL_ACTIONS.index(args.action)
        actions = ALL_ACTIONS[idx:]
    if args.only:
        del actions[1:]
    else:
        try:
            ans = input(f'Publish version \033[91m{version}\033[m (y/n): ')
        except KeyboardInterrupt:
            ans = 'n'
        if ans.lower() != 'y':
            return
    if actions == ['website']:
        actions.insert(0, 'html')
    exec_actions(actions, args)


if __name__ == '__main__':
    main()
"
microsoft_cascadia-code,microsoft_cascadia-code_build.py,24889,https://raw.githubusercontent.com/microsoft/cascadia-code/main/build.py,"import argparse
import multiprocessing
import multiprocessing.pool
import os
import subprocess
from pathlib import Path
from typing import cast
import xml.etree.cElementTree as ET
import tempfile
import glob

import cffsubr.__main__
import fontmake.instantiator
import fontTools.designspaceLib
import fontTools.ttLib
import fontTools.ttLib.tables._g_l_y_f as _g_l_y_f
import psautohint.__main__
from gftools.stat import gen_stat_tables_from_config
import yaml
import ufo2ft
import ufoLib2
import vttLib
import vttLib.transfer
from vttmisc import tsi1, tsic

VERSION_YEAR_MONTH = 2407
VERSION_DAY = 24
OUTPUT_DIR = Path(""build"")
OUTPUT_OTF_DIR = OUTPUT_DIR / ""otf""
OUTPUT_TTF_DIR = OUTPUT_DIR / ""ttf""
OUTPUT_WOFF2_DIR = OUTPUT_DIR / ""woff2""
OUTPUT_STATIC_OTF_DIR = OUTPUT_OTF_DIR / ""static""
OUTPUT_STATIC_TTF_DIR = OUTPUT_TTF_DIR / ""static""
OUTPUT_STATIC_WOFF2_DIR = OUTPUT_WOFF2_DIR / ""static""
INPUT_DIR = Path(""sources"")
VTT_DATA_FILE = INPUT_DIR / ""vtt_data"" / ""CascadiaCode_VTT.ttf""
ITALIC_VTT_DATA_FILE = INPUT_DIR / ""vtt_data"" / ""CascadiaCodeItalic_VTT.ttf""
FEATURES_DIR = INPUT_DIR / ""features""
NERDFONTS_DIR = INPUT_DIR / ""nerdfonts""

# Font modifications
# ****************************************************************


def step_set_font_name(name: str, source: ufoLib2.Font) -> None:
    source.info.familyName = source.info.familyName.replace(""Cascadia Code"", name)
    # We have to change the style map family name because that's what
    # Windows uses to map Bold/Regular/Medium/etc. fonts
    if source.info.styleMapFamilyName:
        source.info.styleMapFamilyName = source.info.styleMapFamilyName.replace(""Cascadia Code"", name)


def step_merge_glyphs_from_ufo(path: Path, instance: ufoLib2.Font) -> None:
    unicodes = []
    for glyph in instance:
        unicodes.append(glyph.unicode)
    ufo = ufoLib2.Font.open(path)
    for glyph in ufo:
        if glyph.unicode:
            if glyph.unicode not in unicodes:
                newName = str(hex(glyph.unicode)).upper().replace(""0X"",""uni"")
                instance.layers.defaultLayer.insertGlyph(ufo[glyph.name],newName, overwrite=False, copy=False)
        else:
            instance.addGlyph(ufo[glyph.name])


def step_set_feature_file(path: Path, name: str, instance: ufoLib2.Font) -> None:
    featureSet = """"
    if ""Italic"" in name: #until I can come up with a more elegent solution, this'll do. 
        featureList = [
            ""header_italic"", # adds definitions, language systems
            ""aalt_italic"",
            ""ccmp"",
            ""locl_italic"", 
            ""calt_italic"", 
            ""figures_italic"", # contains subs/sinf/sups/numr/dnom
            ""frac"", 
            ""ordn"", 
            ""case"", 
            ""salt"",
            ""ss01"",
            ""ss02"",
            ""ss03"", 
            ""ss19"", 
            ""ss20"", 
            ""rclt"", 
            ""zero""
            ]
    else:
        featureList = [
            ""header"", # adds definitions, language systems
            ""aalt"",
            ""ccmp"",
            ""locl"", 
            ""calt"",
            ""figures"", # contains subs/sinf/sups/numr/dnom
            ""frac"", 
            ""ordn"", 
            ""case"", 
            ""ss02"", 
            ""ss19"", 
            ""ss20"", 
            ""rclt"", 
            ""zero"",
            ""init"",
            ""medi"",
            ""fina"",
            ""rlig"",
            ]

    for item in featureList:
        if ""PL"" in name and item == ""rclt"":
            featureSet += Path(path / str(""rclt_PL.fea"")).read_text()
        elif ""NF"" in name and item == ""rclt"":
            featureSet += Path(path / str(""rclt_PL.fea"")).read_text()
        elif ""Mono"" in name and ""calt"" in item:
            featureSet += Path(path / str(item+""_mono.fea"")).read_text() #both Italic and Regular can use same mono
        else:
            featureSet += Path(path / str(item+"".fea"")).read_text()
    instance.features.text = featureSet   


def set_font_metaData(font: ufoLib2.Font) -> None:
    font.info.versionMajor = VERSION_YEAR_MONTH
    font.info.versionMinor = VERSION_DAY

    font.info.openTypeOS2TypoAscender = 1900
    font.info.openTypeOS2TypoDescender = -480
    font.info.openTypeOS2TypoLineGap = 0

    font.info.openTypeHheaAscender = font.info.openTypeOS2TypoAscender
    font.info.openTypeHheaDescender = font.info.openTypeOS2TypoDescender
    font.info.openTypeHheaLineGap = font.info.openTypeOS2TypoLineGap

    font.info.openTypeOS2WinAscent = 2226
    font.info.openTypeOS2WinDescent = abs(font.info.openTypeOS2TypoDescender)

    font.info.openTypeGaspRangeRecords = [
        {""rangeMaxPPEM"": 9, ""rangeGaspBehavior"": [1, 3]},
        {""rangeMaxPPEM"": 50, ""rangeGaspBehavior"": [0, 1, 2, 3]},
        {""rangeMaxPPEM"": 65535, ""rangeGaspBehavior"": [1, 3]},
    ]


def set_overlap_flag(varfont: fontTools.ttLib.TTFont) -> fontTools.ttLib.TTFont:
    glyf = cast(_g_l_y_f.table__g_l_y_f, varfont[""glyf""])
    for glyph_name in glyf.keys():
        glyph = glyf[glyph_name]
        if glyph.isComposite():
            # Set OVERLAP_COMPOUND bit for compound glyphs
            glyph.components[0].flags |= 0x400
        elif glyph.numberOfContours > 0:
            # Set OVERLAP_SIMPLE bit for simple glyphs
            glyph.flags[0] |= 0x40

def prepare_fonts(
    designspace: fontTools.designspaceLib.DesignSpaceDocument, name: str
) -> None:
    designspace.loadSourceFonts(ufoLib2.Font.open)
    for source in designspace.sources:

        step_set_feature_file(FEATURES_DIR, name, source.font)

        if ""PL"" in name or ""NF"" in name or ""Mono"" in name:
            step_set_font_name(name, source.font)

        if ""PL"" in name or ""NF"" in name:
            print(f""[{name} {source.styleName}] Merging PL glyphs"")
            step_merge_glyphs_from_ufo(
                NERDFONTS_DIR / ""NerdfontsPL-Regular.ufo"", source.font
            )

        if ""NF"" in name:
            print(f""[{name} {source.styleName}] Merging NF glyphs"")
            for ufo in Path(NERDFONTS_DIR/""full""/""processed"").glob(""*.ufo""):
                step_merge_glyphs_from_ufo(
                    ufo, source.font
                )

        set_font_metaData(source.font)
    for instance in designspace.instances:
        instance.name = instance.name.replace(""Cascadia Code"", name)
        instance.familyName = instance.familyName.replace(""Cascadia Code"", name)
        if instance.styleMapFamilyName:
            instance.styleMapFamilyName = instance.styleMapFamilyName.replace(""Cascadia Code"", name)


def to_woff2(source_path: Path, target_path: Path) -> None:
    print(f""[WOFF2] Compressing {source_path} to {target_path}"")
    font = fontTools.ttLib.TTFont(source_path)
    font.flavor = ""woff2""
    target_path.parent.mkdir(exist_ok=True, parents=True)
    font.save(target_path)

# Build fonts
# ****************************************************************


def build_font_variable(
    designspace: fontTools.designspaceLib.DesignSpaceDocument,
    name: str,
    vtt_compile: bool = True,
) -> None:
    prepare_fonts(designspace, name)
    compile_variable_and_save(designspace, vtt_compile)


def build_font_static(
    designspace: fontTools.designspaceLib.DesignSpaceDocument,
    instance_descriptor: fontTools.designspaceLib.InstanceDescriptor,
    name: str,
) -> None:
    prepare_fonts(designspace, name)

    generator = fontmake.instantiator.Instantiator.from_designspace(designspace)
    instance = generator.generate_instance(instance_descriptor)
    instance.info.familyName = instance.info.familyName.replace("" Italic"","""")
    if instance.info.styleMapFamilyName:
        instance.info.styleMapFamilyName = instance.info.styleMapFamilyName.replace("" Italic"","""")
    compile_static_and_save(instance, name.replace("" Italic"",""""))


# Export fonts
# ****************************************************************


def compile_variable_and_save(
    designspace: fontTools.designspaceLib.DesignSpaceDocument,
    vtt_compile: bool = True,
) -> None:
    
    if ""Italic"" in designspace.default.font.info.familyName: #Some weird stuff happens with Italics
        designspace.default.font.info.familyName = designspace.default.font.info.familyName.replace("" Italic"", """")
    
    familyName = designspace.default.font.info.familyName
    styleName = designspace.default.font.info.styleName
    file_stem = familyName.replace("" "", """")
    if ""Italic"" in styleName and ""Italic"" not in file_stem:
        file_stem = file_stem+""Italic""
    file_path: Path = (OUTPUT_TTF_DIR / file_stem).with_suffix("".ttf"")

    print(f""[{familyName} {styleName}] Compiling"")
    varFont = ufo2ft.compileVariableTTF(designspace, inplace=True)

    print(f""[{familyName} {styleName}] Merging VTT"")

    if ""Italic"" in styleName:
        font_vtt = fontTools.ttLib.TTFont(ITALIC_VTT_DATA_FILE)
    else:
        font_vtt = fontTools.ttLib.TTFont(VTT_DATA_FILE)
    

    for table in [""TSI0"", ""TSI1"", ""TSI2"", ""TSI3"", ""TSI5"", ""TSIC"", ""maxp""]:
        varFont[table] = fontTools.ttLib.newTable(table)
        varFont[table] = font_vtt[table]

    # this will correct the OFFSET[R] commands in TSI1
    if font_vtt.getGlyphOrder() != varFont.getGlyphOrder():
        tsi1.fixOFFSET(varFont, font_vtt)
        pass

    if vtt_compile:
        print(f""[{familyName} {styleName}] Compiling VTT"")
        vttLib.compile_instructions(varFont, ship=True)
    else:
        file_path = (OUTPUT_TTF_DIR / str(file_stem+""_VTT"")).with_suffix("".ttf"")

    # last minute manual corrections to set things correctly
    # set two flags to enable proper rendering (one for overlaps in Mac, the other for windows hinting)
    # Helping mac office generage the postscript name correctly for variable fonts when an italic is present
    set_overlap_flag(varFont)
    varFont[""head""].flags = 0x000b

    if ""Regular"" in styleName:
        varFont[""name""].setName(familyName.replace("" "","""")+""Roman"", 25, 3, 1, 1033)

    print(f""[{familyName} {styleName}] Saving"")
    file_path.parent.mkdir(exist_ok=True, parents=True)
    varFont.save(file_path)

    print(f""[{familyName}] Done: {file_path}"")


def compile_static_and_save(instance: ufoLib2.Font, name:str) -> None:
    family_name = name
    style_name = instance.info.styleName
    print(f""[{family_name}] Building static instance: {style_name}"")

    # Use pathops backend for overlap removal because it is, at the time of this
    # writing, massively faster than booleanOperations and thanks to autohinting,
    # there is no need to keep outlines compatible to previous releases.
    static_ttf = ufo2ft.compileTTF(
        instance, removeOverlaps=True, overlapsBackend=""pathops""
    )
    static_otf = ufo2ft.compileOTF(
        instance,
        removeOverlaps=True,
        overlapsBackend=""pathops"",
        # Can do inplace now because TTF is already done.
        inplace=True,
        # Don't optimize here, will be optimized after autohinting.
        optimizeCFF=ufo2ft.CFFOptimization.NONE,
    )

    file_name = f""{family_name}-{style_name}"".replace("" "", """")
    file_path_static = (OUTPUT_STATIC_TTF_DIR / file_name).with_suffix("".ttf"")
    file_path_static_otf = (OUTPUT_STATIC_OTF_DIR / file_name).with_suffix("".otf"")

    file_path_static.parent.mkdir(exist_ok=True, parents=True)
    static_ttf.save(file_path_static)
    file_path_static_otf.parent.mkdir(exist_ok=True, parents=True)
    static_otf.save(file_path_static_otf)
    print(f""[{family_name}] Done: {file_path_static}, {file_path_static_otf}"")


# Font hinting
# ****************************************************************


def autohint(otf_path: Path) -> None:
    path = os.fspath(otf_path)

    print(f""Autohinting {path}"")
    psautohint.__main__.main([path])

    print(f""Compressing {path}"")
    cffsubr.__main__.main([""-i"", path])


def ttfautohint(path: str) -> None:
    print(f""Autohinting {path}"")
    subprocess.check_call(
        [
            ""ttfautohint"",
            ""--stem-width"",
            ""nsn"",
            ""--increase-x-height"",
            ""0"",
            ""--reference"",
            os.fspath(OUTPUT_STATIC_TTF_DIR / ""CascadiaCode-Regular.ttf""),
            path,
            path[:-4] + ""-hinted.ttf"",
        ]
    )
    os.remove(path)
    os.rename(path[:-4] + ""-hinted.ttf"", path)


# Main build script
# ****************************************************************

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""build some fonts"")
    parser.add_argument(""-P"", ""--no-powerline"", action=""store_false"", dest=""powerline"")
    parser.add_argument(""-NF"", ""--no-nerdfonts"", action=""store_false"", dest=""nerdfonts"")
    parser.add_argument(""-M"", ""--no-mono"", action=""store_false"", dest=""mono"")
    parser.add_argument(""-S"", ""--static-fonts"", action=""store_true"")
    parser.add_argument(""-I"", ""--no-italic"", action=""store_false"", dest=""italic"")
    parser.add_argument(
        ""-V"",
        ""--no-vtt-compile"",
        action=""store_false"",
        dest=""vtt_compile"",
        help=""Do not compile VTT code but leave in the VTT sources."",
    )
    parser.add_argument(""-W"", ""--web-fonts"", action=""store_true"")
    args = parser.parse_args()

    # Load Designspace and filter out instances that are marked as non-exportable.
    designspace = fontTools.designspaceLib.DesignSpaceDocument.fromfile(
        INPUT_DIR / ""CascadiaCode_variable.designspace""
    )

    designspace.instances = [
        s
        for s in designspace.instances
        if s.lib.get(""com.schriftgestaltung.export"", True)
    ]

    designspaceItalic = fontTools.designspaceLib.DesignSpaceDocument.fromfile(
        INPUT_DIR / ""CascadiaCode_variable_italic.designspace""
    )

    designspaceItalic.instances = [
        s
        for s in designspaceItalic.instances
        if s.lib.get(""com.schriftgestaltung.export"", True)
    ]


    #Stage 1: Make all the things.
    pool = multiprocessing.pool.Pool(processes=multiprocessing.cpu_count())
    processes = []
    processes.append(
        pool.apply_async(
            build_font_variable,
            (
                designspace,
                ""Cascadia Code"",
                args.vtt_compile,
            ),
        )
    )
    if args.italic:
        processes.append(
            pool.apply_async(
                build_font_variable,
                (
                    designspaceItalic,
                    ""Cascadia Code Italic"",
                    args.vtt_compile,
                ),
            )
        )
    if args.mono:
        processes.append(
            pool.apply_async(
                build_font_variable,
                (
                    designspace,
                    ""Cascadia Mono"",
                    args.vtt_compile,
                ),
            )
        )
        if args.italic:
            processes.append(
                pool.apply_async(
                    build_font_variable,
                    (
                        designspaceItalic,
                        ""Cascadia Mono Italic"",
                        args.vtt_compile,
                    ),
                )
            )
    if args.powerline:
        processes.append(
            pool.apply_async(
                build_font_variable,
                (
                    designspace,
                    ""Cascadia Code PL"",
                    args.vtt_compile,
                ),
            )
        )
        if args.italic:
            processes.append(
                pool.apply_async(
                    build_font_variable,
                    (
                        designspaceItalic,
                        ""Cascadia Code PL Italic"",
                        args.vtt_compile,
                    ),
                )
            )
        if args.mono:
            processes.append(
                pool.apply_async(
                    build_font_variable,
                    (
                        designspace,
                        ""Cascadia Mono PL"",
                        args.vtt_compile,
                    ),
                )
            )
            if args.italic:
                processes.append(
                    pool.apply_async(
                        build_font_variable,
                        (
                            designspaceItalic,
                            ""Cascadia Mono PL Italic"",
                            args.vtt_compile,
                        ),
                    )
                )
    if args.nerdfonts:
        processes.append(
            pool.apply_async(
                build_font_variable,
                (
                    designspace,
                    ""Cascadia Code NF"",
                    args.vtt_compile,
                ),
            )
        )
        if args.italic:
            processes.append(
                pool.apply_async(
                    build_font_variable,
                    (
                        designspaceItalic,
                        ""Cascadia Code NF Italic"",
                        args.vtt_compile,
                    ),
                )
            )
        if args.mono:
            processes.append(
                pool.apply_async(
                    build_font_variable,
                    (
                        designspace,
                        ""Cascadia Mono NF"",
                        args.vtt_compile,
                    ),
                )
            )
            if args.italic:
                processes.append(
                    pool.apply_async(
                        build_font_variable,
                        (
                            designspaceItalic,
                            ""Cascadia Mono NF Italic"",
                            args.vtt_compile,
                        ),
                    )
                )

    if args.static_fonts:
        # Build the Regulars
        for instance_descriptor in designspace.instances:
            processes.append(
                pool.apply_async(
                    build_font_static,
                    (
                        designspace,
                        instance_descriptor,
                        ""Cascadia Code"",
                    ),
                )
            )
            if args.mono:
                processes.append(
                    pool.apply_async(
                        build_font_static,
                        (
                            designspace,
                            instance_descriptor,
                            ""Cascadia Mono"",
                        ),
                    )
                )
            if args.powerline:
                processes.append(
                    pool.apply_async(
                        build_font_static,
                        (
                            designspace,
                            instance_descriptor,
                            ""Cascadia Code PL"",
                        ),
                    )
                )
                if args.mono:
                    processes.append(
                        pool.apply_async(
                            build_font_static,
                            (
                                designspace,
                                instance_descriptor,
                                ""Cascadia Mono PL"",
                            ),
                        )
                    )
            if args.nerdfonts:
                processes.append(
                    pool.apply_async(
                        build_font_static,
                        (
                            designspace,
                            instance_descriptor,
                            ""Cascadia Code NF"",
                        ),
                    )
                )
                if args.mono:
                    processes.append(
                        pool.apply_async(
                            build_font_static,
                            (
                                designspace,
                                instance_descriptor,
                                ""Cascadia Mono NF"",
                            ),
                        )
                    )
        if args.italic:
            # Build the Regulars
            for instance_descriptor in designspaceItalic.instances:
                processes.append(
                    pool.apply_async(
                        build_font_static,
                        (
                            designspaceItalic,
                            instance_descriptor,
                            ""Cascadia Code Italic"",
                        ),
                    )
                )
                if args.mono:
                    processes.append(
                        pool.apply_async(
                            build_font_static,
                            (
                                designspaceItalic,
                                instance_descriptor,
                                ""Cascadia Mono Italic"",
                            ),
                        )
                    )
                if args.powerline:
                    processes.append(
                        pool.apply_async(
                            build_font_static,
                            (
                                designspaceItalic,
                                instance_descriptor,
                                ""Cascadia Code PL Italic"",
                            ),
                        )
                    )
                    if args.mono:
                        processes.append(
                            pool.apply_async(
                                build_font_static,
                                (
                                    designspaceItalic,
                                    instance_descriptor,
                                    ""Cascadia Mono PL Italic"",
                                ),
                            )
                        )
                if args.nerdfonts:
                    processes.append(
                        pool.apply_async(
                            build_font_static,
                            (
                                designspaceItalic,
                                instance_descriptor,
                                ""Cascadia Code NF Italic"",
                            ),
                        )
                    )
                    if args.mono:
                        processes.append(
                            pool.apply_async(
                                build_font_static,
                                (
                                    designspaceItalic,
                                    instance_descriptor,
                                    ""Cascadia Mono NF Italic"",
                                ),
                            )
                        )

    pool.close()
    pool.join()
    for process in processes:
        process.get()
    del processes, pool

    # Step 1.5: Adding STAT tables in one go
    print (""[Cascadia Variable fonts] Fixing STAT tables"")
    fontSTAT = [fontTools.ttLib.TTFont(f) for f in list(OUTPUT_TTF_DIR.glob(""*.ttf""))]
    with open(INPUT_DIR/""stat.yaml"") as f:
        config = yaml.load(f, Loader=yaml.SafeLoader)
    gen_stat_tables_from_config(config, fontSTAT)

    for font in fontSTAT:
        font.save(font.reader.file.name)

    # Stage 2: Autohint and maybe compress all the static things.
    if args.static_fonts is True:
        otfs = list(OUTPUT_STATIC_OTF_DIR.glob(""*.otf""))
        if otfs:
            pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
            processes = [pool.apply_async(autohint, (otf,)) for otf in otfs]
            pool.close()
            pool.join()
            for process in processes:
                process.get()
            del processes, pool

        try:
            for ttf_path in OUTPUT_STATIC_TTF_DIR.glob(""*.ttf""):
                if not ttf_path.stem.endswith(""-hinted""):
                    ttfautohint(os.fspath(ttf_path))
        except Exception as e:
            print(f""ttfautohint failed. Please reinstall and try again. {str(e)}"")

    # Stage 3: Have some web fonts.
    if args.web_fonts:
        pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
        processes = [
            pool.apply_async(
                to_woff2,
                (
                    path,
                    # This removes build/ttf from the found files and prepends
                    # build/woff2 instead, keeping the sub-structure.
                    OUTPUT_WOFF2_DIR
                    / path.relative_to(OUTPUT_TTF_DIR).with_suffix("".woff2""),
                ),
            )
            for path in OUTPUT_TTF_DIR.glob(""**/*.ttf"")
        ]
        pool.close()
        pool.join()
        for process in processes:
            process.get()

    print(""All done."")
"
svc-develop-team_so-vits-svc,svc-develop-team_so-vits-svc_utils.py,20720,https://raw.githubusercontent.com/svc-develop-team/so-vits-svc/4.1-Stable/utils.py,"import argparse
import glob
import json
import logging
import os
import re
import subprocess
import sys
import traceback
from multiprocessing import cpu_count

import faiss
import librosa
import numpy as np
import torch
from scipy.io.wavfile import read
from sklearn.cluster import MiniBatchKMeans
from torch.nn import functional as F

MATPLOTLIB_FLAG = False

logging.basicConfig(stream=sys.stdout, level=logging.WARN)
logger = logging

f0_bin = 256
f0_max = 1100.0
f0_min = 50.0
f0_mel_min = 1127 * np.log(1 + f0_min / 700)
f0_mel_max = 1127 * np.log(1 + f0_max / 700)

def normalize_f0(f0, x_mask, uv, random_scale=True):
    # calculate means based on x_mask
    uv_sum = torch.sum(uv, dim=1, keepdim=True)
    uv_sum[uv_sum == 0] = 9999
    means = torch.sum(f0[:, 0, :] * uv, dim=1, keepdim=True) / uv_sum

    if random_scale:
        factor = torch.Tensor(f0.shape[0], 1).uniform_(0.8, 1.2).to(f0.device)
    else:
        factor = torch.ones(f0.shape[0], 1).to(f0.device)
    # normalize f0 based on means and factor
    f0_norm = (f0 - means.unsqueeze(-1)) * factor.unsqueeze(-1)
    if torch.isnan(f0_norm).any():
        exit(0)
    return f0_norm * x_mask
def plot_data_to_numpy(x, y):
    global MATPLOTLIB_FLAG
    if not MATPLOTLIB_FLAG:
        import matplotlib
        matplotlib.use(""Agg"")
        MATPLOTLIB_FLAG = True
        mpl_logger = logging.getLogger('matplotlib')
        mpl_logger.setLevel(logging.WARNING)
    import matplotlib.pylab as plt
    import numpy as np

    fig, ax = plt.subplots(figsize=(10, 2))
    plt.plot(x)
    plt.plot(y)
    plt.tight_layout()

    fig.canvas.draw()
    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')
    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))
    plt.close()
    return data


def f0_to_coarse(f0):
  f0_mel = 1127 * (1 + f0 / 700).log()
  a = (f0_bin - 2) / (f0_mel_max - f0_mel_min)
  b = f0_mel_min * a - 1.
  f0_mel = torch.where(f0_mel > 0, f0_mel * a - b, f0_mel)
  # torch.clip_(f0_mel, min=1., max=float(f0_bin - 1))
  f0_coarse = torch.round(f0_mel).long()
  f0_coarse = f0_coarse * (f0_coarse > 0)
  f0_coarse = f0_coarse + ((f0_coarse < 1) * 1)
  f0_coarse = f0_coarse * (f0_coarse < f0_bin)
  f0_coarse = f0_coarse + ((f0_coarse >= f0_bin) * (f0_bin - 1))
  return f0_coarse

def get_content(cmodel, y):
    with torch.no_grad():
        c = cmodel.extract_features(y.squeeze(1))[0]
    c = c.transpose(1, 2)
    return c

def get_f0_predictor(f0_predictor,hop_length,sampling_rate,**kargs):
    if f0_predictor == ""pm"":
        from modules.F0Predictor.PMF0Predictor import PMF0Predictor
        f0_predictor_object = PMF0Predictor(hop_length=hop_length,sampling_rate=sampling_rate)
    elif f0_predictor == ""crepe"":
        from modules.F0Predictor.CrepeF0Predictor import CrepeF0Predictor
        f0_predictor_object = CrepeF0Predictor(hop_length=hop_length,sampling_rate=sampling_rate,device=kargs[""device""],threshold=kargs[""threshold""])
    elif f0_predictor == ""harvest"":
        from modules.F0Predictor.HarvestF0Predictor import HarvestF0Predictor
        f0_predictor_object = HarvestF0Predictor(hop_length=hop_length,sampling_rate=sampling_rate)
    elif f0_predictor == ""dio"":
        from modules.F0Predictor.DioF0Predictor import DioF0Predictor
        f0_predictor_object = DioF0Predictor(hop_length=hop_length,sampling_rate=sampling_rate) 
    elif f0_predictor == ""rmvpe"":
        from modules.F0Predictor.RMVPEF0Predictor import RMVPEF0Predictor
        f0_predictor_object = RMVPEF0Predictor(hop_length=hop_length,sampling_rate=sampling_rate,dtype=torch.float32 ,device=kargs[""device""],threshold=kargs[""threshold""])
    elif f0_predictor == ""fcpe"":
        from modules.F0Predictor.FCPEF0Predictor import FCPEF0Predictor
        f0_predictor_object = FCPEF0Predictor(hop_length=hop_length,sampling_rate=sampling_rate,dtype=torch.float32 ,device=kargs[""device""],threshold=kargs[""threshold""])
    else:
        raise Exception(""Unknown f0 predictor"")
    return f0_predictor_object

def get_speech_encoder(speech_encoder,device=None,**kargs):
    if speech_encoder == ""vec768l12"":
        from vencoder.ContentVec768L12 import ContentVec768L12
        speech_encoder_object = ContentVec768L12(device = device)
    elif speech_encoder == ""vec256l9"":
        from vencoder.ContentVec256L9 import ContentVec256L9
        speech_encoder_object = ContentVec256L9(device = device)
    elif speech_encoder == ""vec256l9-onnx"":
        from vencoder.ContentVec256L9_Onnx import ContentVec256L9_Onnx
        speech_encoder_object = ContentVec256L9_Onnx(device = device)
    elif speech_encoder == ""vec256l12-onnx"":
        from vencoder.ContentVec256L12_Onnx import ContentVec256L12_Onnx
        speech_encoder_object = ContentVec256L12_Onnx(device = device)
    elif speech_encoder == ""vec768l9-onnx"":
        from vencoder.ContentVec768L9_Onnx import ContentVec768L9_Onnx
        speech_encoder_object = ContentVec768L9_Onnx(device = device)
    elif speech_encoder == ""vec768l12-onnx"":
        from vencoder.ContentVec768L12_Onnx import ContentVec768L12_Onnx
        speech_encoder_object = ContentVec768L12_Onnx(device = device)
    elif speech_encoder == ""hubertsoft-onnx"":
        from vencoder.HubertSoft_Onnx import HubertSoft_Onnx
        speech_encoder_object = HubertSoft_Onnx(device = device)
    elif speech_encoder == ""hubertsoft"":
        from vencoder.HubertSoft import HubertSoft
        speech_encoder_object = HubertSoft(device = device)
    elif speech_encoder == ""whisper-ppg"":
        from vencoder.WhisperPPG import WhisperPPG
        speech_encoder_object = WhisperPPG(device = device)
    elif speech_encoder == ""cnhubertlarge"":
        from vencoder.CNHubertLarge import CNHubertLarge
        speech_encoder_object = CNHubertLarge(device = device)
    elif speech_encoder == ""dphubert"":
        from vencoder.DPHubert import DPHubert
        speech_encoder_object = DPHubert(device = device)
    elif speech_encoder == ""whisper-ppg-large"":
        from vencoder.WhisperPPGLarge import WhisperPPGLarge
        speech_encoder_object = WhisperPPGLarge(device = device)
    elif speech_encoder == ""wavlmbase+"":
        from vencoder.WavLMBasePlus import WavLMBasePlus
        speech_encoder_object = WavLMBasePlus(device = device)
    else:
        raise Exception(""Unknown speech encoder"")
    return speech_encoder_object 

def load_checkpoint(checkpoint_path, model, optimizer=None, skip_optimizer=False):
    assert os.path.isfile(checkpoint_path)
    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')
    iteration = checkpoint_dict['iteration']
    learning_rate = checkpoint_dict['learning_rate']
    if optimizer is not None and not skip_optimizer and checkpoint_dict['optimizer'] is not None:
        optimizer.load_state_dict(checkpoint_dict['optimizer'])
    saved_state_dict = checkpoint_dict['model']
    model = model.to(list(saved_state_dict.values())[0].dtype)
    if hasattr(model, 'module'):
        state_dict = model.module.state_dict()
    else:
        state_dict = model.state_dict()
    new_state_dict = {}
    for k, v in state_dict.items():
        try:
            # assert ""dec"" in k or ""disc"" in k
            # print(""load"", k)
            new_state_dict[k] = saved_state_dict[k]
            assert saved_state_dict[k].shape == v.shape, (saved_state_dict[k].shape, v.shape)
        except Exception:
            if ""enc_q"" not in k or ""emb_g"" not in k:
              print(""%s is not in the checkpoint,please check your checkpoint.If you're using pretrain model,just ignore this warning."" % k)
              logger.info(""%s is not in the checkpoint"" % k)
              new_state_dict[k] = v
    if hasattr(model, 'module'):
        model.module.load_state_dict(new_state_dict)
    else:
        model.load_state_dict(new_state_dict)
    print(""load "")
    logger.info(""Loaded checkpoint '{}' (iteration {})"".format(
        checkpoint_path, iteration))
    return model, optimizer, learning_rate, iteration


def save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):
  logger.info(""Saving model and optimizer state at iteration {} to {}"".format(
    iteration, checkpoint_path))
  if hasattr(model, 'module'):
    state_dict = model.module.state_dict()
  else:
    state_dict = model.state_dict()
  torch.save({'model': state_dict,
              'iteration': iteration,
              'optimizer': optimizer.state_dict(),
              'learning_rate': learning_rate}, checkpoint_path)

def clean_checkpoints(path_to_models='logs/44k/', n_ckpts_to_keep=2, sort_by_time=True):
  """"""Freeing up space by deleting saved ckpts

  Arguments:
  path_to_models    --  Path to the model directory
  n_ckpts_to_keep   --  Number of ckpts to keep, excluding G_0.pth and D_0.pth
  sort_by_time      --  True -> chronologically delete ckpts
                        False -> lexicographically delete ckpts
  """"""
  ckpts_files = [f for f in os.listdir(path_to_models) if os.path.isfile(os.path.join(path_to_models, f))]
  def name_key(_f):
      return int(re.compile(""._(\\d+)\\.pth"").match(_f).group(1))
  def time_key(_f):
      return os.path.getmtime(os.path.join(path_to_models, _f))
  sort_key = time_key if sort_by_time else name_key
  def x_sorted(_x):
      return sorted([f for f in ckpts_files if f.startswith(_x) and not f.endswith(""_0.pth"")], key=sort_key)
  to_del = [os.path.join(path_to_models, fn) for fn in
            (x_sorted('G')[:-n_ckpts_to_keep] + x_sorted('D')[:-n_ckpts_to_keep])]
  def del_info(fn):
      return logger.info(f"".. Free up space by deleting ckpt {fn}"")
  def del_routine(x):
      return [os.remove(x), del_info(x)]
  [del_routine(fn) for fn in to_del]

def summarize(writer, global_step, scalars={}, histograms={}, images={}, audios={}, audio_sampling_rate=22050):
  for k, v in scalars.items():
    writer.add_scalar(k, v, global_step)
  for k, v in histograms.items():
    writer.add_histogram(k, v, global_step)
  for k, v in images.items():
    writer.add_image(k, v, global_step, dataformats='HWC')
  for k, v in audios.items():
    writer.add_audio(k, v, global_step, audio_sampling_rate)


def latest_checkpoint_path(dir_path, regex=""G_*.pth""):
  f_list = glob.glob(os.path.join(dir_path, regex))
  f_list.sort(key=lambda f: int("""".join(filter(str.isdigit, f))))
  x = f_list[-1]
  print(x)
  return x


def plot_spectrogram_to_numpy(spectrogram):
  global MATPLOTLIB_FLAG
  if not MATPLOTLIB_FLAG:
    import matplotlib
    matplotlib.use(""Agg"")
    MATPLOTLIB_FLAG = True
    mpl_logger = logging.getLogger('matplotlib')
    mpl_logger.setLevel(logging.WARNING)
  import matplotlib.pylab as plt
  import numpy as np

  fig, ax = plt.subplots(figsize=(10,2))
  im = ax.imshow(spectrogram, aspect=""auto"", origin=""lower"",
                  interpolation='none')
  plt.colorbar(im, ax=ax)
  plt.xlabel(""Frames"")
  plt.ylabel(""Channels"")
  plt.tight_layout()

  fig.canvas.draw()
  data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')
  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))
  plt.close()
  return data


def plot_alignment_to_numpy(alignment, info=None):
  global MATPLOTLIB_FLAG
  if not MATPLOTLIB_FLAG:
    import matplotlib
    matplotlib.use(""Agg"")
    MATPLOTLIB_FLAG = True
    mpl_logger = logging.getLogger('matplotlib')
    mpl_logger.setLevel(logging.WARNING)
  import matplotlib.pylab as plt
  import numpy as np

  fig, ax = plt.subplots(figsize=(6, 4))
  im = ax.imshow(alignment.transpose(), aspect='auto', origin='lower',
                  interpolation='none')
  fig.colorbar(im, ax=ax)
  xlabel = 'Decoder timestep'
  if info is not None:
      xlabel += '\n\n' + info
  plt.xlabel(xlabel)
  plt.ylabel('Encoder timestep')
  plt.tight_layout()

  fig.canvas.draw()
  data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')
  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))
  plt.close()
  return data


def load_wav_to_torch(full_path):
  sampling_rate, data = read(full_path)
  return torch.FloatTensor(data.astype(np.float32)), sampling_rate


def load_filepaths_and_text(filename, split=""|""):
  with open(filename, encoding='utf-8') as f:
    filepaths_and_text = [line.strip().split(split) for line in f]
  return filepaths_and_text


def get_hparams(init=True):
  parser = argparse.ArgumentParser()
  parser.add_argument('-c', '--config', type=str, default=""./configs/config.json"",
                      help='JSON file for configuration')
  parser.add_argument('-m', '--model', type=str, required=True,
                      help='Model name')

  args = parser.parse_args()
  model_dir = os.path.join(""./logs"", args.model)

  if not os.path.exists(model_dir):
    os.makedirs(model_dir)

  config_path = args.config
  config_save_path = os.path.join(model_dir, ""config.json"")
  if init:
    with open(config_path, ""r"") as f:
      data = f.read()
    with open(config_save_path, ""w"") as f:
      f.write(data)
  else:
    with open(config_save_path, ""r"") as f:
      data = f.read()
  config = json.loads(data)

  hparams = HParams(**config)
  hparams.model_dir = model_dir
  return hparams


def get_hparams_from_dir(model_dir):
  config_save_path = os.path.join(model_dir, ""config.json"")
  with open(config_save_path, ""r"") as f:
    data = f.read()
  config = json.loads(data)

  hparams =HParams(**config)
  hparams.model_dir = model_dir
  return hparams


def get_hparams_from_file(config_path, infer_mode = False):
  with open(config_path, ""r"") as f:
    data = f.read()
  config = json.loads(data)
  hparams =HParams(**config) if not infer_mode else InferHParams(**config)
  return hparams


def check_git_hash(model_dir):
  source_dir = os.path.dirname(os.path.realpath(__file__))
  if not os.path.exists(os.path.join(source_dir, "".git"")):
    logger.warn(""{} is not a git repository, therefore hash value comparison will be ignored."".format(
      source_dir
    ))
    return

  cur_hash = subprocess.getoutput(""git rev-parse HEAD"")

  path = os.path.join(model_dir, ""githash"")
  if os.path.exists(path):
    saved_hash = open(path).read()
    if saved_hash != cur_hash:
      logger.warn(""git hash values are different. {}(saved) != {}(current)"".format(
        saved_hash[:8], cur_hash[:8]))
  else:
    open(path, ""w"").write(cur_hash)


def get_logger(model_dir, filename=""train.log""):
  global logger
  logger = logging.getLogger(os.path.basename(model_dir))
  logger.setLevel(logging.DEBUG)

  formatter = logging.Formatter(""%(asctime)s\t%(name)s\t%(levelname)s\t%(message)s"")
  if not os.path.exists(model_dir):
    os.makedirs(model_dir)
  h = logging.FileHandler(os.path.join(model_dir, filename))
  h.setLevel(logging.DEBUG)
  h.setFormatter(formatter)
  logger.addHandler(h)
  return logger


def repeat_expand_2d(content, target_len, mode = 'left'):
    # content : [h, t]
    return repeat_expand_2d_left(content, target_len) if mode == 'left' else repeat_expand_2d_other(content, target_len, mode)



def repeat_expand_2d_left(content, target_len):
    # content : [h, t]

    src_len = content.shape[-1]
    target = torch.zeros([content.shape[0], target_len], dtype=torch.float).to(content.device)
    temp = torch.arange(src_len+1) * target_len / src_len
    current_pos = 0
    for i in range(target_len):
        if i < temp[current_pos+1]:
            target[:, i] = content[:, current_pos]
        else:
            current_pos += 1
            target[:, i] = content[:, current_pos]

    return target


# mode : 'nearest'| 'linear'| 'bilinear'| 'bicubic'| 'trilinear'| 'area'
def repeat_expand_2d_other(content, target_len, mode = 'nearest'):
    # content : [h, t]
    content = content[None,:,:]
    target = F.interpolate(content,size=target_len,mode=mode)[0]
    return target


def mix_model(model_paths,mix_rate,mode):
  mix_rate = torch.FloatTensor(mix_rate)/100
  model_tem = torch.load(model_paths[0])
  models = [torch.load(path)[""model""] for path in model_paths]
  if mode == 0:
     mix_rate = F.softmax(mix_rate,dim=0)
  for k in model_tem[""model""].keys():
     model_tem[""model""][k] = torch.zeros_like(model_tem[""model""][k])
     for i,model in enumerate(models):
        model_tem[""model""][k] += model[k]*mix_rate[i]
  torch.save(model_tem,os.path.join(os.path.curdir,""output.pth""))
  return os.path.join(os.path.curdir,""output.pth"")
  
def change_rms(data1, sr1, data2, sr2, rate):  # 1是输入音频，2是输出音频,rate是2的占比 from RVC
    # print(data1.max(),data2.max())
    rms1 = librosa.feature.rms(
        y=data1, frame_length=sr1 // 2 * 2, hop_length=sr1 // 2
    )  # 每半秒一个点
    rms2 = librosa.feature.rms(y=data2.detach().cpu().numpy(), frame_length=sr2 // 2 * 2, hop_length=sr2 // 2)
    rms1 = torch.from_numpy(rms1).to(data2.device)
    rms1 = F.interpolate(
        rms1.unsqueeze(0), size=data2.shape[0], mode=""linear""
    ).squeeze()
    rms2 = torch.from_numpy(rms2).to(data2.device)
    rms2 = F.interpolate(
        rms2.unsqueeze(0), size=data2.shape[0], mode=""linear""
    ).squeeze()
    rms2 = torch.max(rms2, torch.zeros_like(rms2) + 1e-6)
    data2 *= (
        torch.pow(rms1, torch.tensor(1 - rate))
        * torch.pow(rms2, torch.tensor(rate - 1))
    )
    return data2

def train_index(spk_name,root_dir = ""dataset/44k/""):  #from: RVC https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI
    n_cpu = cpu_count()
    print(""The feature index is constructing."")
    exp_dir = os.path.join(root_dir,spk_name)
    listdir_res = []
    for file in os.listdir(exp_dir):
       if "".wav.soft.pt"" in file:
          listdir_res.append(os.path.join(exp_dir,file))
    if len(listdir_res) == 0:
        raise Exception(""You need to run preprocess_hubert_f0.py!"")
    npys = []
    for name in sorted(listdir_res):
        phone = torch.load(name)[0].transpose(-1,-2).numpy()
        npys.append(phone)
    big_npy = np.concatenate(npys, 0)
    big_npy_idx = np.arange(big_npy.shape[0])
    np.random.shuffle(big_npy_idx)
    big_npy = big_npy[big_npy_idx]
    if big_npy.shape[0] > 2e5:
        # if(1):
        info = ""Trying doing kmeans %s shape to 10k centers."" % big_npy.shape[0]
        print(info)
        try:
            big_npy = (
                MiniBatchKMeans(
                    n_clusters=10000,
                    verbose=True,
                    batch_size=256 * n_cpu,
                    compute_labels=False,
                    init=""random"",
                )
                .fit(big_npy)
                .cluster_centers_
            )
        except Exception:
            info = traceback.format_exc()
            print(info)
    n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)
    index = faiss.index_factory(big_npy.shape[1] , ""IVF%s,Flat"" % n_ivf)
    index_ivf = faiss.extract_index_ivf(index)  #
    index_ivf.nprobe = 1
    index.train(big_npy)
    batch_size_add = 8192
    for i in range(0, big_npy.shape[0], batch_size_add):
        index.add(big_npy[i : i + batch_size_add])
    # faiss.write_index(
    #     index,
    #     f""added_{spk_name}.index""
    # )
    print(""Successfully build index"")
    return index


class HParams():
  def __init__(self, **kwargs):
    for k, v in kwargs.items():
      if type(v) == dict:
        v = HParams(**v)
      self[k] = v

  def keys(self):
    return self.__dict__.keys()

  def items(self):
    return self.__dict__.items()

  def values(self):
    return self.__dict__.values()

  def __len__(self):
    return len(self.__dict__)

  def __getitem__(self, key):
    return getattr(self, key)

  def __setitem__(self, key, value):
    return setattr(self, key, value)

  def __contains__(self, key):
    return key in self.__dict__

  def __repr__(self):
    return self.__dict__.__repr__()

  def get(self,index):
    return self.__dict__.get(index)

  
class InferHParams(HParams):
  def __init__(self, **kwargs):
    for k, v in kwargs.items():
      if type(v) == dict:
        v = InferHParams(**v)
      self[k] = v

  def __getattr__(self,index):
    return self.get(index)


class Volume_Extractor:
    def __init__(self, hop_size = 512):
        self.hop_size = hop_size
        
    def extract(self, audio): # audio: 2d tensor array
        if not isinstance(audio,torch.Tensor):
           audio = torch.Tensor(audio)
        n_frames = int(audio.size(-1) // self.hop_size)
        audio2 = audio ** 2
        audio2 = torch.nn.functional.pad(audio2, (int(self.hop_size // 2), int((self.hop_size + 1) // 2)), mode = 'reflect')
        volume = torch.nn.functional.unfold(audio2[:,None,None,:],(1,self.hop_size),stride=self.hop_size)[:,:,:n_frames].mean(dim=1)[0]
        volume = torch.sqrt(volume)
        return volume
"
Vision-CAIR_MiniGPT-4,Vision-CAIR_MiniGPT-4_demo_v2.py,23445,https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/demo_v2.py,"import argparse
import os
import random
from collections import defaultdict

import cv2
import re

import numpy as np
from PIL import Image
import torch
import html
import gradio as gr

import torchvision.transforms as T
import torch.backends.cudnn as cudnn

from minigpt4.common.config import Config

from minigpt4.common.registry import registry
from minigpt4.conversation.conversation import Conversation, SeparatorStyle, Chat

# imports modules for registration
from minigpt4.datasets.builders import *
from minigpt4.models import *
from minigpt4.processors import *
from minigpt4.runners import *
from minigpt4.tasks import *


def parse_args():
    parser = argparse.ArgumentParser(description=""Demo"")
    parser.add_argument(""--cfg-path"", default='eval_configs/minigptv2_eval.yaml',
                        help=""path to configuration file."")
    parser.add_argument(""--gpu-id"", type=int, default=0, help=""specify the gpu to load the model."")
    parser.add_argument(
        ""--options"",
        nargs=""+"",
        help=""override some settings in the used config, the key-value pair ""
             ""in xxx=yyy format will be merged into config file (deprecate), ""
             ""change to --cfg-options instead."",
    )
    args = parser.parse_args()
    return args


random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

cudnn.benchmark = False
cudnn.deterministic = True

print('Initializing Chat')
args = parse_args()
cfg = Config(args)

device = 'cuda:{}'.format(args.gpu_id)

model_config = cfg.model_cfg
model_config.device_8bit = args.gpu_id
model_cls = registry.get_model_class(model_config.arch)
model = model_cls.from_config(model_config).to(device)
bounding_box_size = 100

vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train
vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)

model = model.eval()

CONV_VISION = Conversation(
    system="""",
    roles=(r""<s>[INST] "", r"" [/INST]""),
    messages=[],
    offset=2,
    sep_style=SeparatorStyle.SINGLE,
    sep="""",
)


def extract_substrings(string):
    # first check if there is no-finished bracket
    index = string.rfind('}')
    if index != -1:
        string = string[:index + 1]

    pattern = r'<p>(.*?)\}(?!<)'
    matches = re.findall(pattern, string)
    substrings = [match for match in matches]

    return substrings


def is_overlapping(rect1, rect2):
    x1, y1, x2, y2 = rect1
    x3, y3, x4, y4 = rect2
    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)


def computeIoU(bbox1, bbox2):
    x1, y1, x2, y2 = bbox1
    x3, y3, x4, y4 = bbox2
    intersection_x1 = max(x1, x3)
    intersection_y1 = max(y1, y3)
    intersection_x2 = min(x2, x4)
    intersection_y2 = min(y2, y4)
    intersection_area = max(0, intersection_x2 - intersection_x1 + 1) * max(0, intersection_y2 - intersection_y1 + 1)
    bbox1_area = (x2 - x1 + 1) * (y2 - y1 + 1)
    bbox2_area = (x4 - x3 + 1) * (y4 - y3 + 1)
    union_area = bbox1_area + bbox2_area - intersection_area
    iou = intersection_area / union_area
    return iou


def save_tmp_img(visual_img):
    file_name = """".join([str(random.randint(0, 9)) for _ in range(5)]) + "".jpg""
    file_path = ""/tmp/gradio"" + file_name
    visual_img.save(file_path)
    return file_path


def mask2bbox(mask):
    if mask is None:
        return ''
    mask = mask.resize([100, 100], resample=Image.NEAREST)
    mask = np.array(mask)[:, :, 0]

    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)

    if rows.sum():
        # Get the top, bottom, left, and right boundaries
        rmin, rmax = np.where(rows)[0][[0, -1]]
        cmin, cmax = np.where(cols)[0][[0, -1]]
        bbox = '{{<{}><{}><{}><{}>}}'.format(cmin, rmin, cmax, rmax)
    else:
        bbox = ''

    return bbox


def escape_markdown(text):
    # List of Markdown special characters that need to be escaped
    md_chars = ['<', '>']

    # Escape each special character
    for char in md_chars:
        text = text.replace(char, '\\' + char)

    return text


def reverse_escape(text):
    md_chars = ['\\<', '\\>']

    for char in md_chars:
        text = text.replace(char, char[1:])

    return text


colors = [
    (255, 0, 0),
    (0, 255, 0),
    (0, 0, 255),
    (210, 210, 0),
    (255, 0, 255),
    (0, 255, 255),
    (114, 128, 250),
    (0, 165, 255),
    (0, 128, 0),
    (144, 238, 144),
    (238, 238, 175),
    (255, 191, 0),
    (0, 128, 0),
    (226, 43, 138),
    (255, 0, 255),
    (0, 215, 255),
]

color_map = {
    f""{color_id}"": f""#{hex(color[2])[2:].zfill(2)}{hex(color[1])[2:].zfill(2)}{hex(color[0])[2:].zfill(2)}"" for
    color_id, color in enumerate(colors)
}

used_colors = colors


def visualize_all_bbox_together(image, generation):
    if image is None:
        return None, ''

    generation = html.unescape(generation)

    image_width, image_height = image.size
    image = image.resize([500, int(500 / image_width * image_height)])
    image_width, image_height = image.size

    string_list = extract_substrings(generation)
    if string_list:  # it is grounding or detection
        mode = 'all'
        entities = defaultdict(list)
        i = 0
        j = 0
        for string in string_list:
            try:
                obj, string = string.split('</p>')
            except ValueError:
                print('wrong string: ', string)
                continue
            bbox_list = string.split('<delim>')
            flag = False
            for bbox_string in bbox_list:
                integers = re.findall(r'-?\d+', bbox_string)
                if len(integers) == 4:
                    x0, y0, x1, y1 = int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3])
                    left = x0 / bounding_box_size * image_width
                    bottom = y0 / bounding_box_size * image_height
                    right = x1 / bounding_box_size * image_width
                    top = y1 / bounding_box_size * image_height

                    entities[obj].append([left, bottom, right, top])

                    j += 1
                    flag = True
            if flag:
                i += 1
    else:
        integers = re.findall(r'-?\d+', generation)

        if len(integers) == 4:  # it is refer
            mode = 'single'

            entities = list()
            x0, y0, x1, y1 = int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3])
            left = x0 / bounding_box_size * image_width
            bottom = y0 / bounding_box_size * image_height
            right = x1 / bounding_box_size * image_width
            top = y1 / bounding_box_size * image_height
            entities.append([left, bottom, right, top])
        else:
            # don't detect any valid bbox to visualize
            return None, ''

    if len(entities) == 0:
        return None, ''

    if isinstance(image, Image.Image):
        image_h = image.height
        image_w = image.width
        image = np.array(image)

    elif isinstance(image, str):
        if os.path.exists(image):
            pil_img = Image.open(image).convert(""RGB"")
            image = np.array(pil_img)[:, :, [2, 1, 0]]
            image_h = pil_img.height
            image_w = pil_img.width
        else:
            raise ValueError(f""invaild image path, {image}"")
    elif isinstance(image, torch.Tensor):

        image_tensor = image.cpu()
        reverse_norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])[:, None, None]
        reverse_norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])[:, None, None]
        image_tensor = image_tensor * reverse_norm_std + reverse_norm_mean
        pil_img = T.ToPILImage()(image_tensor)
        image_h = pil_img.height
        image_w = pil_img.width
        image = np.array(pil_img)[:, :, [2, 1, 0]]
    else:
        raise ValueError(f""invaild image format, {type(image)} for {image}"")

    indices = list(range(len(entities)))

    new_image = image.copy()

    previous_bboxes = []
    # size of text
    text_size = 0.5
    # thickness of text
    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))
    box_line = 2
    (c_width, text_height), _ = cv2.getTextSize(""F"", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)
    base_height = int(text_height * 0.675)
    text_offset_original = text_height - base_height
    text_spaces = 2

    # num_bboxes = sum(len(x[-1]) for x in entities)
    used_colors = colors  # random.sample(colors, k=num_bboxes)

    color_id = -1
    for entity_idx, entity_name in enumerate(entities):
        if mode == 'single' or mode == 'identify':
            bboxes = entity_name
            bboxes = [bboxes]
        else:
            bboxes = entities[entity_name]
        color_id += 1
        for bbox_id, (x1_norm, y1_norm, x2_norm, y2_norm) in enumerate(bboxes):
            skip_flag = False
            orig_x1, orig_y1, orig_x2, orig_y2 = int(x1_norm), int(y1_norm), int(x2_norm), int(y2_norm)

            color = used_colors[entity_idx % len(used_colors)]  # tuple(np.random.randint(0, 255, size=3).tolist())
            new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)

            if mode == 'all':
                l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1

                x1 = orig_x1 - l_o
                y1 = orig_y1 - l_o

                if y1 < text_height + text_offset_original + 2 * text_spaces:
                    y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces
                    x1 = orig_x1 + r_o

                # add text background
                (text_width, text_height), _ = cv2.getTextSize(f""  {entity_name}"", cv2.FONT_HERSHEY_COMPLEX, text_size,
                                                               text_line)
                text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (
                            text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1

                for prev_bbox in previous_bboxes:
                    if computeIoU((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox['bbox']) > 0.95 and \
                            prev_bbox['phrase'] == entity_name:
                        skip_flag = True
                        break
                    while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox['bbox']):
                        text_bg_y1 += (text_height + text_offset_original + 2 * text_spaces)
                        text_bg_y2 += (text_height + text_offset_original + 2 * text_spaces)
                        y1 += (text_height + text_offset_original + 2 * text_spaces)

                        if text_bg_y2 >= image_h:
                            text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))
                            text_bg_y2 = image_h
                            y1 = image_h
                            break
                if not skip_flag:
                    alpha = 0.5
                    for i in range(text_bg_y1, text_bg_y2):
                        for j in range(text_bg_x1, text_bg_x2):
                            if i < image_h and j < image_w:
                                if j < text_bg_x1 + 1.35 * c_width:
                                    # original color
                                    bg_color = color
                                else:
                                    # white
                                    bg_color = [255, 255, 255]
                                new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(
                                    np.uint8)

                    cv2.putText(
                        new_image, f""  {entity_name}"", (x1, y1 - text_offset_original - 1 * text_spaces),
                        cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA
                    )

                    previous_bboxes.append(
                        {'bbox': (text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), 'phrase': entity_name})

    if mode == 'all':
        def color_iterator(colors):
            while True:
                for color in colors:
                    yield color

        color_gen = color_iterator(colors)

        # Add colors to phrases and remove <p></p>
        def colored_phrases(match):
            phrase = match.group(1)
            color = next(color_gen)
            return f'<span style=""color:rgb{color}"">{phrase}</span>'

        generation = re.sub(r'{<\d+><\d+><\d+><\d+>}|<delim>', '', generation)
        generation_colored = re.sub(r'<p>(.*?)</p>', colored_phrases, generation)
    else:
        generation_colored = ''

    pil_image = Image.fromarray(new_image)
    return pil_image, generation_colored


def gradio_reset(chat_state, img_list):
    if chat_state is not None:
        chat_state.messages = []
    if img_list is not None:
        img_list = []
    return None, gr.update(value=None, interactive=True), gr.update(placeholder='Upload your image and chat',
                                                                    interactive=True), chat_state, img_list


def image_upload_trigger(upload_flag, replace_flag, img_list):
    # set the upload flag to true when receive a new image.
    # if there is an old image (and old conversation), set the replace flag to true to reset the conv later.
    upload_flag = 1
    if img_list:
        replace_flag = 1
    return upload_flag, replace_flag


def example_trigger(text_input, image, upload_flag, replace_flag, img_list):
    # set the upload flag to true when receive a new image.
    # if there is an old image (and old conversation), set the replace flag to true to reset the conv later.
    upload_flag = 1
    if img_list or replace_flag == 1:
        replace_flag = 1

    return upload_flag, replace_flag


def gradio_ask(user_message, chatbot, chat_state, gr_img, img_list, upload_flag, replace_flag):
    if len(user_message) == 0:
        text_box_show = 'Input should not be empty!'
    else:
        text_box_show = ''

    if isinstance(gr_img, dict):
        gr_img, mask = gr_img['image'], gr_img['mask']
    else:
        mask = None

    if '[identify]' in user_message:
        # check if user provide bbox in the text input
        integers = re.findall(r'-?\d+', user_message)
        if len(integers) != 4:  # no bbox in text
            bbox = mask2bbox(mask)
            user_message = user_message + bbox

    if chat_state is None:
        chat_state = CONV_VISION.copy()

    if upload_flag:
        if replace_flag:
            chat_state = CONV_VISION.copy()  # new image, reset everything
            replace_flag = 0
            chatbot = []
        img_list = []
        llm_message = chat.upload_img(gr_img, chat_state, img_list)
        upload_flag = 0

    chat.ask(user_message, chat_state)

    chatbot = chatbot + [[user_message, None]]

    if '[identify]' in user_message:
        visual_img, _ = visualize_all_bbox_together(gr_img, user_message)
        if visual_img is not None:
            file_path = save_tmp_img(visual_img)
            chatbot = chatbot + [[(file_path,), None]]

    return text_box_show, chatbot, chat_state, img_list, upload_flag, replace_flag


def gradio_answer(chatbot, chat_state, img_list, temperature):
    llm_message = chat.answer(conv=chat_state,
                              img_list=img_list,
                              temperature=temperature,
                              max_new_tokens=500,
                              max_length=2000)[0]
    chatbot[-1][1] = llm_message
    return chatbot, chat_state


def gradio_stream_answer(chatbot, chat_state, img_list, temperature):
    if len(img_list) > 0:
        if not isinstance(img_list[0], torch.Tensor):
            chat.encode_img(img_list)
    streamer = chat.stream_answer(conv=chat_state,
                                  img_list=img_list,
                                  temperature=temperature,
                                  max_new_tokens=500,
                                  max_length=2000)
    output = ''
    for new_output in streamer:
        escapped = escape_markdown(new_output)
        output += escapped
        chatbot[-1][1] = output
        yield chatbot, chat_state
    chat_state.messages[-1][1] = '</s>'
    return chatbot, chat_state


def gradio_visualize(chatbot, gr_img):
    if isinstance(gr_img, dict):
        gr_img, mask = gr_img['image'], gr_img['mask']

    unescaped = reverse_escape(chatbot[-1][1])
    visual_img, generation_color = visualize_all_bbox_together(gr_img, unescaped)
    if visual_img is not None:
        if len(generation_color):
            chatbot[-1][1] = generation_color
        file_path = save_tmp_img(visual_img)
        chatbot = chatbot + [[None, (file_path,)]]

    return chatbot


def gradio_taskselect(idx):
    prompt_list = [
        '',
        '[grounding] describe this image in detail',
        '[refer] ',
        '[detection] ',
        '[identify] what is this ',
        '[vqa] '
    ]
    instruct_list = [
        '**Hint:** Type in whatever you want',
        '**Hint:** Send the command to generate a grounded image description',
        '**Hint:** Type in a phrase about an object in the image and send the command',
        '**Hint:** Type in a caption or phrase, and see object locations in the image',
        '**Hint:** Draw a bounding box on the uploaded image then send the command. Click the ""clear"" botton on the top right of the image before redraw',
        '**Hint:** Send a question to get a short answer',
    ]
    return prompt_list[idx], instruct_list[idx]




chat = Chat(model, vis_processor, device=device)

title = """"""<h1 align=""center"">MiniGPT-v2 Demo</h1>""""""
description = 'Welcome to Our MiniGPT-v2 Chatbot Demo!'
# article = """"""<p><a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPTv2.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/GitHub-Repo-blue'></a></p><p><a href='https://www.youtube.com/watch?v=atFCwV2hSY4'><img src='https://img.shields.io/badge/YouTube-Video-red'></a></p>""""""
article = """"""<p><a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p>""""""

introduction = '''
For Abilities Involving Visual Grounding:
1. Grounding: CLICK **Send** to generate a grounded image description.
2. Refer: Input a referring object and CLICK **Send**.
3. Detection: Write a caption or phrase, and CLICK **Send**.
4. Identify: Draw the bounding box on the uploaded image window and CLICK **Send** to generate the bounding box. (CLICK ""clear"" button before re-drawing next time).
5. VQA: Input a visual question and CLICK **Send**.
6. No Tag: Input whatever you want and CLICK **Send** without any tagging

You can also simply chat in free form!
'''

text_input = gr.Textbox(placeholder='Upload your image and chat', interactive=True, show_label=False, container=False,
                        scale=8)
with gr.Blocks() as demo:
    gr.Markdown(title)
    # gr.Markdown(description)
    gr.Markdown(article)

    with gr.Row():
        with gr.Column(scale=0.5):
            image = gr.Image(type=""pil"", tool='sketch', brush_radius=20)

            temperature = gr.Slider(
                minimum=0.1,
                maximum=1.5,
                value=0.6,
                step=0.1,
                interactive=True,
                label=""Temperature"",
            )

            clear = gr.Button(""Restart"")

            gr.Markdown(introduction)

        with gr.Column():
            chat_state = gr.State(value=None)
            img_list = gr.State(value=[])
            chatbot = gr.Chatbot(label='MiniGPT-v2')

            dataset = gr.Dataset(
                components=[gr.Textbox(visible=False)],
                samples=[['No Tag'], ['Grounding'], ['Refer'], ['Detection'], ['Identify'], ['VQA']],
                type=""index"",
                label='Task Shortcuts',
            )
            task_inst = gr.Markdown('**Hint:** Upload your image and chat')
            with gr.Row():
                text_input.render()
                send = gr.Button(""Send"", variant='primary', size='sm', scale=1)

    upload_flag = gr.State(value=0)
    replace_flag = gr.State(value=0)
    image.upload(image_upload_trigger, [upload_flag, replace_flag, img_list], [upload_flag, replace_flag])

    with gr.Row():
        with gr.Column():
            gr.Examples(examples=[
                [""examples_v2/office.jpg"", ""[grounding] describe this image in detail"", upload_flag, replace_flag,
                 img_list],
                [""examples_v2/sofa.jpg"", ""[detection] sofas"", upload_flag, replace_flag, img_list],
                [""examples_v2/2000x1372_wmkn_0012149409555.jpg"", ""[refer] the world cup"", upload_flag, replace_flag,
                 img_list],
                [""examples_v2/KFC-20-for-20-Nuggets.jpg"", ""[identify] what is this {<4><50><30><65>}"", upload_flag,
                 replace_flag, img_list],
            ], inputs=[image, text_input, upload_flag, replace_flag, img_list], fn=example_trigger,
                outputs=[upload_flag, replace_flag])
        with gr.Column():
            gr.Examples(examples=[
                [""examples_v2/glip_test.jpg"", ""[vqa] where should I hide in this room when playing hide and seek"",
                 upload_flag, replace_flag, img_list],
                [""examples_v2/float.png"", ""Please write a poem about the image"", upload_flag, replace_flag, img_list],
                [""examples_v2/thief.png"", ""Is the weapon fateful"", upload_flag, replace_flag, img_list],
                [""examples_v2/cockdial.png"", ""What might happen in this image in the next second"", upload_flag,
                 replace_flag, img_list],
            ], inputs=[image, text_input, upload_flag, replace_flag, img_list], fn=example_trigger,
                outputs=[upload_flag, replace_flag])

    dataset.click(
        gradio_taskselect,
        inputs=[dataset],
        outputs=[text_input, task_inst],
        show_progress=""hidden"",
        postprocess=False,
        queue=False,
    )

    text_input.submit(
        gradio_ask,
        [text_input, chatbot, chat_state, image, img_list, upload_flag, replace_flag],
        [text_input, chatbot, chat_state, img_list, upload_flag, replace_flag], queue=False
    ).success(
        gradio_stream_answer,
        [chatbot, chat_state, img_list, temperature],
        [chatbot, chat_state]
    ).success(
        gradio_visualize,
        [chatbot, image],
        [chatbot],
        queue=False,
    )

    send.click(
        gradio_ask,
        [text_input, chatbot, chat_state, image, img_list, upload_flag, replace_flag],
        [text_input, chatbot, chat_state, img_list, upload_flag, replace_flag], queue=False
    ).success(
        gradio_stream_answer,
        [chatbot, chat_state, img_list, temperature],
        [chatbot, chat_state]
    ).success(
        gradio_visualize,
        [chatbot, image],
        [chatbot],
        queue=False,
    )

    clear.click(gradio_reset, [chat_state, img_list], [chatbot, image, text_input, chat_state, img_list], queue=False)

demo.launch(share=True, enable_queue=True)
"
